<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>Into The Void</title>
<link>https://intothevoid.github.io</link>
<description>Programming, technology, electronics, and other random thoughts</description>
<item>
<title>Likho: A lightweight static site generator written in Go</title>
<link>https://intothevoid.github.io/likho-a-lightweight-static-site-generator-written-in-go-2025-04-24.html</link>
<pubDate>Thu, 24 Apr 2025 17:34:16 +0930</pubDate>
<description><![CDATA[

`Likho` (_verb_): means `to write` (Hindi).

A static site generator is a tool that converts markdown files into a static website. It's a great way to create a simple and fast website. I have been using different static site generators to create my blog in the past. I really liked using [Hugo](https://gohugo.io/) but I always felt it was too much for my needs. I wanted something simple and have always wanted to write my own static site generator.

Likho is a static site generator that transforms markdown files into a beautiful, functional website. It's designed with simplicity in mind while providing powerful features that modern websites need. Whether you're a blogger, technical writer, or just someone who wants to maintain a personal website, Likho has you covered.

## Core Features

- 🚀 **Speed**: Built in Go for performance
- 📝 **Markdown First**: Write content in your favorite markdown editor
- 🎨 **Customizable**: Flexible theming system with support for custom templates
- 📊 **Mermaid Diagrams**: Create beautiful diagrams directly in your markdown
- 🔍 **Syntax Highlighting**: Code blocks that look great
- 📱 **Responsive Design**: Looks great on all devices
- 📰 **RSS & Sitemap**: Automatic generation of RSS feeds and sitemaps

## System Architecture

Here's how Likho works under the hood:

```mermaid
graph TD
    A[Markdown Files] --> B[Likho]
    B --> C[Content Parser]
    C --> D[Template Engine]
    D --> E[HTML Generator]
    E --> F[Static Site]
    G[Config] --> B
    H[Theme Files] --> D
```

The system follows a clear pipeline:

1. **Content Parser**: Reads markdown files and YAML frontmatter
2. **Template Engine**: Processes templates with Go's template engine
3. **HTML Generator**: Converts markdown to HTML and applies templates
4. **Asset Processor**: Handles static assets and theme files

## Directory Structure

```mermaid
graph TD
    A[Project Root] --> B[content]
    A --> C[themes]
    A --> D[public]
    B --> E[posts]
    B --> F[pages]
    B --> G[images]
    C --> H[default]
    H --> I[static]
    H --> J[templates]
    I --> K[css]
    I --> L[js]
    I --> M[images]
```

## How It Works

### 1. Content Processing

Likho processes your content in three main steps:

```mermaid
sequenceDiagram
    participant User
    participant Likho
    participant Parser
    participant Generator
    
    User->>Likho: Run generate command
    Likho->>Parser: Read markdown files
    Parser->>Parser: Parse frontmatter
    Parser->>Parser: Convert markdown to HTML
    Parser->>Generator: Pass processed content
    Generator->>Generator: Apply templates
    Generator->>Generator: Generate static files
    Generator->>User: Output static site
```

### 2. Theme System

The theme system is modular and easy to customize:

```mermaid
graph LR
    A[Theme] --> B[Base Template]
    B --> C[Post Template]
    B --> D[Page Template]
    B --> E[Index Template]
    A --> F[Static Assets]
    F --> G[CSS]
    F --> H[JS]
    F --> I[Images]
```

## Usage Example

Creating a new post is as simple as:

```bash
./likho create post "My New Post" -t "technology,golang" -i "https://example.com/image.jpg"
```

This generates a new markdown file with the following structure:

```yaml
---
title: "My New Post"
date: "2024-03-21"
tags: ["technology", "golang"]
image: "https://example.com/image.jpg"
description: ""
---
```

## Performance

One of Likho's key strengths is its speed and simplicity. Being written in Go, it can process thousands of markdown files in seconds. Here's a distribution of time taken to process files:

```mermaid
pie title Processing Time
    "Parsing" : 30
    "Template Processing" : 20
    "File Generation" : 10
    "Asset Copying" : 5
```

## Getting Started

1. Install Go 1.23 or later
2. Clone the repository
3. Build the application
4. Start creating content

```bash
git clone https://github.com/intothevoid/likho.git
cd likho
go build -o likho cmd/likho/main.go
```

## Building and serving your website

Running the `generate` command will read all the markdown files in the `content` directory and generate the static website in the `public` directory.

```bash
./likho generate
```
You can now locally serve your website using the `serve` command.

```bash
./likho serve
```

This will start a local server and you can view your website at `http://localhost:8080`.

Pro-tip: You can use Github pages to host your website for free, all you need to do is create a repository with the name `username.github.io` and push your `public` directory to the `gh-pages` branch. Github actions will automatically build and serve your website.

## Customizing your website

You can customize the look of your website by editing the themes inside the `themes/` directory. A few example themes have been provided for you to get started.

## Why Likho?

Likho combines the power of Go with the simplicity of markdown to create a static site generator that's both powerful and easy to use. Whether you're a developer looking to document your projects or a writer wanting to share your thoughts, Likho provides all the tools you need to create a beautiful, functional website.

### Proof is in the pudding

This website you are reading is built and powered by Likho. You can find the source code [here](https://github.com/intothevoid/likho-blog).

Here's how the deployment pipeline works:

```mermaid
graph LR
    A[Markdown Files] --> B[Likho]
    B --> C[HTML Files]
    C --> D[GitHub Repository]
    D --> E[GitHub Actions]
    E --> F[GitHub Pages]
    
    subgraph "Local Development"
        A
        B
        C
    end
    
    subgraph "Deployment"
        D
        E
        F
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333,stroke-width:2px
    style F fill:#bfb,stroke:#333,stroke-width:2px
```

The process is fully automated:
1. Write content in Markdown
2. Likho converts it to HTML
3. Push changes to GitHub
4. GitHub Actions automatically builds and deploys
5. Your site is live on GitHub Pages

Give it a try and let me know what you think! You can find the project on [GitHub](https://github.com/intothevoid/likho). 
]]></description>
</item>
<item>
<title>Top CLI tools for programmers on a Mac</title>
<link>https://intothevoid.github.io/top-cli-tools-for-programmers-on-a-mac-2025-04-21.html</link>
<pubDate>Mon, 21 Apr 2025 09:42:47 +0930</pubDate>
<description><![CDATA[

The aim of this post is to enhance and speed up your terminal terminal usage on a Mac. As a programmer you will spend a significant amount of time on the terminal. These tools will speed up common actions and help you become more productive. This post assumes you have Homebrew installed. If you don't follow the instructions on https://brew.sh/

```zsh
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

## 1. Mas - The Terminal App Store Manager

Tired of the clunky Mac App Store? **Mas** is a terminal app store manager that allows you to easily control and manage your apps downloaded from the App Store. It can be quite annoying and slow to open the App Store on your Macbook and this is a much cleaner and faster way to manage your apps.

**Installation:**

To install Mas, run the following command in your terminal:

```bash
brew install mas
```

**Usage:**

*   **Listing installed apps:** To see a list of your installed apps and their versions, use:

    ```bash
    mas list
    ```

    ![Screenshot of `mas list` output](images/cli-tools/mas-list.jpg)

*   **Getting app information:** You can retrieve detailed information about an app using its ID:

    ```bash
    mas info <app_id>
    ```

    To find the app ID, you can use `mas list`. For example, for the "Neptune" app, you can grab its ID and run:

    ```bash
    mas info <Neptune_app_id>
    ```

*   **Checking for outdated apps:** To see if any of your installed apps have updates available, run:

    ```bash
    mas outdated
    ```

*   **Upgrading apps:** To install available updates, use:

    ```bash
    mas upgrade
    ```

    This is particularly useful for updating Xcode command-line tools, which the App Store might not always handle correctly.

*   **Searching for apps:** You can search for specific apps in the App Store using:

    ```bash
    mas search <app_name>
    ```

*   **Downloading apps:** To download an app from the App Store, use its ID with the `purchase` command:

    ```bash
    mas purchase <app_id>
    ```

    For more commands, you can refer to the Mas GitHub page.

## 2. fzf - Fuzzy Finder

**fzf** is a powerful fuzzy finder that allows you to search through your files and directories efficiently in the terminal. The lesser you need to `cd`into directories, the faster your navigation will be.

![Screenshot of `fzf` output](images/cli-tools/fzf.jpg)

**Installation:**

Install fzf using Homebrew:

```bash
brew install fzf
```

**Integration:**

To integrate fzf with your shell (like zsh), you need to initialize it by adding the following line to your `~/.zshrc` (or equivalent RC file):

```bash
[[ -s "$(brew --prefix)/opt/fzf/shell/completion.zsh" ]] && source "$(brew --prefix)/opt/fzf/shell/completion.zsh"
[[ -s "$(brew --prefix)/opt/fzf/shell/key-bindings.zsh" ]] && source "$(brew --prefix)/opt/fzf/shell/key-bindings.zsh"
```

After adding these lines, **save and source your RC file** (e.g., `source ~/.zshrc`). You can verify the installation by running `fzf` or `fzf --version`.

**Key Bindings and Usage**

Custom fzf configurations in the `.zshrc` file.

*   **`Ctrl+T`:** This key binding performs a fuzzy search for all files in the current directory (and subdirectories), using `fd` (explained later) to speed up the search and excluding `.git` files.

*   **`Option+C` (or `Alt+C`):** This command specifically searches for directories, allowing you to quickly jump to a desired directory.

*   **Custom Command (`nvim Ctrl+T` or `code Ctrl+T`):** You can also use fzf with other commands like `nvim` (neovim) and `code` (VS Code) to fuzzy find files and open them in the respective editor. For example, `nvim` followed by `Ctrl+T` allows fuzzy searching through all files and opening the selected one in neovim. Similarly, `code Ctrl+T` does the same for VS Code.

## 3. fd - A Simple, Fast and User-Friendly Alternative to 'find'

You can use **fd** as a faster alternative to the `find` command, especially in your fzf setup.

**Installation:**

Install fd using Homebrew:

```bash
brew install fd
```

fd is used in the default fzf command (`Ctrl+T`) with the `-H` flag to show hidden directories and the `-I` flag to ignore `.git` files.

## 4. bat - A cat(1) Clone with Wings

**bat** is a "must-have" tool, acting as a `cat` replacement but with syntax highlighting for code views.

**Installation:**

Install bat using Homebrew:

```bash
brew install bat
```

**Usage:**

Simply use `bat` followed by a filename to view its content with syntax highlighting:

```bash
bat <filename>
```

![Screenshot of `bat` output](images/cli-tools/bat.jpg)

This is especially useful when combined with fzf.

## 5. tldr - Collaborative Cheat Sheets for Console Commands

**tldr** is similar to man pages but providing only the most commonly used commands and examples. You can use it to quickly get the essential commands without diving into the detailed explanations of man pages.

**Installation:**

Install tldr using Homebrew:

```bash
brew install tldr
```

**Usage:**

To get a quick overview of a command, use `tldr` followed by the command name:

```bash
tldr <command>
```

For example:

```bash
tldr git
```

![Screenshot of `tldr git` output](images/cli-tools/tldr-git.jpg)

## 6. fzf.git - Fuzzy Finder Extensions for Git

**fzf.git** integrates fzf with Git, providing convenient fuzzy searching for Git branches, remotes, and worktrees. This can be sourced into your .zshrc file

**Installation:**

Start by cloning the fzf.git repository:

```bash
git clone https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install
```

**Usage (via key bindings):**

The following key bindings should work in a Git repository with a remote:

*   **`Ctrl+GB`:** Shows all Git branches.

*   **`Ctrl+GR`:** Shows all Git remotes.

*   **`Ctrl+GW`:** Shows Git worktrees.

## 7. zoxide - A Smarter cd Command

**zoxide** is a tool that learns your most frequently used directories and allows you to jump to them quickly without typing the full path. It remembers paths based on recency.

**Installation:**

Install zoxide using Homebrew:

```bash
brew install zoxide
```

**Integration:**

Add the following line to your `~/.zshrc` file to initialize zoxide:

```bash
eval "$(zoxide init zsh)"
```

I have added an alias to zoxide in my ~/.bashrc to replace the `cd` command -

```bash
alias cd='z'
```

This can be super convenient if you frequently traverse deep folder hierarchies.

![Screenshot of `zoxide` output](images/cli-tools/zoxide.jpg)

**Save and source your RC file**.

**Usage:**

*   **Jumping to a directory:** Once zoxide has learned some directories (by you navigating into them using `cd` at least once), you can jump to them using the `z` command followed by a part of the directory name:

    ```bash
    z <partial_directory_name>
    ```

    For example, after navigating to a "projects" directory, you can later jump back with `z pro`.

*   **Returning to home:** Typing `z` alone will take you back to your home directory.

## 8. eza - A Modern, Feature-Rich Replacement for ls

**eza** is an enhanced version of the `ls` command, offering improved output with icons, colors, and more. It provides better visual clarity.

**Installation:**

Install eza using Homebrew:

```bash
brew install eza
```

**Usage:**

You can have your `ls` command aliased to `eza` with various options configured in your `.zshrc`:

```bash
alias ls="eza -l --icons --group-directories-first"
```

![Screenshot of `eza` output](images/cli-tools/eza.jpg)

*   **Basic listing:** Running `ls` (which is `eza` in this case) will provide a detailed listing with icons and grouped directories.

*   **Grid view:** You can also configure eza to display output in a grid format:

    ```bash
    alias ls="eza --grid --icons"
    ```

*   **Integration with fzf:** The `Option+C` (directory search) in fzf also includes eza for previews, showing detailed information about the selected directory.

## 9. yazi - A Blazing Fast Terminal File Manager

**yazi** is a terminal file manager built on Rust. It is a great addition to your terminal.

**Installation:**

Install yazi using Homebrew:

```bash
brew install yazi ffmpeg sevenzip jq poppler fd ripgrep fzf zoxide resvg imagemagick font-symbols-only-nerd-font
```

yazi works well with other tools but if you don't want to install the others use `brew install yazi` only


**Usage:**

Running `y` will open the yazi file manager if you add the following function to your ~/.bashrc -

```bash
function y() {
	local tmp="$(mktemp -t "yazi-cwd.XXXXXX")" cwd
	yazi "$@" --cwd-file="$tmp"
	if cwd="$(command cat -- "$tmp")" && [ -n "$cwd" ] && [ "$cwd" != "$PWD" ]; then
		builtin cd -- "$cwd"
	fi
	rm -f -- "$tmp"
}
```

*   **Navigation:** Use the up and down arrow keys to navigate within the current directory, and the left and right arrow keys to go to the previous and next directories.

    ![Screenshot of the yazi interface](images/cli-tools/yazi.jpg)

*   **Customization:** Yazi can be customized by creating a `.config/yazi` directory and adding configuration files like `yazi.toml`, `keymaps.toml`, and `themes.toml`. Themes can be downloaded from the Yazi GitHub repository.

*   **Key Mappings:** Use Vim-like key mappings (configured in `keymaps.toml`) such as `j` and `k` for moving up and down, `l` to enter a directory, and `h` to go back.

*   **Themes:** Themes can be configured in the `themes.toml` file.

*   **Basic File Operations:** Yazi offers various commands:
    *   `a`: Add a file (e.g., `test.js`).
    *   `/`: Create a directory.
    *   `y`: Yank (copy) a file.
    *   `p`: Paste a file.
    *   `d`: Delete a file to trash.
    *   `D`: Permanently delete a file.
    *   `x`: Cut a file.
    *   `/` or `?`: Start a search.
    *   `Enter`: Open a file with the default editor (can be configured, e.g., to neovim).
    *   `: `: Run a shell command.

## 10. tmux - Terminal Multiplexer

**tmux** is a powerful terminal multiplexer that allows you to manage multiple terminal sessions within a single window.

**Installation:**

Install tmux using Homebrew:

```bash
brew install tmux
```

**Configuration:**

The `~/.tmux.conf` file is used for custom configurations and there's a `~/.tmux/plugins` directory for tmux plugins.

**Key Bindings and Usage:**

*   **Prefix Key:** `Ctrl+b` is used as the prefix key.
*   **Splitting Panes:**
    *   `Ctrl+b` then `%` Split vertically.
    *   `Ctrl+b` then `"`: Split horizontally.
*   **Reload Configuration:** `Ctrl+b` then `r`: Reloads the tmux configuration.
*   **Pane Resizing:** `Ctrl+b` then `H`, `J`, `K`, or `L` (using Vim-like motion keys): Resizes panes.
*   **Selection Mode:** `Ctrl+b` then `[`: Enters selection mode, allowing you to copy text.
*   **Tmux Sessionizer (via script):** A script to manage tmux sessions.
*   **Tmux Plugin Manager (TPM):** TPM is used to manage tmux plugins. After installing TPM (by cloning its GitHub repository), you need to add a line to the end of your `~/.tmux.conf` to run the TPM configuration.

    ```tmux
    run '~/.tmux/plugins/tpm/tpm'
    ```

*   **Plugins:** The following plugins can be used:
    *   `tmux-plugins/tpm`: The tmux plugin manager itself.
    *   `christoomey/vim-tmux-navigator`: Allows seamless navigation between Vim and tmux panes using Vim motion keys.
    *   `thewtex/tmux-sessionx`: Provides an enhanced session management interface, accessible with `Ctrl+b` then `O`. It allows easy switching, creation (by typing a new name), and renaming (with `Ctrl+r`) of tmux sessions.
    *   Plugins for online status, battery, and theming.
    *   `tmux-plugins/tmux-resurrect`: Saves and restores tmux sessions.
    *   `tmux-plugins/tmux-continuum`: Automatically saves tmux sessions every 15 minutes (requires `tmux-resurrect`).

**Plugin Management:** After setting up TPM and listing plugins in `~/.tmux.conf`, you need to:

1.  Save the `~/.tmux.conf` file.
2.  Reload the tmux configuration: `Ctrl+b` then `r`.
3.  Install the plugins: `Ctrl+b` then `I` (capital I).

After this, kill all tmux servers (`tmux kill-server`) and start a new tmux session.

Tmux allows for quickly switching between different project sessions, reducing the need to constantly navigate through directories. I might end up writing a more detailed post on Tmux because it offers so much functionality and deep customisation.

These command-line tools can significantly enhance a programmer's terminal experience, making tasks more efficient and enjoyable. By following the installation and usage instructions on this page, you can start integrating these tools into your workflow. Don't hesitate to explore their advanced features and configurations to tailor them to your specific needs. Remember to refer to the respective project GitHub pages for more detailed information and options.
]]></description>
</item>
<item>
<title>Daysync - An ESP32 based smart screen for your desk</title>
<link>https://intothevoid.github.io/daysync---an-esp32-based-smart-screen-for-your-desk-2025-04-17.html</link>
<pubDate>Thu, 17 Apr 2025 15:32:35 +0930</pubDate>
<description><![CDATA[

## Overview

![Daysync](images/esp32/daysync.gif)

Daysync combines an ESP32 microcontroller with a TFT display (ESP32-2432S028R) to create a smart information display system. It fetches and displays various types of data including weather information, MotoGP and Formula 1 race calendars, cryptocurrency prices, stock market data and news headlines.

```mermaid
graph LR
    subgraph ESP32["ESP32 Display"]
        E[ESP32 Board]
        T[TFT Display]
        E --> T
    end

    subgraph Backend["Go Backend"]
        B[API Server]
        W[Weather API]
        M[MotoGP API]
        F[F1 API]
        C[Crypto API]
        N[News API]
        S[Stock API]
    end

    E <-->|HTTP Requests| B
    B <-->|Data Fetch| W
    B <-->|Data Fetch| M
    B <-->|Data Fetch| F
    B <-->|Data Fetch| C
    B <-->|Data Fetch| N
    B <-->|Data Fetch| S
```

## Parts

### ESP32-2432S028R with 2.4" TFT Display

These boards are cheap and easy to find on AliExpress. You can find them [here](https://vi.aliexpress.com/item/1005008598530650.html?spm=a2g0o.productlist.main.73.472b1ae23nfIwc&algo_pvid=2f24f0e3-cf71-4b57-84db-2e338bede7a4&algo_exp_id=2f24f0e3-cf71-4b57-84db-2e338bede7a4-36&pdp_ext_f=%7B%22order%22%3A%22-1%22%2C%22eval%22%3A%221%22%7D&pdp_npi=4%40dis%21AUD%2120.59%2113.59%21%21%2194.28%2162.23%21%40210337bc17448542486703194e17c5%2112000045890694919%21sea%21AU%212747051205%21X&curPageLogUid=nbT3rzhrKgn2&utparam-url=scene%3Asearch%7Cquery_from%3A).

### 3D Printed Enclosure 

I designed a 3D printed enclosure using the free version of Autodesk Fusion 360 for the ESP32-2432S028R with 2.4" TFT Display. This was quite time consuming and I had to use Vernier Callipers to get accurate measurements. At the end of it, the result was better than I expected. I used a Bambu Labs P1S printer to print the enclosure. I even managed to get a cool `bindok` logo etched at the back of the enclosure.

`bindok` meaning no brains is my gamer alias 🤣 

![Enclosure Design](images/esp32/cyd-case.jpg)

You can download the STL file [here](other/esp32/esp32-2432s028r-bindok.stl)

### Other Parts

- Micro USB Cable for programming the ESP32
- USB power (any cheap 5V adaptor)
- USB-C to micro usb adaptor if your PC only has USB-C ports
- Vernier Callipers for measuring the dimensions of the board

## Source Code

The source code is available on [GitHub](https://github.com/intothevoid/daysync).

## System Architecture

The project consists of two main components:

1. **ESP32 Display System**
   - ESP32 microcontroller
   - TFT display
   - WiFi connectivity
   - LVGL for UI rendering

2. **Go Backend Server**
   - RESTful API endpoints
   - Data caching
   - External API integration
   - Configuration management

```mermaid
graph TD
    subgraph ESP32["ESP32 System"]
        W[WiFi Module]
        M[Memory]
        D[Display Driver]
        W --> M
        M --> D
    end

    subgraph Backend["Backend Services"]
        C[Cache Layer]
        A[API Layer]
        E[External APIs]
        C --> A
        A --> E
    end

    ESP32 <-->|HTTP| Backend
```

## Backend Functionality

### API Endpoints

The Go backend provides several RESTful endpoints:

```mermaid
graph TD
    API[API Server] --> MotoGP[MotoGP Endpoints]
    API --> F1[F1 Endpoints]
    API --> Weather[Weather Endpoint]
    API --> Crypto[Crypto Endpoint]
    API --> News[News Endpoint]
    API --> Finance[Finance Endpoint]

    MotoGP --> |/api/motogp| SeasonData
    MotoGP --> |/api/motogpnextrace| NextRace
    F1 --> |/api/formula1| SeasonData
    F1 --> |/api/formula1nextrace| NextRace
    Weather --> |/api/weather| LocationData
    Crypto --> |/api/crypto| PriceData
    News --> |/api/news| Headlines
    Finance --> |/api/finance| StockData
```

### Data Flow

```mermaid
sequenceDiagram
    participant ESP32
    participant Backend
    participant ExternalAPI

    ESP32->>Backend: HTTP Request
    Backend->>Backend: Check Cache
    alt Cache Hit
        Backend-->>ESP32: Return Cached Data
    else Cache Miss
        Backend->>ExternalAPI: Fetch New Data
        ExternalAPI-->>Backend: Return Data
        Backend->>Backend: Update Cache
        Backend-->>ESP32: Return Fresh Data
    end
```

## ESP32 Display Features

### Screen Management

The ESP32 display automatically cycles through different information screens:

```mermaid
stateDiagram-v2
    [*] --> Weather
    Weather --> MotoGP
    MotoGP --> Formula1
    Formula1 --> StockMarket
    StockMarket --> Cryptocurrency
    Cryptocurrency --> News1
    News1 --> News2
    News2 --> About
    About --> Weather
```

### Data Display

Each screen type has specific information layout:

```mermaid
graph TD
    subgraph Weather["Weather Screen"]
        W1[Location]
        W2[Date/Time]
        W3[Temperature]
        W4[Humidity]
        W5[Conditions]
    end

    subgraph Racing["Race Calendar Screen"]
        R1[Race Name]
        R2[Circuit]
        R3[Date]
        R4[Next Race Info]
    end

    subgraph Finance["Financial Screen"]
        F1[Symbol]
        F2[Current Price]
        F3[Price Change]
        F4[Market Data]
    end
```

## Technical Implementation

### Backend Features

1. **Caching System**
   - 60-minute cache duration
   - Memory-efficient storage
   - Automatic cache invalidation

2. **API Integration**
   - RESTful endpoints
   - Error handling
   - Rate limiting
   - CORS support

3. **Configuration Management**
   - YAML configuration
   - Environment variables
   - Test mode support

### ESP32 Features

1. **Display Management**
   - LVGL for UI
   - Screen rotation
   - Automatic updates (Server side, microcontroller auto updates not supported)
   - Error handling

2. **Data Processing**
   - JSON parsing
   - Data validation
   - Error recovery
   - WiFi reconnection

## Development and Testing

The project includes several development features:

```mermaid
graph LR
    subgraph Dev["Development Tools"]
        T[Test Mode]
        C[Cache Control]
        D[Debug Logging]
        M[Mock Data]
    end

    subgraph Test["Testing Features"]
        U[Unit Tests]
        I[Integration Tests]
        E[Error Simulation]
        P[Performance Tests]
    end
```

## Possible Future Enhancements

1. **Planned Features**
   - Ability for user to change their wifi SSID and password (harcoded at the moment)
   - Custom screen layouts
   - User preferences
   - More data sources
   - Offline mode
   - Automatic OTA updates

This was a fun project that taught me how to use a low power microcontroller to display information. The ESP32 only makes REST calls, the heavy lifting is done by the Go backend. By combining the power of ESP32 with a Go backend, it provides a flexible way to display various types of real-time data.

There can be many improvements to enhance the user experience. User preferences via a web interface would be great. I'll hopefully find time to keep improving this project and make it more user friendly.
]]></description>
</item>
<item>
<title>Kramerbot - a deal finding Telegram bot</title>
<link>https://intothevoid.github.io/kramerbot---a-deal-finding-telegram-bot-2025-04-04.html</link>
<pubDate>Fri, 04 Apr 2025 12:13:19 +1030</pubDate>
<description><![CDATA[

## Demo

A demo of the bot is running at [https://t.me/kramerbot](https://t.me/kramerbot).

## Overview

KramerBot is a Telegram bot designed to help users stay updated with the latest deals from websites like www.ozbargain.com.au and Amazon (via CamelCamelCamel). This bot acts as your personal deal hunter, constantly monitoring for the best bargains and notifying you instantly when they're found.

This bot was created out of a need to find specific deals. Each time I wanted to purchase something online, it was tedious waiting for that product to go on sale. So I decided to write a bot that would notify me when the price dropped below a certain threshold.

Ozbargain is a community driven website where users post deals they find. The deals are voted on by other users and the top deals are displayed on the front page. 

Amazon needs no introduction. CamelCamelCamel is a website that scrapes Amazon and provides an API for the price history of a given product.

## Key Features

The bot is built using Go and uses the Telegram Bot API to send messages to users. Some of its features are:

- **Real-time Deal Notifications**: Get instant updates about deals through Telegram
- **Multiple Deal Sources**:
  - OzBargain (Good deals with 25+ votes)
  - OzBargain (Super deals with 50+ votes)
  - Amazon Australia (Daily deals)
  - Amazon Australia (Weekly deals)
- **Custom Keyword Watchlists**: Set up personalized deal alerts based on keywords
- **Android TV Notifications**: Optional integration with Pipup for TV notifications
- **Admin Features**: Send announcements to all users, if you are an admin
- **SQLite Database**: Persistent storage of user preferences and deal history
- **Docker Support**: Easy deployment with containerization

## Architecture

The architecture of the bot and its main system components are shown in the diagram below:

### System Components

```mermaid
graph TD
    A[Telegram Bot API] --> B[KramerBot]
    B --> C[OzBargain Scraper]
    B --> D[Amazon Scraper]
    B --> E[SQLite Database]
    B --> F[Pipup Service]
    C --> G[OzBargain Website]
    D --> H[Amazon/CamelCamelCamel]
```

### Core Components

1. **Bot Core (`bot/`)**
   - Handles Telegram API interactions
   - Manages user commands and responses
   - Processes deal notifications

2. **Scrapers (`scrapers/`)**
   - OzBargainScraper: Monitors OzBargain deals
   - CamCamCamScraper: Tracks Amazon deals via CamelCamelCamel

3. **Data Models (`models/`)**
   - UserData: Stores user preferences and settings
   - Deal structures for different platforms
   - Notification configurations

4. **Persistence Layer**
   - SQLite database for user data
   - Deal history tracking
   - User preferences storage

## Design

The data model and the flow of the deal processing are shown in the diagrams below:

### User Data Model

```mermaid
classDiagram
    class UserData {
        +int64 ChatID
        +string Username
        +bool OzbGood
        +bool OzbSuper
        +bool AmzDaily
        +bool AmzWeekly
        +[]string Keywords
        +[]string OzbSent
        +[]string AmzSent
    }
```

### Deal Processing Flow

```mermaid
sequenceDiagram
    participant S as Scraper
    participant B as Bot
    participant D as Database
    participant U as User
    
    S->>B: New Deal Found
    B->>D: Check Deal History
    D-->>B: Deal Status
    B->>U: Send Notification
    B->>D: Update History
```

## Functionality

### User Commands

The bot supports the following commands:

1. **Basic Commands**
   - `/start` - Register or view status
   - `/help` - Show help message
   - `/preferences` - View current settings
   - `/test` - Send test notification

2. **Deal Type Toggles**
   - `/ozbgood` - Toggle OzBargain Good deals
   - `/ozbsuper` - Toggle OzBargain Super deals
   - `/amzdaily` - Toggle Amazon Daily deals
   - `/amzweekly` - Toggle Amazon Weekly deals

3. **Keyword Management**
   - `/addkeyword <keyword>` - Add keyword to watchlist
   - `/removekeyword <keyword>` - Remove keyword
   - `/listkeywords` - View current keywords

### Deal Processing

1. **Scraping**
   - Regular interval-based scraping
   - Configurable scrape intervals
   - Maximum deal storage limits
   - Duplicate detection

2. **Notification**
   - Instant Telegram notifications
   - Optional Android TV notifications
   - Deal history tracking
   - Custom formatting for different deal types

### Admin Features

- Send announcements to all users
- System status monitoring
- User management capabilities

## Deployment

### Requirements

- Go 1.18+
- SQLite3
- Telegram Bot Token
- (Optional) Pipup configuration for TV notifications

### Configuration

Primary configuration through `config.yaml`:
- Scraper intervals
- Logging settings
- Database paths
- Notification settings

Environment variables for sensitive data:
- `TELEGRAM_BOT_TOKEN`
- `KRAMERBOT_ADMIN_PASS`
- `SQLITE_DB_PATH`

### Docker Deployment

```bash
# Build
docker build -t kramerbot:latest .

# Run
docker run -d --name kramerbot \
  --env-file ./kramerbot.env \
  -v "$(pwd)/data:/app/data" \
  --restart unless-stopped \
  kramerbot:latest
```

## In the pipeline (and some ideas)

1. Web interface for user management
2. Additional deal sources
3. Advanced filtering options
4. Deal analytics and trends
5. User preferences synchronization
6. Enhanced admin dashboard

KramerBot provides a solution for deal hunters who want to stay updated with the latest bargains without constantly checking multiple websites. It has been designed to be modular and easy to extend. New websites can be added by implementing the `scraper` interface.]]></description>
</item>
<item>
<title>Vim Setup From Scratch</title>
<link>https://intothevoid.github.io/vim-setup-from-scratch-2024-10-14.html</link>
<pubDate>Mon, 14 Oct 2024 09:31:35 +0800</pubDate>
<description><![CDATA[

Vim setups can be quite involved. There is the batteries included approach (SpaceVim, LazyVim etc.) and there's the hand rolled approach. Although the readymade approach is easier, manually setting up your Vim configuration can be a good learning experience.

This guide is a way for me to have all the steps I normally follow when installing Vim. I'll try and keep it simple and focus on languages I enjoy using as a developer - Golang and Python.

This guide will also focus on MacOS as I am using an M1 Macbook Pro at the moment. Most of the steps can be easily translated to other operating systems.

## Installation

Install Vim by issuing the following command -

```bash
brew install vim
```

## Sensible defaults

The sensible defaults plugin is a great way to have a starting point with your .vimrc (Vim's configuration file) without having to copy over someone else's configuration file. 

However, this is a plugin which will have to be installed using the steps in section 'Plugin Manager'

## Plugin Manager

One of the easiest ways to set things up in Vim and install plugins is to use a plugin manager. We'll use vim-plug here in our example -

```bash
sh -c 'curl -fLo "${XDG_DATA_HOME:-$HOME/.local/share}"/nvim/site/autoload/plug.vim --create-dirs \
       https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim'
```
Add a vim-plug section to your ~/.vimrc (or ~/.config/nvim/init.vim for Neovim)

```bash
call plug#begin()

" List your plugins here
Plug 'tpope/vim-sensible'

call plug#end()
```
Reload the file or restart Vim, then you can,
`:PlugInstall` to install the plugins
`:PlugUpdate` to install or update the plugins
`:PlugDiff` to review the changes from the last update
`:PlugClean` to remove plugins no longer in the list


## Useful developer plugins

Below is a list of useful plugins to install in Vim -

* Vim sensible - A set of sensible defaults for your .vimrc
* NERDTree - File explorer
* fzf.vim - Fuzzy file finder
* ALE - Linting and static analysis
* vim-fugitive - Git integration

Add these between the call plug#begin() and call plug#end() sections of your .vimrc -

```bash
Plug 'tpope/vim-sensible'
Plug 'scrooloose/nerdtree'
Plug 'junegunn/fzf'
Plug 'w0rp/ale'
Plug 'tpope/vim-fugitive'
```

Use the `:PlugInstall` command to install above plugins.

## Language support

I am tailoring this guide for Golang as it is my preferred programming language for now. Install the following plugins, by modifying your ~/.vimrc file -

```bash
Plug 'fatih/vim-go'
Plug 'vim-python/python-syntax'

```

## Colour scheme

Choose a color scheme that's easy on the eyes for long coding sessions. Molokai is a popular choice -

### Download

```bash
curl -fLo ~/.vim/colors/monokai.vim --create-dirs https://raw.githubusercontent.com/crusoexia/vim-monokai/refs/heads/master/colors/monokai.vim
```

### Installation

Select the theme by adding the following lines to your ~/.vimrc

```bash
colorscheme monokai
```
## Key mappings

You can add custom keymappings, such as invoking NERDTree, by adding the following to your ~/.vimrc

```bash
nnoremap <C-n> :NERDTreeToggle<CR>
nnoremap <C-p> :FZF<CR>
```

## Go specific plugins

```bash
Plug 'fatih/vim-go', { 'do': ':GoUpdateBinaries' }
Plug 'neoclide/coc.nvim', {'branch': 'release'}
Plug 'SirVer/ultisnips'
Plug 'AndrewRadev/splitjoin.vim'
```

* vim-go: The essential plugin for Go development in Vim
* coc.nvim: For advanced code completion (install gopls separately)
* ultisnips: For code snippets
* splitjoin.vim: Useful for splitting/joining struct literals

## Go specific settings

Use the following settings for an optimal Go programming experience -

```bash
" Go syntax highlighting
let g:go_highlight_fields = 1
let g:go_highlight_functions = 1
let g:go_highlight_function_calls = 1
let g:go_highlight_extra_types = 1
let g:go_highlight_operators = 1

" Auto formatting and importing
let g:go_fmt_autosave = 1
let g:go_fmt_command = "goimports"

" Status line types/signatures
let g:go_auto_type_info = 1

" Run :GoBuild or :GoTestCompile based on the go file
function! s:build_go_files()
  let l:file = expand('%')
  if l:file =~# '^\f\+_test\.go$'
    call go#test#Test(0, 1)
  elseif l:file =~# '^\f\+\.go$'
    call go#cmd#Build(0)
  endif
endfunction

" Map keys for most used commands.
" Ex: `\b` for building, `\r` for running and `\b` for running test.
autocmd FileType go nmap <leader>b :<C-u>call <SID>build_go_files()<CR>
autocmd FileType go nmap <leader>r  <Plug>(go-run)
autocmd FileType go nmap <leader>t  <Plug>(go-test)
```

Remember to run :GoInstallBinaries after setting this up to install necessary Go tools. Also, ensure you have gopls installed (go install golang.org/x/tools/gopls@latest) for the best code completion experience with coc.nvim.

This should give you a solid starting point for your Vim install. You can visit a website like https://vimawesome.com/ to get inspiration for plugins and further customisation.

]]></description>
</item>
<item>
<title>Variable Power Supply</title>
<link>https://intothevoid.github.io/variable-power-supply-2024-07-21.html</link>
<pubDate>Sun, 21 Jul 2024 22:01:24 +0930</pubDate>
<description><![CDATA[

## Introduction

In today's world of electronics and DIY projects, having a reliable power supply is essential. However, purchasing a variable power supply can be quite expensive. In this blog post, we'll guide you through creating your own variable power supply using an old laptop charger. This project not only saves you money but also gives new life to old electronic components that might otherwise end up in landfill.

## Parts Required

To embark on this project, you'll need the following items:

- A 3D printer
- An old laptop power supply with an output DC voltage greater than 6 volts
- A Programmable Constant Voltage Current Step-down Power Supply Module [WZ3605E](https://vi.aliexpress.com/w/wholesale-WZ3605E.html?spm=a2g0o.home.search.0)
- Banana plug socket [Link](https://vi.aliexpress.com/w/wholesale-banana-socket.html?spm=a2g0o.productlist.search.0)
- Barrel connector [Link] https://vi.aliexpress.com/item/1005006512605602.html?spm=a2g0o.productlist.main.1.154d72a0O2bLhC&algo_pvid=a48a2e5b-c98f-46a3-9efb-e96b140d6b7e&algo_exp_id=a48a2e5b-c98f-46a3-9efb-e96b140d6b7e-0&pdp_npi=4%40dis%21AUD%213.02%213.02%21%21%2114.53%2114.53%21%402101c80017215653450678713e0b25%2112000037482647997%21sea%21AU%212747051205%21&curPageLogUid=9XwmU1UcmZtW&utparam-url=scene%3Asearch%7Cquery_from%3A
- Soldering iron
- Quality wires rated for ~5amps
- Autodesk Fusion 360 design files 

## Steps with Assembly Pics

### Step 1: Gather All Components

Ensure all the required components are at hand before starting the assembly process. This includes checking the output voltage of the old laptop charger to confirm it meets the minimum requirement.

### Step 2.1: Design the Enclosure in Autodesk Fusion 360

I designed the enclosure in Autodesk Fusion 360. The sketch of the enclosure is shown below.

![Enclosure Sketch](images/power-supply/sketch.JPG)

The enclosure is a simple box with a lid. The lid has a hole for the power supply module and a hole for the barrel connector. The enclosure will be 3D printed in PLA. The enclosure looks like this once it has been designed.

![Enclosure Design](images/power-supply/model.JPG)

### Step 2.2: 3D Print the Enclosure

Using the Autodesk Fusion 360 design files provided, print the enclosure for your power supply. This will house all the components securely.

![3D Printed Enclosure](images/power-supply/box.jpg)

### Step 3: Prepare the Laptop Charger

If the laptop charger does not have the same barrel connector as the socket you purchased, you can splice the cable and only connect the power cables to the barrel jack. I used a 2.1mm barrel jack and socket, purchased from [Jaycar](https://www.jaycar.com.au)

### Step 4: Assemble the Power Supply Module

Connect the Programmable Constant Voltage Current Step-down Power Supply Module WZ3605E to the barrel connector attached to the back of the lid (see the STL files linked above). Ensure proper connections for input and output voltages to the module. The module I have linked has simple screw in connectors.

Inputs (6V-36V) - Connected from the barrel connector to the modules input terminals.

Outputs (6V-36V) - Connected to the banana socket terminals (which we will use as our outputs)

![Assembled Power Supply](images/power-supply/assemble.jpg)

### Step 5: Install Banana Plug Socket and Barrel Connector

Solder the banana plug socket and barrel connector to the output terminals of the power supply module. These will serve as the connection points for your devices.

![2.1mm Barrel Connector](images/power-supply/barrel.jpg)

### Step 6: Final Assembly and Testing

Place all assembled components inside the 3D printed enclosure. Connect the wires appropriately, ensuring safety and functionality. Test the power supply with a multimeter to confirm the output voltage range.

![Final Assembly](images/power-supply/finished.jpg)

## Conclusion

Creating a variable power supply from an old laptop charger is not only cost-effective but also environmentally friendly. By following the steps outlined in this guide, you've successfully repurposed an old electronic device into a useful tool for your projects. Remember, safety first when dealing with electronics. Enjoy experimenting with your new variable power supply!

---
]]></description>
</item>
<item>
<title>Organise Videos By Resolution</title>
<link>https://intothevoid.github.io/organise-videos-by-resolution-2024-05-18.html</link>
<pubDate>Sat, 18 May 2024 21:40:39 +0930</pubDate>
<description><![CDATA[

Recently I was clearing out some old movie rips that I had created from VideoCDs and DVDs many years ago. I wanted a quick and dirty way to organise these i.e. get rid of the videos of resolutions lower than 720p (1280x720)

With LLMs all the rage at the moment, all it took is a few prompts and corrections and in 15 minutes I had this script ready to go -


```python
import os
import shutil
from moviepy.editor import VideoFileClip

def analyze_and_move_folders(parent_folder):
    # Create the 'inferior' folder if it doesn't exist
    inferior_folder = os.path.join(parent_folder, 'inferior')
    if not os.path.exists(inferior_folder):
        os.makedirs(inferior_folder)

    # Walk through all subdirectories
    for root, dirs, files in os.walk(parent_folder):
        # Check if the current directory contains any video files
        video_files_found = False
        for file in files:
            if file.endswith(('.mkv', '.avi', '.mp4')):
                video_files_found = True
                break
        
        if video_files_found:
            # Load the first video file to check its resolution
            first_video_file = next((f for f in files if f.endswith(('.mkv', '.avi', '.mp4'))), None)
            if first_video_file:
                file_path = os.path.join(root, first_video_file)
                try:
                    clip = VideoFileClip(file_path)
                    
                    # Check if the video resolution is less than 720p
                    if clip.size[0] < 1280 or clip.size[1] < 720:
                        # Use shutil.move() instead of os.rename()
                        shutil.move(root, os.path.join(inferior_folder, os.path.basename(root)))
                        print(f'Moved folder {root} to inferior due to low resolution.')
                except UnicodeDecodeError as e:
                    print(f"Skipping {file_path} due to decoding error: {e}")
                    continue  # Skip this file and continue with the next one

# Specify the path to the parent folder
parent_folder = '/path/to/your/folder'
analyze_and_move_folders(parent_folder)

```

The script uses the moviepy library to analyse movie files. The script works in the following manner -

1. Scan all sub-folders within the root folder for movie files i.e. mkv, avi, and mp4 files
2. Once a movie file is encountered, analyse this file using moviepy
3. Find  out the resolution of the movie
4. If the resolution of the movie is < 1280p horizontally or < 720p vertically, move the file to a folder called 'inferior'
5. Repeat above steps until all sub-folders within the parent folder have been scanned

That's it. The script saved me time and effort from having to load each movie in a video player before deleting it.
]]></description>
</item>
<item>
<title>An introduction to the OpenAI API using Python</title>
<link>https://intothevoid.github.io/an-introduction-to-the-openai-api-using-python-2023-04-19.html</link>
<pubDate>Wed, 19 Apr 2023 22:31:18 +0930</pubDate>
<description><![CDATA[
## Introduction

Ever wanted to play chess against an AI? With the OpenAI API, you can! This blog post will show you how to use Python to interface with the OpenAI API, send and receive requests, and even play a game of chess against an AI opponent. We'll be using the FEN representation of the chess board to communicate game states with the API.

## Setting Up

Before we start, make sure you have the following:

* Python installed on your machine (preferably Python 3.6 or later)
* A valid OpenAI API key (you can sign up at https://beta.openai.com/signup/)

To install the OpenAI package, run the following command:

```bash
pip install openai
```

Now, let's import the required libraries and set up the API key:
    
```python
import openai
import os

openai.api_key = os.environ["OPENAI_API_KEY"]
```

Make sure to replace "OPENAI_API_KEY" with your actual API key or set it as an environment variable.

## Interacting with the OpenAI API

Now that we have the API key set up, let's create a function to send a prompt to the OpenAI API and receive a response. The function will receive the current FEN representation of the chess board and return the next FEN representation after the AI's move.

```python
def get_next_move(fen):
    prompt = f"Given the chess position in FEN notation: {fen}, what is the best move? Please provide the resulting FEN representation after the move."
    
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        max_tokens=100,
        n=1,
        stop=None,
        temperature=0.5,
    )
    
    return response.choices[0].text.strip()
```

This function sends a POST request to the OpenAI API with the current FEN representation and receives the next FEN representation after the AI's move.

## Playing Chess

Now let's create a simple chess game loop that takes user input and sends it to the API for processing. We'll be using the python-chess library to handle the FEN representation and validate moves. You can install it with:

```bash
pip install python-chess
```

Here's the code to set up a simple chess game:

```python
import chess

board = chess.Board()

while not board.is_game_over():
    print(board)
    if board.turn == chess.WHITE:
        move = input("Your move (in UCI format): ")
        try:
            board.push_san(move)
        except ValueError:
            print("Invalid move, try again.")
            continue
    else:
        print("AI is thinking...")
        current_fen = board.fen()
        next_fen = get_next_move(current_fen)
        try:
            board.set_fen(next_fen)
        except ValueError:
            print("Error: AI provided an invalid FEN.")
            break
```

This code sets up a chess board and enters a game loop where the user (White) enters moves in UCI format, and the AI (Black) calculates its move using the OpenAI API.

## Conclusion

And there you have it! You've created a simple chess game using the OpenAI API and Python. ]]></description>
</item>
<item>
<title>Type safety in Python with Pydantic</title>
<link>https://intothevoid.github.io/type-safety-in-python-with-pydantic-2023-04-10.html</link>
<pubDate>Mon, 10 Apr 2023 16:56:00 +0930</pubDate>
<description><![CDATA[

## Introduction to Pydantic

Pydantic is a Python library that provides data validation and settings management, using Python type annotations. It is designed to make it easy to define and validate data models, allowing you to catch errors and handle them gracefully, making your code more robust and easier to maintain.

Pydantic is particularly useful for handling data that is passed between different parts of a system, such as between a front-end web form and a back-end API, or between different microservices. By defining a clear schema for the data, you can ensure that it is consistent and valid, regardless of where it comes from.

## Features of Pydantic

One of the key features of Pydantic is the use of Python type annotations. These annotations provide a way to specify the expected type of a variable or argument, which can be used to validate the data and catch errors early in the development process. Pydantic also provides a number of built-in validators, such as "email" and "url", that can be used to ensure that data conforms to specific formats.

To define a Pydantic model, you simply create a class that inherits from the BaseModel class. In this class, you define the fields that make up the model, using Python type annotations to specify the expected types. You can also specify default values and validation rules for each field.

## Example

```python
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

```
In this model, we have defined four fields: id, name, email, and password. The id field is expected to be an integer, while the other fields are expected to be strings.

To create an instance of the User model, we simply pass in the data as a dictionary:

```python
data = {
    "id": 1,
    "name": "Alice",
    "email": "alice@example.com",
    "password": "secretpassword",
}

user = User(**data)

```
Pydantic will automatically validate the data and ensure that it conforms to the expected types and validation rules. If any errors are found, Pydantic will raise a ValidationError exception, which includes details about the error and the field that caused it.

Pydantic also provides a number of other features, such as support for custom validation functions, automatic conversion of data types, and support for parsing and serializing data to and from JSON.

Pydantic is a powerful library that provides an easy and efficient way to validate data and handle errors in your Python applications. By using Pydantic to define and validate your data models, you can catch errors early in the development process and ensure that your code is more robust and easier to maintain.

## Validation in Pydantic

In Pydantic, to get error details, you need to use a try/except block. The error type will be pydantic.error_wrappers.ValidationError.

Here is an example:

```python
from pydantic import BaseModel, ValidationError

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

try:

    data = {
        "id": 1,
        "name": "Alice",
        "email": "alice@example.com",
        "password": "secret",
        }

    user = User(**data)

except ValidationError as e:
    print(e.json())
``` 

## Custom Validation

Pydantic provides a number of built-in validators, such as "email" and "url", that can be used to ensure that data conforms to specific formats. However, you can also define your own custom validation functions, which can be used to validate data in any way you like.

To define a custom validation function, you simply create a function that accepts a single argument, which will be the value of the field being validated. The function should raise a ValueError if the value is invalid, or return the value if it is valid.

Here is an example of a custom validation function that checks whether a string is a valid email address:

```python
from pydantic import BaseModel, ValidationError, validator
from email_validator import validate_email, EmailNotValidError

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

    @validator("email")
    def email_validator(cls, v):
        try:
            validate_email(v)
        except EmailNotValidError as e:
            raise ValueError("Invalid email address")
        return v

try:
    user = User(**data)
    pprint(user)

except ValidationError as e:
    print(e.json())
```

## Conclusion

In this article, we have looked at Pydantic, a Python library that provides data validation and settings management, using Python type annotations. We have also looked at some of the features of Pydantic, including the use of Python type annotations to specify the expected types of fields, and the use of custom validation functions to validate data in any way you like.

Pydantic is a powerful library that provides an easy and efficient way to validate data and handle errors in your Python applications. By using Pydantic to define and validate your data models, you can catch errors early in the development process and ensure that your code is more robust and easier to maintain.

## References

- [Pydantic](https://pydantic-docs.helpmanual.io/)]]></description>
</item>
<item>
<title>NoSQL Databases - MongoDB</title>
<link>https://intothevoid.github.io/nosql-databases---mongodb-2022-12-29.html</link>
<pubDate>Fri, 30 Dec 2022 09:51:13 +1030</pubDate>
<description><![CDATA[

NoSQL databases are a type of database that is designed to handle large amounts of data that is distributed across a large number of servers. NoSQL databases are particularly well-suited for handling unstructured data, such as text, images, and videos, and for handling data that is generated by web and mobile applications.

There are several different types of NoSQL databases, including:

- Document databases: These databases store data in the form of documents, which are similar to JSON objects. Document databases are designed to be flexible and scalable, and they are often used for storing large amounts of data that is not well-suited to the tabular structure of a traditional relational database. Examples of document databases include MongoDB, Apache Cassandra, Couchbase, and Amazon DocumentDB.
- Key-value stores: These databases store data as a collection of keys and values. Key-value stores are very fast and scalable, but they do not offer the same level of querying and indexing capabilities as other types of NoSQL databases. Examples of key-value stores include Redis and DynamoDB.
- Column-family databases: These databases store data as a collection of columns, rather than rows. Column-family databases are highly scalable and are often used for storing large amounts of data that needs to be accessed and processed quickly. Examples of column-family databases include Apache Cassandra and Google BigTable.
- Graph databases: These databases store data as a network of nodes and edges, which can be used to represent complex relationships between data items. Graph databases are often used for storing and querying data that has complex relationships, such as social networks or recommendation systems. Examples of graph databases include Neo4j and TigerGraph.

One of the main benefits of NoSQL databases is their ability to scale horizontally, meaning that they can easily add more servers to the database cluster as the amount of data or number of users increases. This makes them well-suited for handling the high volume of data and traffic that is common in modern web and mobile applications. NoSQL databases are also generally easier to set up and maintain than traditional relational databases, which can require more complex schema design and administration.

However, NoSQL databases do have some disadvantages compared to traditional relational databases. One of the main limitations of NoSQL databases is that they do not offer the same level of querying and indexing capabilities as relational databases. This can make it more difficult to perform complex queries and analysis on the data, and it can also make it more difficult to enforce data integrity and consistency. NoSQL databases are also generally not as good at handling transactions and ACID (atomic, consistent, isolated, and durable) guarantees as relational databases.

Despite these limitations, NoSQL databases are a popular choice for many modern applications, and they have a strong following among developers. If you're interested in using a NoSQL database in your project, one option to consider is MongoDB. MongoDB is a popular open-source document database that is widely used for storing and accessing data in web and mobile applications. Here's some sample Go code that demonstrates how to connect to a MongoDB database and insert a document:

```go
package main

import (
	"context"
	"fmt"
	"log"

	"go.mongodb.org/mongo-driver/mongo"
	"go.mongodb.org/mongo-driver/mongo/options"
)

func main() {
	// Set up a connection to the MongoDB server
	client, err := mongo.NewClient(options.Client().ApplyURI("mongodb://localhost:27017"))

if err != nil { log.Fatal(err) } err = client.Connect(context.TODO()) if err != nil { log.Fatal(err) }

// Choose the database and collection to use
collection := client.Database("test").Collection("people")

// Insert a new document
doc := map[string]interface{}{
	"name": "John Smith",
	"age":  30,
}
result, err := collection.InsertOne(context.TODO(), doc)
if err != nil {
	log.Fatal(err)
}
fmt.Println("Inserted document", result.InsertedID)
}

// Choose the database and collection to use
collection := client.Database("test").Collection("people")

// Insert a new document
doc := map[string]interface{}{
	"name": "John Smith",
	"age":  30,
}
result, err := collection.InsertOne(context.TODO(), doc)
if err != nil {
	log.Fatal(err)
}
fmt.Println("Inserted document", result.InsertedID)

```

NoSQL databases are a powerful tool for handling large amounts of unstructured data and for building scalable web and mobile applications. While they do have some limitations compared to traditional relational databases, they are still a popular choice for many developers due to their ease of use and ability to scale. If you're considering using a NoSQL database in your project, be sure to carefully weigh the pros and cons and choose the database that is best suited to your needs.
]]></description>
</item>
<item>
<title>Levenshtein Distance</title>
<link>https://intothevoid.github.io/levenshtein-distance-2022-12-25.html</link>
<pubDate>Mon, 26 Dec 2022 00:04:51 +1030</pubDate>
<description><![CDATA[
Levenshtein distance is a measure of the similarity between two strings, calculated as the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into the other. Here is an example of how you can calculate the Levenshtein distance between two strings in Go:

```Go
package main

import (
	"fmt"
	"math"
)

func main() {
	s1 := "kitten"
	s2 := "sitting"

	distance := levenshteinDistance(s1, s2)
	fmt.Printf("The Levenshtein distance between %s and %s is %d\n", s1, s2, distance)
}

func levenshteinDistance(s1, s2 string) int {
	// Convert strings to rune slices for Unicode support
	r1 := []rune(s1)
	r2 := []rune(s2)

	// Initialize a two-dimensional matrix with all values set to zero
	matrix := make([][]int, len(r1)+1)
	for i := range matrix {
		matrix[i] = make([]int, len(r2)+1)
	}

	// Set the initial values of the first row and column of the matrix
	for i := 1; i <= len(r1); i++ {
		matrix[i][0] = i
	}
	for j := 1; j <= len(r2); j++ {
		matrix[0][j] = j
	}

	// Calculate the Levenshtein distance using the dynamic programming algorithm
	for i := 1; i <= len(r1); i++ {
		for j := 1; j <= len(r2); j++ {
			cost := 0
			if r1[i-1] != r2[j-1] {
				cost = 1
			}
			matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+cost)
		}
	}

	// Return the final value in the bottom-right corner of the matrix
	return matrix[len(r1)][len(r2)]
}

func min(a, b, c int) int {
	// Return the minimum of three integers
	return int(math.Min(float64(a), math.Min(float64(b), float64(c))))
}
```
This code defines a function `levenshteinDistance` that takes in two strings as input and returns an integer representing the Levenshtein distance between them. The function first converts the input strings to slices of runes (Unicode characters) to support Unicode characters. It then initializes a two-dimensional matrix with all values set to zero, and sets the initial values of the first row and column of the matrix to the lengths of the input strings.

The function then uses a dynamic programming algorithm to calculate the Levenshtein distance. It iterates over the elements of the input strings and compares them, adding a cost of 1 if they are different and 0 if they are the same. It then calculates the minimum of the three values in the matrix: the value above, the value to the left, and the value to the upper-left (diagonal) of the current position. This value is then added to the current position in the matrix.

Finally, the function returns the final value in the bottom-right corner of the matrix, which represents the Levenshtein distance between the input strings.

Here is an example of how you can use this function:
```go
s1 := "kitten"
s2 := "sitting"
distance := levenshteinDistance(s1, s2)
fmt.Printf("The Levenshtein distance between %s and %s is %d\n", s1, s2, distance)
```

This will output the following:
```bash
The Levenshtein distance between kitten and sitting is 3
```


]]></description>
</item>
<item>
<title>Spotify Playlist Backups using Python</title>
<link>https://intothevoid.github.io/spotify-playlist-backups-using-python-2022-12-11.html</link>
<pubDate>Sun, 11 Dec 2022 11:43:47 +1030</pubDate>
<description><![CDATA[

To create a web application that backs up your Spotify playlists as a JSON file, you will need to do the following:

1.  First, you will need to install the `spotipy` library, which provides a Python interface for the Spotify Web API. You can do this by running the following command:
```bash
pip install spotipy 
```
2. Next, you will need to create a Spotify app and obtain a client ID and client secret for the app. You can do this by logging in to the [Spotify Developer Dashboard](https://developer.spotify.com/dashboard) and following the instructions on the website.

## Creating the Python Backend
Once you have obtained your client ID and client secret, you can use the `spotipy.Spotify` class to authenticate yourself and access the Spotify Web API. An example of how you can do this is shown below:

```python
import spotipy

client_id = "your-client-id"
client_secret = "your-client-secret"

spotify = spotipy.Spotify(
    client_id=client_id,
    client_secret=client_secret
)
```

After authenticating, you can use the `spotify.user_playlists()` method to retrieve a list of all your Spotify playlists. This method returns a paginated list of playlists, so you will need to iterate through the pages and collect the playlists from each page. Here is an example of how you can do this:

```python
playlists = []

response = spotify.user_playlists()
playlists.extend(response["items"])

while response["next"]:
    response = spotify.next(response)
    playlists.extend(response["items"])

```

Once you have collected all your playlists, you can use the `json` module to convert the list of playlists to a JSON string, and then write the JSON string to a file. Here is an example of how you can do this:

```python
import json

with open("playlists.json", "w") as file:
    json.dump(playlists, file)

```

Finally, you can create a simple Flask app that exposes a `/backup` endpoint, which accepts `POST` requests and uses the code from the previous steps to back up your playlists as a JSON file. Here is an example of how you can do this:

```python
from flask import Flask, request
import spotipy
import json

app = Flask(__name__)

client_id = "your-client-id"
client_secret = "your-client-secret"
spotify = spotipy.Spotify(
    client_id=client_id,
    client_secret=client_secret
)

@app.route("/backup", methods=["POST"])
def backup():
    playlists = []

    response = spotify.user_playlists()
    playlists.extend(response["items"])

    while response["next"]:
        response = spotify.next(response)
        playlists.extend(response["items"])

    with open("playlists.json", "w") as file:
        json.dump(playlists, file)

    return "Playlists backed up successfully!"

```

To run the Flask app, you will need to first start the Flask development server by running the following command:

```bash
export FLASK_APP=app.py
flask run

```

This will start the Flask development server and make your app available at `http://localhost:5000`. To make a `POST` request to the `/backup` endpoint, you can use a tool like `curl` or `Postman` to send a `POST` request to `http://localhost:5000/backup`.

Here is an example of how you can use `curl` to make a `POST` request:
```bash
curl -X POST http://localhost:5000/backup

```

Once you have made the request, your playlists should be backed up as a JSON file named `playlists.json` in the current directory.

Note: This code is provided for illustration purposes only, and is not intended for production use. It does not include error handling or other best practices for building a Flask app.

## Creating the Web Frontend
Here's how you could create a web page to create a nice frontend for the Python program that backs up your Spotify playlists:

```html
<!DOCTYPE html>
<html>
<head>
    <title>Spotify Playlist Backup</title>
    <style>
        /* This is some CSS that styles the page */
        body {
            font-family: sans-serif;
            text-align: center;
        }
        input, button {
            padding: 10px 20px;
            font-size: 16px;
        }
        input {
            width: 300px;
        }
        button {
            background-color: #1DB954;
            color: white;
            cursor: pointer;
        }
        pre {
            text-align: left;
            margin: 20px;
            padding: 20px;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
    </style>
</head>
<body>
    <h1>Spotify Playlist Backup</h1>
    <p>Enter your Spotify credentials and username below to backup your playlists as a JSON file.</p>
    <form>
        <input type="text" id="client_id" placeholder="Your Spotify client ID">
        <input type="text" id="client_secret" placeholder="Your Spotify client secret">
        <input type="text" id="username" placeholder="Your Spotify username">
        <button type="submit">Backup Playlists</button>
    </form>
    <pre id="output"></pre>
    <script>
        // This is the JavaScript code that runs when the page is loaded

        // This gets the form element
        const form = document.querySelector("form");

        // This adds an event listener that runs when the form is submitted
        form.addEventListener("submit", async (e) => {
            // This prevents the page from reloading
            e.preventDefault();

            // This gets the input elements
            const clientIdInput = document.querySelector("#client_id");
            const clientSecretInput = document.querySelector("#client_secret");
            const usernameInput = document.querySelector("#username");

            // This gets the values from the input elements
            const clientId = clientIdInput.value;
            const clientSecret = clientSecretInput.value;
            const username = usernameInput.value;

            // This shows a message while the playlists are being backed up
            const output = document.querySelector("#output");
            output.innerHTML = "Backing up your playlists... please wait.";

            // This sends a request to the server to backup the playlists
            const response = await fetch("/backup", {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                },
                body: JSON.stringify({
                    clientId,
                    clientSecret,
                    username,
                }),
            });

            // This gets the response from the server
            const data = await response.json();

            // This displays the response from the server
            output.innerHTML = JSON.stringify(data, null, 4);
        });
    </script>
</body>

```

## Spotify backup playlist as a standalone script

```python
import json
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

# This is your Spotify client ID and secret
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"

# This is your Spotify username
username = "YOUR_USERNAME"

# This is the path to the JSON file where your playlists will be saved
json_file = "playlists.json"

# This creates a Spotify client using your client ID and secret
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
spotify = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

# This gets your user ID
user_id = spotify.current_user()["id"]

# This gets all your playlists
playlists = spotify.user_playlists(user_id)

# This creates an empty list where your playlists will be saved
playlists_data = []

# This iterates over your playlists
for playlist in playlists["items"]:
    # This gets the playlist ID and name
    playlist_id = playlist["id"]
    playlist_name = playlist["name"]

    # This gets the tracks in the playlist
    tracks = spotify.user_playlist_tracks(user_id, playlist_id)

    # This creates an empty list where the tracks will be saved
    tracks_data = []

    # This iterates over the tracks
    for track in tracks["items"]:
        # This gets the track data
        track_data = track["track"]

        # This saves the track data
        tracks_data.append({
            "id": track_data["id"],
            "name": track_data["name"],
            "artists": [artist["name"] for artist in track_data["artists"]],
            "album": track_data["album"]["name"],
        })

	# This saves the playlist data
	playlists_data.append({
	    "id": playlist_id,
	    "name": playlist_name,
	    "tracks": tracks_data,
	})

# This saves the playlists data to the JSON file
with open(json_file, "w") as f: 
	json.dump(playlists_data, f, indent=4)

print("Successfully backed up your playlists to", json_file)
```

In this code, we use the `spotipy` library to access the Spotify API and retrieve the data for your playlists and tracks. We then save this data to a JSON file using the `json` library.

To use this program, you will need to replace the `client_id`, `client_secret`, and `username` variables with your own Spotify credentials and username. You will also need to specify the path to the JSON file where you want your playlists to be saved.

Once you have done this, you can run the program and it will retrieve your playlists and tracks and save them to the JSON file. You can then use this file to backup your playlists or to transfer them to another Spotify account.
]]></description>
</item>
<item>
<title>Transferring Files from iOS to Linux wirelessly</title>
<link>https://intothevoid.github.io/transferring-files-from-ios-to-linux-wirelessly-2022-08-05.html</link>
<pubDate>Sat, 06 Aug 2022 09:29:40 +0930</pubDate>
<description><![CDATA[

## Overview
I was an Android user for a long period of time, right from the Jellybean and 
Gingerbread days. After almost a decade being with Android I finally moved to
iOS, because I was sick of the fragmentation and I wish Android took privacy 
as seriously as iOS does. One of the most common things I need to do is transfer
files across from my iOS device to my Linux laptop. One way of doing this is to 
connect my laptop and iPhone with a lightning cable, however it is super
inconvenient and you don't always have a cable lying around. You can email 
the file to yourself but thats a really roundabout way to do it. 

With an Android device, things are easier as Google allows apps on the playstore
that have built inservers. iOS does not allow such apps. However, what iOS does
support is being able to connect to other servers like an ssh server.

If you have a Macbook and an iOS device, a wireless transfer is very easy - 
Airdrop. However, with Linux, you're out of luck. The only reliable way to
transfer files to a Linux laptop is via sftp or ftp over ssh.

## Install ssh server on Linux
In this part of the post, I'll show you how to ==install== an SSH server on Fedora Linux. 

First, we'll need to install the openssh-server package:
```bash
sudo dnf install openssh-server
```

Once the package is installed, we'll need to start the SSH service:
```bash
sudo systemctl enable sshd # ensure sshd starts up after reboot
sudo systemctl start sshd # starts the ssh daemon
sudo systemctl status sshd # check status of service
```

After you have installed the ssh server on your laptop move on to the next part
of this post. You will need your iOS device - iPhone or iPad for this.

## Install FE File Explorer on your iOS device
FE File Explorer is an app which lets you connect to servers from your iOS device.
It can connect to a variety of sources including NAS, FTP, SAMBA etc.

It is a free application that also has a paid version but even the free version
has a lot of built in functionality and it will suit the needs of most users.

Once you launch the app, hit the + button on the top right hand corner and the 
following screen will be shown -

![FE File Explorer New Window](/images/fexplorer/fexplorer_add.jpg "FE File Explorer New Window")
<p class="subtitle">FE File Explorer New Window</p>

Select 'SFTP'. If 'SFTP' is not available you should be able to select 'FTP' although
I have not tested it with this option.

Enter details in the next screen. The 'Hostname' (IP Address), 'Username' and 'Password'
are the most important fields. You can check the IP address of the target computer
using the following command -
```bash
ip a
```

The username and password are the account details you use on your Linux computer.

If you followed the instructions correctly, you should now see an entry for your
connection at the main screen of the app. Once you tap it, you should be able
to view all the files and folders of your Linux computer.

![FE File Explorer Connection Added](/images/fexplorer/fexplorer_main.jpg "FE File Explorer Connection Added")
<p class="subtitle">FE File Explorer Connection Added</p>

Transferring files is easy, you can simply hit the 3 dots for a File context menu 
as shown below -

![FE File Explorer Popup Menu](/images/fexplorer/fexplorer_filepopup.jpg "FE File Explorer Popup Menu")
<p class="subtitle">FE File Explorer Popup Menu</p>

This seems like a lot of steps but once you have set things up its like using
any other app on your phone.
]]></description>
</item>
<item>
<title>Setup Vim (Astro Vim) on a Macbook</title>
<link>https://intothevoid.github.io/setup-vim-astro-vim-on-a-macbook-2022-07-31.html</link>
<pubDate>Sun, 31 Jul 2022 11:53:48 +0930</pubDate>
<description><![CDATA[

## Overview

Use the following steps to have a decent vim installation relatively quickly on a Macbook with a M1 / M2 chipset

## Brew installation

Brew is a package manager for MacOS which makes it very easy to install packages. I highly recommend this package manager on MacOS. Its worth the effort to get it installed you will thank yourself later on.

From www.brew.sh -

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

Sample command using brew -

```bash
brew install wget
```

## Install NeoVim

Neovim is our base vim install. Install Neovim using the command -

```bash
brew install neovim
```
## Install Astro Vim

AstroNvim is an aesthetic and feature-rich neovim config that is extensible and easy to use with a great set of plugins. Install AstroNvim by using the commands

```bash
git clone https://github.com/AstroNvim/AstroNvim ~/.config/nvim

nvim +PackerSync
```

## Setup Astro Vim

Make a backup of your existing nvim configuration -

```bash
mv ~/.config/nvim ~/.config/nvimbackup
```

#### Install LSP

Enter `:LspInstall` followed by the name of the server you want to install  
Example: `:LspInstall pyright`

#### Install language parser

Enter `:TSInstall` followed by the name of the language you want to install  
Example: `:TSInstall python`

#### Manage plugins

Run `:PackerClean` to remove any disabled or unused plugins  
Run `:PackerSync` to update and clean plugins

#### Update AstroNvim

Run `:AstroUpdate` to get the latest updates from the repository

#### Install NerdFonts

Follow this step to install your desired fonts and so font glyphs are rendered correctly

Firacode is my current favourite ♥️

Fonts can be downloaded from https://www.nerdfonts.com/font-downloads

#### Optional requirements

These are other optional packages which can be installed -

[ripgrep](https://github.com/BurntSushi/ripgrep "ripgrep") - live grep telescope search (leader + fw)

[lazygit](https://github.com/jesseduffield/lazygit "lazygit") - git ui toggle terminal (leader + tl or leader + gg)

[NCDU](https://dev.yorhel.nl/ncdu "NCDU") - disk usage toggle terminal (leader + tu)

[Htop](https://htop.dev/ "Htop") - process viewer toggle terminal (leader + tt)

[Python](https://www.python.org/ "Python") - python repl toggle terminal (leader + tp)

[Node](https://nodejs.org/en/ "Node") - node repl toggle terminal (leader + tn)


]]></description>
</item>
<item>
<title>Create a blog using Hugo and Github Pages</title>
<link>https://intothevoid.github.io/create-a-blog-using-hugo-and-github-pages-2021-04-05.html</link>
<pubDate>Mon, 05 Apr 2021 14:13:49 +0930</pubDate>
<description><![CDATA[
Update: Added section on adding Github Action for Hugo build

This is a guide on how you can create a blog or website from scratch using Hugo and Github. This is also my first post on this blog, which has been created using the steps mentioned in this article using Hugo and Github Pages.

It is quite easy to create a blog using Wordpress or some other CMS but the simplicity of Hugo is what drove me to it. Hugo is a static html generator. This means, you write posts using Markdown and templates using any text editor. Hugo then processes these files and generates a bunch of html and css. 

This has the benefit of making it extremely easy to deploy to just about any cloud service or provider. This is where Github Pages comes in. Github Pages is free and if you have a Github account, you are already well on your way.

I used a Macbook Air M1 for creating this blog and the steps outlined in this article should be more or less the same whether you use Windows / MacOS or Linux. Lets get our elbows greasy.

### Create Repos
We will begin by creating 2 repositories in our Github account -
1. Base repo 'blog' for hosting our posts / source code / files created by Hugo
2. Blog hosting repo 'username.github.io' for hosting the actual blog, which consists of generated html and css

Create the base repo on Github by clicking the '+' button on your [Github page](https://www.github.com). I've decided to call my base repo 'blog'. Clone this repository to your machine (even if it doesn't contain any files at the moment)
```bash
git clone git@github.com:username/blog.git
cd blog/
```

From the Github website, click the '+' button again to create a new repository called 'username.github.io'. Replace username with your Github user name.

Add this second repo (which we will use for hosting our blog) as a submodule of our base repo -
```bash
git submodule add git@github.com:username/username.github.io.git public
```
The above command will add the hosting repo as a submodule into the 'public' folder.

### Install Hugo
Download the Hugo binaries from the [Hugo Releases](https://github.com/gohugoio/hugo/releases) page. As I am using MacOS, I used the command -
```bash
brew install hugo
```
For detailed installation instructions, visit https://gohugo.io/getting-started/installing/
Verify Hugo installed correctly by 
```bash
hugo version
```

### Create a new site
```bash
hugo create new site myblogname
```
The above command will create a basic website with enough boilerplate to let us get started.

### Add a theme
Visit https://themes.gohugo.io/ for a collection of available Hugo themes. Once you have decided which theme you wish to use for your website / blog, 
```bash
cd myblogname
git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke
```
Your folder structure should now look like
**~/blog/myblogname/config.toml** | Configuration File
**~/blog/myblogname/themes/ananke/** | Theme folder
**~/blog/myblogname/public** | Second hosting repo

### Update config.toml 
```toml
theme = "ananke"
title = "Karan Kadam"
baseURL = "https://username.github.io"
theme = "ananke"
themesDir = "./themes"
```

### Create your first post
```bash
hugo create new posts/my-first-post.md
``` 

### Build your blog
```bash
hugo -t "ananke"
```
This will generate all the html and styling required for your blog and publish them under the "public" folder. The -t flag tells hugo to use your selected theme.

### Local test
```bash
hugo server --bind 0.0.0.0
```
This will launch the built in webserver and your blog will be available locally. Goto a webbrowser and access http://localhost:1313 and you should see your blog, styled according to the theme you chose and containing your first sample post.

### Deploy your website to Github Pages
To be able to share your website with the rest of the world, you will need to host it somewhere. We chose the repository at 'https://username.github.io'. Check in all your changes and push them to the username.github.io -
```bash
cd ~/blog/myblogname/public
git status
git add -A
git commit -m "initial test commit" # commit contents of ~/blog/myblogname/public
git push
cd ../..
git add -A
git commit -m "initial test commit" # commit contents of ~/blog/
```
If it all went well, accessing http://username.github.io should take you to your freshly baked blog. Here on, all you need to do is,
```bash
hugo create new posts/postname.md
hugo -t "ananke"
```

### Markdown resources
It is a good idea to read up and familiarise yourself with Markdown Syntax. It is quite compact and can be learned in a single sitting. I would recommend starting with [The Markdown Cheatsheet](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf) to quickly understand how to write posts in any editor using the Markdown syntax. Keep writing! :metal:

### Bonus - Automate the Hugo build
To automate your flow even further, you can create a Github Action, which will automatically run Hugo for you and publish to your username.github.io repository for you. This will save you any manual work of building your blog each time and pushing back to your public repo.
The added benefit of this approach is you no longer need access to the cli. You can log into your Github base repo from your phone, add Mardown posts from the WebUI and your Github Actions pipeline will take care of the rest.
Begin by adding a GitHub action file to your base repo -
```bash
vim ./github/workflows/main.yml
```
Here is a what my main.yml file looks like -
```yaml
name: CI
on: push
jobs:
  deploy:
    runs-on: ubuntu-18.04
    steps:
      - name: Git checkout
        uses: actions/checkout@v2

      - name: Update theme
        run: git submodule update --init --recursive

      - name: Setup hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: "0.82.0"

      - name: Build
        run: cd username && hugo -t ananke && pwd && ls -a ./public

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}
          external_repository: username/username.github.io
          publish_dir: ./intothevoid/public
          user_name: username
          user_email: username@email.com
          publish_branch: main
```
Replace values with your own where needed.

A quick explanation of the above Github Action is given below -
1. Checkout - checkout the base repo
2. Update - Update your theme which was added as a submodule
3. Setup Hugo - Call an external action which basically installs Hugo
4. Build - Build the website by calling **hugo -t ananke**
5. Deploy - Publish the built files to your external public repo username.github.io

**Note:** There is a bit of setup involved where you need to generate your token (indicated by deploy key above). I would highly recommend visiting https://github.com/peaceiris/actions-gh-pages for details of the external action and how to setup your token.

This should get you at a stage where you can simply push a post written in Markdown format to your base repo and the rest should happen automatically. Post --> Commit Base Repo --> Github Action --> Build Blog --> Publish Static HTML.
]]></description>
</item>
</channel>
</rss>