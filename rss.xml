<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
<title>Into The Void</title>
<link>https://intothevoid.github.io</link>
<description>Programming, technology, electronics, and other random thoughts</description>
<item>
<title>Elden Ring: Nightreign DB</title>
<link>https://intothevoid.github.io/elden-ring-nightreign-db-2026-02-02.html</link>
<pubDate>Mon, 02 Feb 2026 14:08:15 +1030</pubDate>
<description><![CDATA[

## Introduction

In this post, we'll walk through building a web application: a searchable database for the game Elden Ring: Nightreign. This project demonstrates modern web development practices, from Excel data parsing to Docker deployment. 

**What we'll build:**
- A React-based single-page application (SPA)
- Excel file parsing and dynamic search functionality
- Category-based filtering with custom icons
- Expandable data cards with smart field display
- Fully responsive mobile-first design
- Production-ready Docker deployment with nginx

**Tech Stack:**
- Frontend: React 18 + Vite
- Styling: Tailwind CSS
- Data: SheetJS (xlsx) for Excel parsing
- Icons: Lucide React
- Deployment: Docker + nginx

**Live Features:**
- Real-time search across all game items, effects, and stats
- Category filtering (Weapons, Talismans, Relics, etc.)
- Expandable cards showing detailed information
- Mobile-responsive design
- Fast load times with optimized builds

---

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Project Structure](#project-structure)
3. [Data Layer: Excel Parsing](#data-layer-excel-parsing)
4. [Search Engine Implementation](#search-engine-implementation)
5. [Component Architecture](#component-architecture)
6. [State Management & Data Flow](#state-management--data-flow)
7. [Responsive Design Patterns](#responsive-design-patterns)
8. [Docker Deployment](#docker-deployment)
9. [Performance Optimizations](#performance-optimizations)
10. [Lessons Learned](#lessons-learned)

---

## Architecture Overview

### High-Level Architecture

```mermaid
graph TB
    subgraph Browser["User Browser"]
        subgraph UI["UI Layer"]
            Header["Header Component"]
            SearchBar["SearchBar Component"]
            CategoryFilter["Category Filter"]
        end

        subgraph App["Main App Component"]
            AppState["Global State Management<br/>- Search Query<br/>- Active Category<br/>- Filtered Results"]
        end

        subgraph Results["Results Display"]
            DataCardGrid["DataCard Grid<br/>- Dynamic rendering<br/>- Expandable cards<br/>- Smart field display"]
        end

        subgraph DataLayer["Data Layer"]
            Hook["useExcelData Hook"]
            Parser["ExcelParser Utility"]
            Search["SearchEngine Utility"]
        end
    end

    subgraph Assets["Static Assets"]
        Excel["nightreign-data.xlsx"]
        Images["Character Images"]
    end

    SearchBar --> AppState
    CategoryFilter --> AppState
    AppState --> DataCardGrid

    App --> Hook
    Hook --> Parser
    Parser --> Search
    Search --> AppState

    Parser -.loads.-> Excel
    Header -.loads.-> Images

    style Browser fill:#1a1a1a,stroke:#666,stroke-width:2px,color:#fff
    style DataLayer fill:#2a2a2a,stroke:#888,stroke-width:2px,color:#fff
    style Assets fill:#3a3a3a,stroke:#aaa,stroke-width:2px,color:#fff
```

### Why This Architecture?

1. **Separation of Concerns**: Components handle UI, hooks manage state/data fetching, utilities handle business logic
2. **Reusability**: Each component is self-contained and can be reused or tested independently
3. **Performance**: Client-side Excel parsing means no backend needed, reducing infrastructure costs
4. **Scalability**: Easy to add new data sources or features without refactoring core logic

---

## Project Structure

```
nightreign-dashboard/
├── public/                          # Static assets served directly
│   ├── duchess.png                  # Logo 
│   └── nightreign-data.xlsx         # Game data (Excel format)
│
├── src/
│   ├── components/                  # React components
│   │   ├── DataCard.jsx            # Individual result card with expand/collapse
│   │   ├── SearchBar.jsx           # Search input with clear button
│   │   ├── CategoryFilter.jsx      # Category filtering buttons
│   │   └── Header.jsx              # App header with branding
│   │
│   ├── hooks/                       # Custom React hooks
│   │   └── useExcelData.js         # Hook for loading & caching Excel data
│   │
│   ├── utils/                       # Business logic utilities
│   │   ├── excelParser.js          # Excel file parsing with SheetJS
│   │   └── searchEngine.js         # Search & categorization logic
│   │
│   ├── styles/
│   │   └── index.css               # Tailwind imports + custom styles
│   │
│   ├── App.jsx                      # Main application component
│   └── main.jsx                     # Application entry point
│
├── Dockerfile                       # Multi-stage Docker build
├── docker-compose.yml               # Container orchestration
├── nginx.conf                       # Web server configuration
├── vite.config.js                  # Build tool configuration
├── tailwind.config.js              # Styling configuration
└── package.json                    # Dependencies & scripts
```

### Key Design Decisions

**Why Vite over Create React App?**
- Faster development server startup (uses native ES modules)
- Optimized production builds with Rollup
- Better hot module replacement (HMR) experience
- Smaller bundle sizes out of the box

**Why Tailwind CSS?**
- Utility-first approach reduces CSS file size
- Built-in responsive design utilities
- No context switching between HTML and CSS files
- Easy to maintain consistent design system

**Why SheetJS for data?**
- Client-side parsing = no backend needed
- Familiar Excel format for non-technical data updates
- Rich ecosystem and good documentation
- Handles complex multi-sheet workbooks

---

## Data Layer: Excel Parsing

### The Challenge

I've been a huge fan of this difficult and deep game. There's a lot of scattered information about it all over the internet but I wanted to organise and have all this information in one place.

I first came across this excel sheet on Reddit created by data miners of the game -

![Nightreign Relic Stat Spreadsheet](https://steamcommunity.com/app/2622380/discussions/0/597404329477993941/)

This sheet contained a lot of information about the game, but it is frustrating to quickly find relevant information in it, especially on a mobile screen.

We have an Excel workbook with 18 sheets containing different types of game data:
- Weapon Effects
- Talisman Effects
- Relic Effects
- Dormant Powers
- Character Stats
- Boss Stats (Nightlord, Everdark Sovereign)
- Consumables
- And more...

Each sheet has different columns, and we need to parse them all into a searchable format.

### Solution: Flexible Parser

**`src/utils/excelParser.js`**

```javascript
import * as XLSX from 'xlsx';

// Sheets we want to ignore (credits, outdated data, etc.)
const IGNORED_SHEETS = [
  'Credits and Useful Links',
  'Chalices',
  'Character Stats Table (Outdated',
  'Guaranteed Relics'
];

/**
 * Loads Excel file from URL and parses all sheets
 * Returns: { sheetName: [row1, row2, ...], ... }
 */
export async function loadExcelFile(url) {
  try {
    // Fetch the Excel file
    const response = await fetch(url);
    if (!response.ok) {
      throw new Error(`Failed to fetch: ${response.statusText}`);
    }

    // Convert to ArrayBuffer (binary data)
    const arrayBuffer = await response.arrayBuffer();

    // Parse with SheetJS
    const workbook = XLSX.read(arrayBuffer, { type: 'array' });

    return parseWorkbook(workbook);
  } catch (error) {
    console.error('Error loading Excel file:', error);
    throw error;
  }
}

/**
 * Parses workbook into structured data
 */
export function parseWorkbook(workbook) {
  const data = {};

  workbook.SheetNames.forEach(sheetName => {
    // Skip ignored sheets
    if (IGNORED_SHEETS.includes(sheetName)) {
      return;
    }

    const worksheet = workbook.Sheets[sheetName];

    // Convert sheet to JSON array of objects
    // Each row becomes an object with column names as keys
    const jsonData = XLSX.utils.sheet_to_json(worksheet, {
      raw: false,  // Convert dates/numbers to strings
      defval: ''   // Default value for empty cells
    });

    // Only include sheets with data
    if (jsonData.length > 0) {
      data[sheetName] = jsonData;
    }
  });

  return data;
}
```

### Key Concepts

**1. Fetch API for File Loading**
```javascript
const response = await fetch(url);
const arrayBuffer = await response.arrayBuffer();
```
- Modern way to load files from URLs
- `arrayBuffer()` gives us raw binary data needed by SheetJS

**2. SheetJS sheet_to_json()**
```javascript
XLSX.utils.sheet_to_json(worksheet, {
  raw: false,  // Why? Ensures consistent string values
  defval: ''   // Why? Prevents null/undefined in our data
});
```
- Converts spreadsheet rows to JavaScript objects
- Column headers become object keys
- Each row becomes one object in the array

**3. Dynamic Sheet Handling**
- No hardcoded column names
- Automatically adapts to any Excel structure
- Easy to add new sheets without code changes

### Custom Hook for Data Management

**`src/hooks/useExcelData.js`**

```javascript
import { useState, useEffect } from 'react';
import { loadExcelFile } from '../utils/excelParser';

export function useExcelData(fileUrl = '/nightreign-data.xlsx') {
  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState(null);

  const loadData = async () => {
    setLoading(true);
    setError(null);

    try {
      const parsedData = await loadExcelFile(fileUrl);
      setData(parsedData);
    } catch (err) {
      setError(err.message);
    } finally {
      setLoading(false);
    }
  };

  // Load data on component mount
  useEffect(() => {
    loadData();
  }, [fileUrl]);

  return {
    data,      // Parsed Excel data
    loading,   // Loading state for UI
    error,     // Error message if load fails
    reload: loadData  // Function to reload data
  };
}
```

**Why a Custom Hook?**
- Encapsulates data loading logic
- Provides loading/error states automatically
- Reusable across components
- Follows React best practices for data fetching

**Usage in Components:**
```javascript
function App() {
  const { data, loading, error } = useExcelData();

  if (loading) return <div>Loading...</div>;
  if (error) return <div>Error: {error}</div>;
  if (!data) return <div>No data</div>;

  // Use data...
}
```

---

## Search Engine Implementation

### The Requirements

1. Search across ALL sheets and ALL columns
2. Case-insensitive partial matching
3. Category-based filtering
4. Fast enough for real-time search (as user types)
5. Return results with metadata (sheet name, category)

### Solution: Efficient Search Algorithm

**`src/utils/searchEngine.js`**

```javascript
/**
 * Categorizes sheet names into display categories
 */
export function categorizeSheet(sheetName) {
  const lower = sheetName.toLowerCase();

  // Priority order matters! More specific checks first
  if (lower === 'level up cost') return 'Levels';
  if (lower.includes('nightlord')) return 'Nightlord Stats';
  if (lower.includes('everdark') || lower.includes('sovereign'))
    return 'Everdark Sovereign Stats';

  if (lower.includes('talisman')) return 'Talismans';
  if (lower.includes('weapon')) return 'Weapons';
  if (lower.includes('dormant')) return 'Dormant Powers';
  if (lower.includes('relic')) return 'Relics';
  if (lower.includes('consumable')) return 'Consumables';
  if (lower.includes('character') && lower.includes('stat')) return 'Stats';

  return 'Other';  // Catch-all for unknown sheets
}

/**
 * Searches all sheets for matching rows
 */
export function searchAllSheets(data, query = '', categoryFilter = 'all') {
  if (!data || typeof data !== 'object') return [];

  const normalizedQuery = query.toLowerCase().trim();
  const results = [];

  // Iterate through each sheet
  Object.entries(data).forEach(([sheetName, rows]) => {
    if (!Array.isArray(rows)) return;

    const category = categorizeSheet(sheetName);

    // Skip 'Other' category completely
    if (category === 'Other') return;

    // Apply category filter
    if (categoryFilter !== 'all' && category !== categoryFilter) {
      return;
    }

    // Search through rows
    rows.forEach(row => {
      // If no query, include all items (with category filter applied)
      if (normalizedQuery === '') {
        results.push({
          ...row,
          _sheet: sheetName,
          _category: category
        });
        return;
      }

      // Check if any cell value contains the search term
      const matches = Object.entries(row).some(([key, value]) => {
        // Skip empty values
        if (value === null || value === undefined || value === '')
          return false;

        // Convert to string and search (case-insensitive)
        return String(value).toLowerCase().includes(normalizedQuery);
      });

      if (matches) {
        results.push({
          ...row,
          _sheet: sheetName,
          _category: category
        });
      }
    });
  });

  return results;
}
```

### Algorithm Analysis

**Time Complexity:** O(n × m × k)
- n = number of sheets
- m = number of rows per sheet
- k = number of columns per row

**Why This Is Fast Enough:**
1. JavaScript engines optimize string operations
2. Early returns reduce unnecessary iterations
3. Client-side means no network latency
4. Typical dataset: ~500 rows across 18 sheets = fast enough

**Performance Optimization Opportunities:**
- Add debouncing to search input (wait for user to stop typing)
- Use Web Workers for parsing/searching (non-blocking)
- Implement virtual scrolling for results (only render visible cards)
- Cache search results for repeated queries

### Category Extraction

```javascript
export function getCategories(data) {
  if (!data || typeof data !== 'object') return [];

  const categories = new Set();  // Set = no duplicates

  Object.keys(data).forEach(sheetName => {
    const category = categorizeSheet(sheetName);
    if (category !== 'Other') {
      categories.add(category);
    }
  });

  return Array.from(categories).sort();  // Alphabetical order
}
```

**Why Set?**
- Automatically removes duplicates
- O(1) lookups and insertions
- Clean API with `add()`, `has()`, etc.

---

## Component Architecture

### 1. DataCard Component

The heart of the application - displays individual items with smart field handling.

**Key Features:**
- Dynamic title selection based on sheet type
- Expandable fields (show first 8, expand for all)
- Syntax highlighting for search matches
- Category-based color coding
- Responsive design

**`src/components/DataCard.jsx`**

```javascript
import React, { useState } from 'react';
import { ChevronDown, ChevronUp, /* ...icons... */ } from 'lucide-react';

// Color schemes for each category
const CATEGORY_CONFIG = {
  'Talismans': {
    icon: Shield,
    color: 'text-blue-400 border-blue-900/30 bg-blue-900/10',
    accent: 'bg-blue-600'
  },
  'Weapons': {
    icon: Sword,
    color: 'text-red-400 border-red-900/30 bg-red-900/10',
    accent: 'bg-red-600'
  },
  // ... more categories
};

// Sheet-specific title columns
const SHEET_TITLE_COLUMN = {
  'Weapon Effects': 'Effect Description In-Game',
  'Relic Effects': 'Relic Description',
  'Dormant Powers': 'Dormant Power'
};

export function DataCard({ item, searchQuery }) {
  const [isExpanded, setIsExpanded] = useState(false);

  const category = item._category || 'Other';
  const sheetName = item._sheet || 'Unknown';
  const config = CATEGORY_CONFIG[category] || CATEGORY_CONFIG['Other'];

  // Determine title column for this sheet
  const titleColumn = SHEET_TITLE_COLUMN[sheetName];
  const title = titleColumn && item[titleColumn]
    ? item[titleColumn]
    : item.Name || 'Unknown Item';

  // Filter display keys (exclude metadata and title)
  const displayKeys = Object.keys(item).filter(key =>
    !key.startsWith('_') &&
    key !== titleColumn &&
    item[key] !== null &&
    item[key] !== undefined &&
    item[key] !== ''
  );

  const hasMoreFields = displayKeys.length > 8;
  const fieldsToShow = isExpanded ? displayKeys : displayKeys.slice(0, 8);

  return (
    <div className="card-container">
      {/* Colored accent line */}
      <div className={`accent-line ${config.accent}`} />

      <div className="card-content">
        {/* Header with title and icon */}
        <div className="card-header">
          <h3>{title}</h3>
          <span className="category-badge">{category}</span>
        </div>

        {/* Data fields */}
        <div className="fields-container">
          {fieldsToShow.map(key => (
            <div key={key} className="field-row">
              <span className="field-label">{key}</span>
              <span className="field-value">{item[key]}</span>
            </div>
          ))}

          {/* Expand/collapse button */}
          {hasMoreFields && (
            <button onClick={() => setIsExpanded(!isExpanded)}>
              {isExpanded ? (
                <>Show less <ChevronUp /></>
              ) : (
                <>+{displayKeys.length - 8} more fields <ChevronDown /></>
              )}
            </button>
          )}
        </div>
      </div>
    </div>
  );
}
```

**Design Patterns Used:**

1. **Controlled Components**: `isExpanded` state controlled by React
2. **Conditional Rendering**: Show different UI based on state
3. **Dynamic Styling**: Tailwind classes computed from data
4. **Component Composition**: Small, focused component with single responsibility

### 2. SearchBar Component

Simple but effective search input with clear functionality.

**`src/components/SearchBar.jsx`**

```javascript
export function SearchBar({ value, onChange, onClear }) {
  return (
    <div className="search-container">
      <Search className="search-icon" />
      <input
        type="text"
        placeholder="Search for effects, items, stats..."
        value={value}
        onChange={(e) => onChange(e.target.value)}
      />
      {value && (
        <button onClick={onClear}>
          <X className="clear-icon" />
        </button>
      )}
    </div>
  );
}
```

**Key Points:**
- Controlled component (parent manages state)
- Conditional clear button (only show when value exists)
- Accessible (proper labels, keyboard navigation)

### 3. CategoryFilter Component

Dynamic category buttons generated from data.

**`src/components/CategoryFilter.jsx`**

```javascript
const CATEGORY_ICONS = {
  'All': Sparkles,
  'Talismans': Shield,
  'Weapons': Sword,
  'Levels': TrendingUp,
  // ... more mappings
};

export function CategoryFilter({ categories, activeCategory, onCategoryChange }) {
  const allCategories = ['All', ...categories];

  return (
    <div className="category-filters">
      {allCategories.map(category => {
        const Icon = CATEGORY_ICONS[category] || Sparkles;
        const isActive = activeCategory === category;

        return (
          <button
            key={category}
            onClick={() => onCategoryChange(category)}
            className={isActive ? 'active' : 'inactive'}
          >
            <Icon />
            {category}
          </button>
        );
      })}
    </div>
  );
}
```

**Responsive Design:**
- `flex-wrap` allows buttons to wrap on small screens
- Touch-friendly button sizes on mobile
- Icons scale with text size

---

## State Management & Data Flow

### Application State Architecture

**`src/App.jsx`**

```javascript
import { useState, useMemo } from 'react';
import { useExcelData } from './hooks/useExcelData';
import { searchAllSheets, getCategories } from './utils/searchEngine';

function App() {
  // Load Excel data
  const { data, loading, error } = useExcelData();

  // UI state
  const [searchQuery, setSearchQuery] = useState('');
  const [activeCategory, setActiveCategory] = useState('All');

  // Derive categories from data
  const categories = useMemo(() => {
    return data ? getCategories(data) : [];
  }, [data]);

  // Perform search (memoized for performance)
  const searchResults = useMemo(() => {
    if (!data) return [];

    const categoryFilter = activeCategory === 'All'
      ? 'all'
      : activeCategory;

    return searchAllSheets(data, searchQuery, categoryFilter);
  }, [data, searchQuery, activeCategory]);

  // Event handlers
  const handleSearch = (query) => {
    setSearchQuery(query);
  };

  const handleCategoryChange = (category) => {
    setActiveCategory(category);
  };

  const handleClearSearch = () => {
    setSearchQuery('');
  };

  // Render
  return (
    <div className="app">
      <Header loading={loading} />

      <main>
        <SearchBar
          value={searchQuery}
          onChange={handleSearch}
          onClear={handleClearSearch}
        />

        <CategoryFilter
          categories={categories}
          activeCategory={activeCategory}
          onCategoryChange={handleCategoryChange}
        />

        <div className="results-grid">
          {searchResults.map((item, index) => (
            <DataCard
              key={`${item._sheet}-${index}`}
              item={item}
              searchQuery={searchQuery}
            />
          ))}
        </div>

        {searchResults.length === 0 && (
          <div className="no-results">
            No results found for "{searchQuery}"
          </div>
        )}
      </main>
    </div>
  );
}
```

### Data Flow Diagram

```mermaid
flowchart TD
    A[User Input<br/>Search/Filter] --> B[Update State<br/>useState]
    B --> C[Trigger Recalculation<br/>useMemo]
    C --> D[searchAllSheets<br/>runs]
    D --> E[Return Filtered<br/>Results]
    E --> F[React Re-renders<br/>Components]
    F --> G[Display Updated<br/>Results]

    style A fill:#4a5568,stroke:#718096,stroke-width:2px,color:#fff
    style B fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style C fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style D fill:#d97706,stroke:#f59e0b,stroke-width:2px,color:#fff
    style E fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#fff
    style F fill:#059669,stroke:#10b981,stroke-width:2px,color:#fff
    style G fill:#059669,stroke:#10b981,stroke-width:2px,color:#fff
```

### Performance: useMemo Explained

**Without useMemo:**
```javascript
// BAD: Runs on EVERY render, even unrelated changes
const searchResults = searchAllSheets(data, searchQuery, activeCategory);
```

**With useMemo:**
```javascript
// GOOD: Only runs when dependencies change
const searchResults = useMemo(() => {
  return searchAllSheets(data, searchQuery, activeCategory);
}, [data, searchQuery, activeCategory]);
```

**Why It Matters:**
- Searching 500+ rows is expensive
- React re-renders on any state change
- useMemo caches results until dependencies change
- Result: Smooth UI, no lag when typing

---

## Responsive Design Patterns

### Mobile-First Approach

This was an important requirement for me since quickly finding information in the excel workbook on a mobile screen was a major pain point for me, which led to the creation of this webapp.

Tailwind's responsive utilities work mobile-first:

```javascript
// Mobile: text-base (16px) → Desktop: md:text-lg (18px)
className="text-base md:text-lg"
```

### Key Breakpoints

```javascript
// Tailwind default breakpoints
sm: 640px   // Small tablets
md: 768px   // Tablets
lg: 1024px  // Laptops
xl: 1280px  // Desktops
```

### Common Patterns Used

**1. Responsive Spacing:**
```javascript
// Mobile: 4px, Desktop: 8px
className="mt-4 md:mt-8"
```

**2. Responsive Sizing:**
```javascript
// Header logo: 112px mobile, 160px desktop
className="h-28 w-28 md:h-40 md:w-40"
```

**3. Flex Wrapping:**
```javascript
// Single column mobile, wraps to multi-column desktop
className="flex flex-wrap gap-2"
```

**4. Text Scaling:**
```javascript
// Button text: 12px mobile, 14px desktop
className="text-xs md:text-sm"
```

### Mobile-Specific Fixes

**Problem: iOS Auto-Zoom on Input Focus**
```css
/* BAD: iOS zooms on inputs < 16px */
font-size: 14px;

/* GOOD: Prevents auto-zoom */
font-size: 16px;
```

**Solution in Code:**
```javascript
className="text-base md:text-lg"  // 16px mobile, 18px desktop
```

---

## Docker Deployment

### Multi-Stage Build

**`Dockerfile`**

```dockerfile
# Stage 1: Build
FROM node:20-alpine AS builder

WORKDIR /app

# Copy dependency files
COPY package*.json ./

# Install dependencies
RUN npm ci --only=production

# Copy source code
COPY . .

# Build production bundle
RUN npm run build

# Stage 2: Production
FROM nginx:alpine

# Copy built files from builder stage
COPY --from=builder /app/dist /usr/share/nginx/html

# Copy nginx configuration
COPY nginx.conf /etc/nginx/conf.d/default.conf

# Fix file permissions for nginx user
RUN chmod -R 755 /usr/share/nginx/html && \
    chown -R nginx:nginx /usr/share/nginx/html

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

**Why Multi-Stage Build?**
1. **Smaller Image**: Final image only includes built files, not source code or build tools
2. **Security**: No development dependencies in production
3. **Faster Deploys**: Smaller images = faster transfers
4. **Clean Separation**: Build environment isolated from runtime

**Size Comparison:**
- Single-stage: ~400MB (includes Node.js, npm, source)
- Multi-stage: ~25MB (only nginx + built files)

### nginx Configuration

**`nginx.conf`**

```nginx
server {
    listen 80;
    server_name localhost;
    root /usr/share/nginx/html;
    index index.html;

    # Include default MIME types (CRITICAL!)
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Enable CORS (for local dev/testing)
    add_header Access-Control-Allow-Origin * always;

    # SPA fallback: serve index.html for all routes
    location / {
        try_files $uri $uri/ /index.html;
    }

    # Excel files: correct content type
    location ~* \.(xlsx|xls)$ {
        add_header Content-Type application/vnd.openxmlformats-officedocument.spreadsheetml.sheet always;
        add_header Cache-Control "public, max-age=3600" always;
    }

    # Static assets: aggressive caching
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

**Key Configurations Explained:**

1. **MIME Types**: Ensures browsers know how to handle files
2. **SPA Fallback**: All routes serve index.html (React Router)
3. **Excel MIME Type**: Critical for fetch() to work correctly
4. **Caching Strategy**:
   - Excel: 1 hour (data may update)
   - Static assets: 1 year (hashed filenames)

### Docker Compose

**`docker-compose.yml`**

```yaml
version: '3.8'

services:
  nightreign-dashboard:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: nightreign-dashboard
    ports:
      - "3383:80"
    restart: unless-stopped
    environment:
      - NODE_ENV=production
```

**Usage:**
```bash
# Build and start
docker-compose up --build

# Run in background
docker-compose up -d

# Stop
docker-compose down

# View logs
docker-compose logs -f
```

---

## Performance Optimizations

### 1. Vite Build Optimizations

**`vite.config.js`**

```javascript
export default defineConfig({
  plugins: [react()],
  build: {
    outDir: 'dist',
    sourcemap: false,  // Smaller production bundle
    rollupOptions: {
      output: {
        manualChunks: {
          'vendor': ['react', 'react-dom'],  // Separate vendor chunk
          'xlsx': ['xlsx']  // Separate Excel library
        }
      }
    }
  }
});
```

**Benefits:**
- Vendor code cached separately (rarely changes)
- Parallel loading of chunks
- Better browser caching
- Faster subsequent loads

### 2. Code Splitting

Vite automatically splits:
- `main.js` - App code
- `vendor.js` - React, ReactDOM
- `xlsx.js` - SheetJS library

### 3. Image Optimization

```javascript
// Duchess character image
<img
  src="/duchess.png"
  alt="The Duchess"
  className="h-28 w-28 md:h-40 md:w-40 object-cover"
  loading="lazy"  // Lazy load if off-screen
/>
```

### 4. React Performance

**useMemo for Expensive Calculations:**
```javascript
const searchResults = useMemo(() => {
  return searchAllSheets(data, searchQuery, activeCategory);
}, [data, searchQuery, activeCategory]);
```

**Proper Key Props:**
```javascript
{results.map((item, index) => (
  <DataCard
    key={`${item._sheet}-${index}`}  // Unique key
    item={item}
  />
))}
```

### 5. Bundle Size Analysis

```bash
# Build with analysis
npm run build -- --mode=analyze

# Or manually check
ls -lh dist/assets/
```

**Current Bundle Sizes:**
- Main JS: ~45KB (gzipped)
- Vendor JS: ~140KB (gzipped) - React + dependencies
- XLSX JS: ~180KB (gzipped) - Excel parsing library
- CSS: ~12KB (gzipped)

**Total: ~377KB** - Reasonable for a data-heavy application

---

## Lessons Learned

### 1. Excel as a Data Source

**Pros:**
- Non-technical users can update data
- Familiar format for game data management
- No database setup required
- Version control with Git

**Cons:**
- Large file size (1MB+ Excel files)
- Parse time on initial load (~500ms)
- No real-time updates
- Limited query capabilities

**When to Use:**
- Small to medium datasets (< 10,000 rows)
- Infrequent updates
- Non-technical content editors
- Static data (not user-generated)

**When to Avoid:**
- Frequently changing data
- Large datasets (> 100MB)
- Need for relational queries
- User-generated content

### 2. Client-Side Search Limitations

**Works Well For:**
- < 10,000 total items
- Simple text matching
- Single-user applications

**Breaks Down At:**
- > 100,000 items (slow search)
- Complex queries (joins, aggregations)
- Multi-user scenarios (no shared state)

**Solution for Scale:**
- Backend search API (Elasticsearch, Algolia)
- Database with indexes
- Server-side filtering/pagination

### 3. Responsive Design Insights

**Mobile-First Is Critical:**
- 60%+ of traffic is mobile
- Easier to scale up than down
- Forces focus on essential features

**Common Mobile Issues:**
- Font size < 16px triggers zoom (iOS)
- Touch targets < 44px are hard to tap
- Horizontal scrolling is frustrating

**Testing Strategy:**
- Chrome DevTools device emulation
- Real device testing (iPhone, Android)
- Lighthouse mobile audits

### 4. Docker Deployment Gotchas

**MIME Types Are Critical:**
```nginx
# Without this, browsers download HTML as .dms files!
include /etc/nginx/mime.types;
```

**File Permissions Matter:**
```dockerfile
# nginx user needs read access
RUN chmod -R 755 /usr/share/nginx/html
```

**CORS for Local Dev:**
```nginx
# Allow localhost testing
add_header Access-Control-Allow-Origin * always;
```

### 5. Component Architecture Lessons

**Keep Components Small:**
- Single responsibility principle
- Easier to test and debug
- Better reusability

**Lift State Up:**
- Shared state in parent components
- Props flow down, events flow up
- Predictable data flow

**Use Custom Hooks:**
- Encapsulate complex logic
- Share stateful logic between components
- Cleaner component code

---

## Conclusion

We've built a complete, production-ready web application that demonstrates:

✅ **Modern React Patterns**: Hooks, custom hooks, memoization
✅ **Responsive Design**: Mobile-first, Tailwind utilities
✅ **Client-Side Data Processing**: Excel parsing, search algorithms
✅ **Production Deployment**: Docker, nginx, optimizations
✅ **User Experience**: Fast search, expandable cards, clear UI

### Key Takeaways

1. **Architecture Matters**: Separation of concerns makes code maintainable
2. **Performance First**: useMemo, code splitting, caching strategies
3. **Responsive is Non-Negotiable**: Mobile-first design is table stakes
4. **Deploy Early**: Docker + nginx = production-ready from day one
5. **User Experience**: Small details (expand cards, clear search) matter

### Next Steps & Improvements

**Short Term:**
- Add keyboard shortcuts (Cmd+K for search)
- Implement URL-based search (shareable links)
- Add favorites/bookmarking
- Export search results to CSV

**Medium Term:**
- Backend API for search (Elasticsearch)
- User accounts and saved searches
- Comment system for community notes
- Mobile app (React Native)

**Long Term:**
- Real-time data updates (WebSockets)
- Collaborative editing of Excel data
- Advanced filtering (multi-select, ranges)
- Analytics dashboard for popular searches

**Ideas:**
- Add a build creation engine for different characters in the game
- Allow social posting of various builds, ability to add screenshots, comments etc.

### Resources

**Documentation:**
- [React Docs](https://react.dev)
- [Vite Guide](https://vitejs.dev/guide/)
- [Tailwind CSS](https://tailwindcss.com/docs)
- [SheetJS Documentation](https://docs.sheetjs.com)

**Tools Used:**
- [Claude](https://claude.ai)
- [Lucide Icons](https://lucide.dev)
- [Docker](https://docs.docker.com)
- [nginx](https://nginx.org/en/docs/)

**Source Code:**
- Full project available on GitHub [https://github.com/intothevoid/nightreign]
- Live demo at [https://nightreign.karan.myds.me]

---

## About This Project

**Elden Ring: Nightreign DB** is a database for Elden Ring: Nightreign. Built with modern web technologies, it provides fast, searchable access to game mechanics, items, and stats.

**Tech Stack Summary:**
- React 18 + Vite
- Tailwind CSS
- SheetJS (xlsx)
- Docker + nginx

**Contributing:**
- Data updates via Excel file PRs welcome! See CONTRIBUTING.md for guidelines.
- Github PRs for bugs and features also welcome!

---
]]></description>
</item>
<item>
<title>Fujifilm Film Simulations - My Favourite Recipes</title>
<link>https://intothevoid.github.io/fujifilm-film-simulations---my-favourite-recipes-2025-10-03.html</link>
<pubDate>Fri, 03 Oct 2025 16:13:00 +0930</pubDate>
<description><![CDATA[

Film simulations are what makes Fujifilm cameras truly special. These carefully crafted digital profiles transform raw sensor data into images that capture the essence and character of legendary analog film stocks. Far from simple filters or presets, these sophisticated algorithms represent decades of colour science expertise and pay homage to Fujifilm's rich heritage.

## Understanding Film Simulations and X-Trans Sensors

At their core, film simulations are complex processing algorithms that recreate the distinctive characteristics of classic photographic films. Unlike basic colour presets or LUTs, these sophisticated simulations adjust contrast curves, colour rendition, and tonal responses based on exposure levels, luminosity and ISO settings. The magic happens when these simulations interact with Fujifilm's unique X-Trans sensor technology.

The X-Trans sensor differs fundamentally from traditional Bayer sensors used by most camera manufacturers. Instead of the standard RGGB pattern, X-Trans sensors arrange colour filters in a more randomised pattern that resembles the grain structure of film. 

## The Fujifilm X-T2: My current camera

The X-T2, is my current Fujifilm camera. I bought it used for about $800 AUD about 6 years ago. It has a 24.3-megapixel X-Trans CMOS III sensor and X-Processor Pro. My first Fujifilm camera was a X100T, which was a birthday gift from my wife. I've also owned a Fujfilm X100VI, which I bought after falling for the hype and the fact that it was sold out everywhere, only added fuel to the fire. I sold it after a recent trip to New Zealand, as I found the X-T2 perfectly suited to my needs. I also didn't end up using any of the features that the X100VI had, such as the hybrid viewfinder. The 40MP sensor also made image sizes quite large, and for what I used it for (street photography mainly), the extra resolution wasn't necessary.

The X-T2 includes the following film simulations:

- **Provia/Standard** - The neutral baseline simulation offering accurate colour reproduction
- **Velvia/Vivid** - Punchy, saturated colours ideal for landscapes and nature photography
- **Astia/Soft** - Gentle contrast with pleasing skin tones, perfect for portraits
- **Classic Chrome** - Muted, sophisticated tones with distinctive character
- **Pro Neg. Hi** - Professional portrait rendering with lifted shadows and flattering skin tones
- **Pro Neg. Std** - Standard professional negative film look with natural colour balance
- **Acros** - Premium black and white simulation with exceptional grain structure
- **Acros+Ye (Yellow Filter)** - Enhanced contrast with slightly darkened skies
- **Acros+R (Red Filter)** - Dramatic sky contrast and enhanced skin rendering
- **Acros+G (Green Filter)** - Lightened foliage with natural skin tones
- **Monochrome** - Standard black and white conversion
- **Monochrome+Ye/R/G** - Traditional black and white with filter effects
- **Sepia** - Classic warm monochrome tones

Newer cameras have added additional simulations like Classic Negative, Nostalgic Neg, and Reala Ace, but the X-T2's selection remains highly versatile and capable of covering a wide range of photographic styles.

## My Favourite Film Simulation (Colour): Classic Chrome

Among all colour film simulations, Classic Chrome is my favourite out of the box film simulation. Loosely inspired by the aesthetic of Kodachrome slide film, Classic Chrome delivers a distinctive look that has become synonymous with contemporary street and documentary photography.

**Key Characteristics:**

- **Muted colour palette** with reduced global saturation that prevents oversaturated, unnatural-looking images
- **Cool colour bias** with blues shifting towards cyan, creating a sophisticated modern aesthetic
- **Elevated shadow contrast** whilst maintaining softer highlight rendering for balanced exposure
- **Distinctive skin tone rendering** that can be both flattering and challenging depending on lighting
- **Excellent editing latitude** making it ideal as a base for custom recipes and post-processing

Classic Chrome's genius lies in its complexity. Rather than applying uniform desaturation across the spectrum, it selectively reduces saturation in yellows and greens whilst preserving some vibrancy in blues and reds. This selective colour treatment, combined with its unique contrast curve, creates that unmistakable "chrome" look that is both modern and timeless.

The simulation excels in mixed lighting conditions and provides exceptional latitude for both JPEG shooters and those who prefer to use it as a starting point for post-processing. Its muted nature prevents the "over-cooking" that can plague more saturated simulations, making it an excellent choice for photographers who value subtlety and sophistication over punch and drama.

Whether shooting urban landscapes, street photography, or documentary work, Classic Chrome consistently delivers.

## My Recipe - Classy Chrome

Building upon Classic Chrome's exceptional foundation, I've developed a personalised recipe that enhances its characteristics whilst maintaining its distinctive character. This tweaked version addresses some of the simulation's more challenging aspects while amplifying its strengths:

![Classy Chrome Recipe)](images/fuji/cc_card.jpg)
*Classy Chrome Recipe Settings*

### Image Examples

![Provia Standard)](images/fuji/Provia1.jpg)
*A photo captured using the Provia Standard recipe*

![Classy Chrome)](images/fuji/ClassyChrome1.jpg)
*A photo captured using the Classy Chrome recipe*

![Provia Standard)](images/fuji/Provia2.jpg)
*A photo captured using the Provia Standard recipe*

![Classy Chrome)](images/fuji/ClassyChrome2.jpg)
*A photo captured using the Classy Chrome recipe*

This recipe maintains Classic Chrome's sophisticated character whilst adding slightly more warmth and shadow detail. The modest grain effect provides subtle texture reminiscent of actual film photography, whilst the adjusted highlight and shadow settings improve the overall tonal balance for more challenging lighting conditions commonly encountered in street and documentary work. I add some warmth to skin tones to make them more flattering, especially in mixed lighting.

The slight colour boost prevents the simulation from appearing too flat whilst maintaining its distinctive muted aesthetic. Overall, this has become my go-to recipe which I use extensively for street photography, travel, and everyday shooting.

## My Favourite Film Simulation (Black and White): Acros

I am biased towards black and white photography, and most of my photos are shot in black and white. Whilst Classic Chrome captures my heart for colour work, Acros is my goto for black and white street photography. Named after Fujifilm's legendary Neopan Acros 100 film, this simulation represents over a decade of research and development in digital monochrome reproduction.

**Key Characteristics:**

- **Ultra-fine grain structure** that scales naturally with ISO, maintaining character without becoming distracting
- **Exceptional shadow detail** without sacrificing contrast or losing the dramatic black and white aesthetic
- **Print-like texture** that evokes traditional darkroom aesthetics and silver gelatin prints
- **Intelligent noise handling** that mimics film grain characteristics rather than digital artifacts
- **Superior highlight rolloff** preventing harsh clipping whilst maintaining detail in bright areas
- **Natural tonal gradation** that creates smooth transitions between tones

Acros differs fundamentally from simple black and white conversion or basic monochrome modes. Its sophisticated tone curve provides harder contrast in the midtones and highlights whilst maintaining gentle shadow gradation. This creates images with exceptional clarity and depth that genuinely rival traditional film photography.

The simulation includes three filter variants that replicate the effect of coloured filters used in traditional black and white film photography:

- **Acros+Y (Yellow Filter):** Slightly enhanced contrast with darkened skies and improved cloud definition 
- **Acros+R (Red Filter):** Dramatic sky contrast, enhanced skin tones, and increased separation between red objects and backgrounds
- **Acros+G (Green Filter):** Lightened foliage with natural skin rendering and improved landscape tonal separation

The simulation's ability to maintain detail across the entire tonal range makes it exceptionally versatile, working equally well in high-contrast situations and subtle, low-contrast scenes.

## My Recipe - Acros The Street

![Acros The Street Recipe)](images/fuji/acros_card.jpg)
*Acros The Street Recipe Settings*

### Image Examples

![Provia Standard)](images/fuji/Provia1.jpg)
*A photo captured using the Provia Standard recipe*

![Acros The Street)](images/fuji/AcrosTheStreet1.jpg)
*A photo captured using the Acros The Street recipe*

![Provia Standard)](images/fuji/Provia2.jpg)
*A photo captured using the Provia Standard recipe*

![Acros The Street)](images/fuji/AcrosTheStreet2.jpg)
*A photo captured using the Acros The Street recipe*

This recipe enhances some of Acros's already exceptional characteristics. I've tweaked it to suit my everyday walk around street photography needs. The yellow filter variant provides a nice balance between contrast and tonal separation without being too harsh.

## Customising Your Look: Camera Settings That Transform Simulations

Fujifilm cameras offer extensive customisation options that can dramatically alter any film simulation's character. Understanding these controls allows photographers to create truly personalised looks that suit their vision and shooting style:

### Tone Curve Controls:

#### **Highlight (-2 to +4)** - 
Adjusts bright area contrast and detail retention. Negative values preserve highlight detail, while positive values increase contrast

#### **Shadow (-2 to +4)** - 
Controls dark area detail and overall mood. Positive values lift shadows for a more open look, negative values deepen shadows for drama

### Colour and Detail Enhancement:

#### **Colour (-4 to +4)** - 
Global saturation adjustment that works differently with each simulation's colour science

#### **Sharpness (-4 to +4)** - 
In-camera detail enhancement that affects both capture and JPEG processing

#### **Noise Reduction (-4 to +4)** - 
Controls texture preservation vs. smoothing, crucial for maintaining film-like character

### Dynamic Range and Exposure:

#### **DR100/200/400** - 
Expanded highlight retention at higher ISO settings, particularly useful in challenging lighting

#### **Auto ISO** - Intelligent exposure control that works seamlessly with simulation characteristics

### Film-Like Effects:

#### **Grain Effect (Off/Weak/Strong)** - 
Adds authentic film-like texture that scales naturally with ISO and simulation choice

#### **Colour Chrome Effect** - 
Enhanced colour separation in highly saturated areas for more natural colour transitions (available on newer models)

#### **Colour Chrome FX Blue** - 
Improved sky and water rendering with better gradation and colour depth (available on newer models)

### White Balance Fine-Tuning:

#### **Red/Blue bias adjustments** - 
Fine-tune colour temperature and tint to match shooting conditions or creative vision

#### **Custom white balance presets** - 
Create specific colour temperature settings for consistent results

### Advanced Processing (newer cameras):

#### **Clarity** - 
Localised contrast enhancement that adds punch without global contrast adjustments (available on newer models)

#### **Color Chrome Blue** - 
Enhanced blue colour reproduction for more natural skies and water (available on newer models)

I hope you like my favourite recipes and use them while shooting with your Fujifilm camera. Fujifilm cameras may not be the best in autofocus and low light performance, but their film simulations provide film and analog shooters a way to recreate the look of film in a digital world.]]></description>
</item>
<item>
<title>A dive into the Model Context Protocol (MCP) by Anthropic</title>
<link>https://intothevoid.github.io/a-dive-into-the-model-context-protocol-mcp-by-anthropic-2025-05-04.html</link>
<pubDate>Sun, 04 May 2025 18:07:12 +0930</pubDate>
<description><![CDATA[

## Overview

This page introduces the Model Context Protocol (MCP). MCP is a protocol to provide context to a Large Language Model. LLMs are all the rage at the moment, with a large number of high quality open source LLMs now available with permissive licensing. MCPs are a great way to extend the capabilities of LLMs to access data and services, acting as a bridge between the LLM and the services it needs to access.

## Architecture and Components

### Architecture

```mermaid
flowchart LR
    Client[MCP Client] -->|MCP Request| Server[MCP Server]
    
    Server -->|Query| LocalDB[(Local Data Source)]
    LocalDB -->|Results| Server
    
    Server -->|MCP Request| RemoteServer[Remote MCP Server]
    RemoteServer -->|Query| RemoteService[Remote Service]
    RemoteService -->|Results| RemoteServer
    RemoteServer -->|MCP Response| Server
    
    Server -->|MCP Response| Client
```
Fig. The architecture of the Model Context Protocol

1. An MCP Client - that sends requests using the Model Context Protocol to an MCP Server

2. The MCP Server - which accesses a local data source for information and makes MCP requests to another remote MCP server when needed

3. The Remote MCP Server - that communicates with a remote service returns responses back to the original MCP server

`Note`: An MCP Host (Claude desktop, IDEs etc.) access data via MCP clients and MCP servers.

### Key Components

```mermaid
sequenceDiagram
    participant Host as MCP Host
    participant Client as MCP Client
    participant Local as Local MCP Server
    participant Remote as Remote MCP Server

    Host->>Client: 1. User Request
    Client->>Local: 2. Process Request
    Local-->>Client: 3. Local Response
    Client->>Remote: 4. Additional Context
    Remote-->>Client: 5. Remote Response
    Client-->>Host: 6. Combined Response
```

Fig. The key components of the Model Context Protocol architecture



The flow illustrates the basic chain of communication in the Model Context Protocol ecosystem, showing how MCP enables standardized access to both local and remote data sources through a consistent protocol.

## MCP Server Development

Lets dive straight in, and try and build a MCP server, that does something simple - access Stock information. Here's what we are trying to accomplish -

```mermaid
graph TD
    MCP_Host -->|Request| LLM
    LLM -->|Processed Request| MCP_Server(Stocks)
    MCP_Server(Stocks) -->|Request| Remote_Service(Yahoo Stocks API)
    Remote_Service(Yahoo Stocks API) -->|Response| MCP_Server(Stocks)
    MCP_Server(Stocks) -->|Response| LLM
    LLM -->|Final Response| MCP_Host
```

Fig. Simple Stocks MCP Server

The key elements here are the actual server which we will develop, and the Yahoo Stocks API (external service). We access the Stocks API via a standard protocol, and the MCP server 
which we are about to build will help us accomplish this.

To keep things simple, we will develop a MCP server which supports a `get_stock_data(ticker)` function only.

### Creating Stocks API (based on Yahoo Stocks API)

#### Install required libraries

```bash
pip install flask requests
```

#### Stocks API source code

```python
from flask import Flask, request, jsonify
import requests
import time
import random
import logging
from functools import lru_cache

# Configure logging
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('stock_api')

app = Flask(__name__)

# Yahoo Finance API endpoint for stock chart data
def get_chart_url(ticker):
    return f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?range=1d&interval=1d"

def get_stock_price(ticker):
    """
    Fetch stock chart data from Yahoo Finance API by properly mimicking a browser request.
    """
    # No query parameters needed in URL as they're now part of the URL itself
    url = get_chart_url(ticker)
    
    # Comprehensive browser-like headers - this is critical to avoid 429 errors
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://finance.yahoo.com/",
        "sec-ch-ua": '"Google Chrome";v="120", "Chromium";v="120", "Not-A.Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "same-origin",
        "Cache-Control": "max-age=0",
        "Connection": "keep-alive"
    }
    
    try:
        # Use a new session for each request to avoid cookie tracking issues
        with requests.Session() as session:
            # First visit the base page to get cookies (like a real browser would)
            session.get("https://finance.yahoo.com/", headers=headers)
            
            # Then make the actual API request
            response = session.get(url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            # Extract data from chart response structure
            chart_result = data.get("chart", {}).get("result", [])
            if not chart_result:
                return {"error": f"Chart data not found for {ticker}"}
                
            quote = chart_result[0].get("meta", {})
            indicators = chart_result[0].get("indicators", {})
            
            # Get the latest price from quote
            current_price = quote.get("regularMarketPrice")
            
            if current_price is not None:
                return {
                    "ticker": ticker,
                    "price": float(current_price),
                    "currency": quote.get("currency", "USD"),
                    "exchange": quote.get("exchangeName", ""),
                    "timestamp": quote.get("regularMarketTime", 0)
                }
            else:
                return {"error": f"Price data not found for {ticker}"}
                
    except requests.exceptions.RequestException as e:
        logger.error(f"Request failed: {str(e)}")
        return {"error": f"API request failed: {str(e)}"}
    except (KeyError, IndexError, ValueError) as e:
        logger.error(f"Error parsing response: {str(e)}")
        return {"error": f"Failed to parse API response: {str(e)}"}

@app.route("/get_stock_data", methods=["GET"])
def handle_get_stock_data():
    ticker = request.args.get("ticker")
    if not ticker:
        return jsonify({"error": "Missing 'ticker' parameter"}), 400
    
    result = get_stock_price(ticker)
    
    # Return appropriate status code based on result
    if "error" in result and "not found" in result["error"]:
        return jsonify(result), 404
    elif "error" in result:
        return jsonify(result), 500
    
    return jsonify(result)

@app.route("/health", methods=["GET"])
def health_check():
    """Simple health check endpoint"""
    return jsonify({"status": "healthy"})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5566, debug=True)

```

#### Usage

You can use and test this server in the foll. manner -

1. Start the server

```bash
python mcp_server.py
```

2. Test with curl 

```bash
curl "http://localhost:5566/get_stock_data?ticker=AAPL"
```

So far we've created an API /get_stock_data which accepts a ticker and returns stock price information. Now we move on to adding MCP support.

### Implementing the MCP protocol

To add this as an MCP (Model Control Protocol) server for Claude Desktop, we need to implement the MCP protocol in our Flask application.

#### Updated Stocks API source code

Ok, I need to complain a little. I had this annoying issue with Flask putting out debug output which was messing up the Claude desktop MCP parsing. I was about to give up for the day, but Claude AI suggested I try waitress. I updated the code to use waitress to avoid putting out any debug output. **Voila** it works.

```python
from flask import Flask, request, jsonify
import requests
import logging
import json
import time
import sys
import os
import socket
from waitress import serve

# Completely disable all logging
logging.basicConfig(level=logging.CRITICAL)
logger = logging.getLogger('mcp_stock_server')
logger.disabled = True

# Create Flask app with minimal configuration
app = Flask(__name__)
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = False
app.config['JSON_SORT_KEYS'] = False

# Disable all Flask logging
app.logger.disabled = True
log = logging.getLogger('werkzeug')
log.disabled = True

def is_port_in_use(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        return s.connect_ex(('localhost', port)) == 0

def find_available_port(start_port):
    port = start_port
    while is_port_in_use(port):
        port += 1
    return port

# Yahoo Finance API endpoint for stock chart data
def get_chart_url(ticker):
    return f"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?range=1d&interval=1d"

def get_stock_price(ticker):
    """
    Fetch stock chart data from Yahoo Finance API by properly mimicking a browser request.
    """
    # No query parameters needed in URL as they're now part of the URL itself
    url = get_chart_url(ticker)
    
    # Comprehensive browser-like headers - this is critical to avoid 429 errors
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Referer": "https://finance.yahoo.com/",
        "sec-ch-ua": '"Google Chrome";v="120", "Chromium";v="120", "Not-A.Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "document",
        "sec-fetch-mode": "navigate",
        "sec-fetch-site": "same-origin",
        "Cache-Control": "max-age=0",
        "Connection": "keep-alive"
    }
    
    try:
        # Use a new session for each request to avoid cookie tracking issues
        with requests.Session() as session:
            # First visit the base page to get cookies (like a real browser would)
            session.get("https://finance.yahoo.com/", headers=headers)
            
            # Then make the actual API request
            response = session.get(url, headers=headers)
            response.raise_for_status()
            
            data = response.json()
            
            # Extract data from chart response structure
            chart_result = data.get("chart", {}).get("result", [])
            if not chart_result:
                return {"error": f"Chart data not found for {ticker}"}
                
            quote = chart_result[0].get("meta", {})
            
            # Get the latest price from quote
            current_price = quote.get("regularMarketPrice")
            
            if current_price is not None:
                return {
                    "ticker": ticker,
                    "price": float(current_price),
                    "currency": quote.get("currency", "USD"),
                    "exchange": quote.get("exchangeName", ""),
                    "timestamp": quote.get("regularMarketTime", 0)
                }
            else:
                return {"error": f"Price data not found for {ticker}"}
                
    except requests.exceptions.RequestException as e:
        logger.error(f"Request failed: {str(e)}")
        return {"error": f"API request failed: {str(e)}"}
    except (KeyError, IndexError, ValueError) as e:
        logger.error(f"Error parsing response: {str(e)}")
        return {"error": f"Failed to parse API response: {str(e)}"}

# Original stock data endpoint
@app.route("/get_stock_data", methods=["GET"])
def handle_get_stock_data():
    ticker = request.args.get("ticker")
    if not ticker:
        return jsonify({"error": "Missing 'ticker' parameter"}), 400
    
    result = get_stock_price(ticker)
    
    # Return appropriate status code based on result
    if "error" in result and "not found" in result["error"]:
        return jsonify(result), 404
    elif "error" in result:
        return jsonify(result), 500
    
    return jsonify(result)

# MCP Protocol Implementation
@app.route("/mcp", methods=["POST"])
def mcp_endpoint():
    try:
        # Parse the incoming request
        mcp_request = request.json
        
        # Check for required fields in MCP request
        if not mcp_request or "messages" not in mcp_request:
            return jsonify({
                "error": "Invalid MCP request format"
            }), 400
        
        # Extract the latest user message
        user_messages = [msg for msg in mcp_request["messages"] if msg.get("role") == "user"]
        if not user_messages:
            return jsonify({
                "error": "No user message found in request"
            }), 400
            
        latest_user_message = user_messages[-1]["content"]
        
        # Process the user message to extract ticker symbol
        # This is a simple implementation - you might want to add more sophisticated parsing
        ticker = None
        
        # Check if message contains stock price query format
        words = latest_user_message.lower().split()
        
        # Look for patterns like "price of AAPL" or "AAPL stock price" or "stock price for AAPL"
        for i, word in enumerate(words):
            if word.upper() == word and len(word) >= 1 and len(word) <= 5 and word.isalpha():
                # This looks like a ticker symbol (all caps, 1-5 letters)
                ticker = word.upper()
                break
                
            if i < len(words) - 1:
                if (word == "for" or word == "of") and words[i+1].upper() == words[i+1] and len(words[i+1]) <= 5:
                    ticker = words[i+1].upper()
                    break
        
        # If we couldn't find a ticker in the message
        if not ticker:
            return jsonify({
                "messages": [{
                    "role": "assistant",
                    "content": "I couldn't identify a stock ticker symbol in your message. Please specify a valid stock symbol like AAPL, MSFT, or TSLA."
                }]
            })
        
        # Get stock data
        stock_data = get_stock_price(ticker)
        
        # Format the response for Claude Desktop
        if "error" in stock_data:
            response_content = f"I encountered an error looking up {ticker}: {stock_data['error']}"
        else:
            # Format price with 2 decimal places
            price = f"{stock_data['price']:.2f}"
            response_content = f"The current price of {ticker} is {price} {stock_data['currency']} on {stock_data['exchange']}."
        
        # Return response in MCP format
        return jsonify({
            "messages": [{
                "role": "assistant",
                "content": response_content
            }]
        })
        
    except Exception as e:
        logger.error(f"MCP processing error: {str(e)}")
        return jsonify({
            "messages": [{
                "role": "assistant",
                "content": f"I encountered an error processing your request: {str(e)}"
            }]
        })

@app.route("/health", methods=["GET"])
def health_check():
    """Simple health check endpoint"""
    return jsonify({"status": "healthy", "service": "MCP Stock Server"})

# MCP Info endpoint to provide information about this MCP server
@app.route("/mcp/info", methods=["GET"])
def mcp_info():
    return jsonify({
        "name": "Stock Price Lookup",
        "version": "1.0.0",
        "description": "Get real-time stock prices from Yahoo Finance",
        "author": "Karan Kadam",
        "properties": {
            "max_tokens_to_sample": 1000,
            "temperature": 0
        }
    })

if __name__ == "__main__":
    # Get port from environment or use default
    default_port = 8081
    port = int(os.environ.get('PORT', default_port))
    
    # Find an available port if the specified one is in use
    if is_port_in_use(port):
        port = find_available_port(port)
        print(f"Port {port} is available", file=sys.stderr)
    
    # Check if running in Claude Desktop environment
    if 'WERKZEUG_SERVER_FD' in os.environ:
        try:
            # Running in Claude Desktop - use the provided file descriptor
            serve(
                app,
                host="0.0.0.0",
                port=port,
                threads=1,
                _quiet=True,
                fd=int(os.environ["WERKZEUG_SERVER_FD"])
            )
        except Exception as e:
            print(f"Error starting server: {str(e)}", file=sys.stderr)
            sys.exit(1)
    else:
        # Running standalone - use waitress server
        serve(
            app,
            host="0.0.0.0",
            port=port,
            threads=1,
            _quiet=True
        )

```

#### What This Implementation Includes:

- Implements the /mcp endpoint required by Claude Desktop

- Handles MCP request format and returns proper MCP responses

- Provides /mcp/info endpoint with metadata about your service

- Smart Ticker Symbol Detection:
a. Tries to identify stock symbols in natural language queries
b. Looks for uppercase words that are 1-5 letters long
c. Handles common phrases like "price of AAPL" or "AAPL stock price"

- Formatted Responses: Returns nicely formatted answers with price, currency and exchange

## Integration with a LLM

Lets integrate this server we created with a LLM. I will be using Claude desktop as an example here -

### Install Claude for desktop

You can download Claude for desktop here [Download](https://claude.ai/download)

### Run the server

Install dependencies and run the server we created -

```bash
python -m venv venv
source venv/bin/activate
pip install flask requests waitressç
python mcp_server.py
```

This will start the server on port 8081 if everything worked properly.

### Add server to Claude desktop

1. Open Claude Desktop
2. Go to Settings > Developer
3. Click "Edit Config"
4. Update the config `claude_desktop_config.json` as shown below
5. Restart Claude Desktop

Here is what your `claude_desktop_config.json` should look like -

```json
{
    "serverConfig": {
      "command": "/bin/sh",
      "args": [
        "-c"
      ]
    },
     "stock_price_server": {
        "command": "python",
        "args": [
          "mcp_server.py"
        ],
        "env": {
          "FLASK_APP": "mcp_server.py",
          "FLASK_ENV": "production",
          "PORT": "8081"
        }
      }
    }
  }
```

`Note`: Update the paths according to your machine. Verify that your mcp server is registered and working well under Settings > Developer.

![Claude Developer Settings](images/mcp/claude-settings.jpg)

### Using the MCP server from Claude desktop

Once connected, you can ask Claude Desktop questions about stock prices like:

    "What's the stock price of AAPL?"
    "Get me the current stock price for MSFT"
    "How much is TSLA stock trading for?"

Claude will recognize these queries and route them to your MCP server, which will fetch the real-time stock data and return it.

## Testing and Troubleshooting:

1. You can test the stock data API directly at: http://localhost:8081/get_stock_data?ticker=AAPL
2. You can check if the server is running at: http://localhost:8081/health
3. You can verify the MCP info at: http://localhost:8081/mcp/info

The server keeps your existing stock API implementation intact while adding the MCP functionality on top, so both can be used independently.

MCP is a great way to extend the capabilities of LLMs to access data and services. It is a simple protocol to implement and can be used to access a wide range of data sources. I see myself implementing more MCP servers to access different data sources in the future.
]]></description>
</item>
<item>
<title>Likho: A lightweight static site generator written in Go</title>
<link>https://intothevoid.github.io/likho-a-lightweight-static-site-generator-written-in-go-2025-04-24.html</link>
<pubDate>Thu, 24 Apr 2025 17:34:16 +0930</pubDate>
<description><![CDATA[

![Likho](/images/likho/logo.png "Likho")

`Likho` (_verb_): means `to write` (Hindi).

A static site generator is a tool that converts markdown files into a static website. It's a great way to create a simple and fast website. I have been using different static site generators to create my blog in the past. I really liked using [Hugo](https://gohugo.io/) but I always felt it was too much for my needs. I wanted something simple and have always wanted to write my own static site generator.

Likho is a static site generator that transforms markdown files into a beautiful, functional website. It's designed with simplicity in mind while providing powerful features that modern websites need. Whether you're a blogger, technical writer, or just someone who wants to maintain a personal website, Likho has you covered.

## Core Features

- 🚀 **Speed**: Built in Go for performance
- 📝 **Markdown First**: Write content in your favorite markdown editor
- 🎨 **Customizable**: Flexible theming system with support for custom templates
- 📊 **Mermaid Diagrams**: Create beautiful diagrams directly in your markdown
- 🔍 **Syntax Highlighting**: Code blocks that look great
- 📱 **Responsive Design**: Looks great on all devices
- 📰 **RSS & Sitemap**: Automatic generation of RSS feeds and sitemaps

## System Architecture

Here's how Likho works under the hood:

```mermaid
graph TD
    A[Markdown Files] --> B[Likho]
    B --> C[Content Parser]
    C --> D[Template Engine]
    D --> E[HTML Generator]
    E --> F[Static Site]
    G[Config] --> B
    H[Theme Files] --> D
```

The system follows a clear pipeline:

1. **Content Parser**: Reads markdown files and YAML frontmatter
2. **Template Engine**: Processes templates with Go's template engine
3. **HTML Generator**: Converts markdown to HTML and applies templates
4. **Asset Processor**: Handles static assets and theme files

## Directory Structure

```mermaid
graph TD
    A[Project Root] --> B[content]
    A --> C[themes]
    A --> D[public]
    B --> E[posts]
    B --> F[pages]
    B --> G[images]
    C --> H[default]
    H --> I[static]
    H --> J[templates]
    I --> K[css]
    I --> L[js]
    I --> M[images]
```

## How It Works

### 1. Content Processing

Likho processes your content in three main steps:

```mermaid
sequenceDiagram
    participant User
    participant Likho
    participant Parser
    participant Generator
    
    User->>Likho: Run generate command
    Likho->>Parser: Read markdown files
    Parser->>Parser: Parse frontmatter
    Parser->>Parser: Convert markdown to HTML
    Parser->>Generator: Pass processed content
    Generator->>Generator: Apply templates
    Generator->>Generator: Generate static files
    Generator->>User: Output static site
```

### 2. Theme System

The theme system is modular and easy to customize:

```mermaid
graph LR
    A[Theme] --> B[Base Template]
    B --> C[Post Template]
    B --> D[Page Template]
    B --> E[Index Template]
    A --> F[Static Assets]
    F --> G[CSS]
    F --> H[JS]
    F --> I[Images]
```

## Usage Example

Creating a new post is as simple as:

```bash
./likho create post "My New Post" -t "technology,golang" -i "https://example.com/image.jpg"
```

This generates a new markdown file with the following structure:

```yaml
---
title: "My New Post"
date: "2024-03-21"
tags: ["technology", "golang"]
image: "https://example.com/image.jpg"
description: ""
---
```

## Performance

One of Likho's key strengths is its speed and simplicity. Being written in Go, it can process thousands of markdown files in seconds. Here's a distribution of time taken to process files:

```mermaid
pie title Processing Time
    "Parsing" : 30
    "Template Processing" : 20
    "File Generation" : 10
    "Asset Copying" : 5
```

## Getting Started

1. Install Go 1.23 or later
2. Clone the repository
3. Build the application
4. Start creating content

```bash
git clone https://github.com/intothevoid/likho.git
cd likho
go build -o likho cmd/likho/main.go
```

## Building and serving your website

Running the `generate` command will read all the markdown files in the `content` directory and generate the static website in the `public` directory.

```bash
./likho generate
```
You can now locally serve your website using the `serve` command.

```bash
./likho serve
```

This will start a local server and you can view your website at `http://localhost:8080`.

Pro-tip: You can use Github pages to host your website for free, all you need to do is create a repository with the name `username.github.io` and push your `public` directory to the `gh-pages` branch. Github actions will automatically build and serve your website.

## Customizing your website

You can customize the look of your website by editing the themes inside the `themes/` directory. A few example themes have been provided for you to get started.

## Why Likho?

Likho combines the power of Go with the simplicity of markdown to create a static site generator that's both powerful and easy to use. Whether you're a developer looking to document your projects or a writer wanting to share your thoughts, Likho provides all the tools you need to create a beautiful, functional website.

### Proof is in the pudding

This website you are reading is built and powered by Likho. You can find the source code [here](https://github.com/intothevoid/likho-blog).

Here's how the deployment pipeline works:

```mermaid
graph LR
    A[Markdown Files] --> B[Likho]
    B --> C[HTML Files]
    C --> D[GitHub Repository]
    D --> E[GitHub Actions]
    E --> F[GitHub Pages]
    
    subgraph "Local Development"
        A
        B
        C
    end
    
    subgraph "Deployment"
        D
        E
        F
    end
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333,stroke-width:2px
    style F fill:#bfb,stroke:#333,stroke-width:2px
```

The process is fully automated:
1. Write content in Markdown
2. Likho converts it to HTML
3. Push changes to GitHub
4. GitHub Actions automatically builds and deploys
5. Your site is live on GitHub Pages

Give it a try and let me know what you think! You can find the project on [GitHub](https://github.com/intothevoid/likho). 
]]></description>
</item>
<item>
<title>Top CLI tools for programmers on a Mac</title>
<link>https://intothevoid.github.io/top-cli-tools-for-programmers-on-a-mac-2025-04-21.html</link>
<pubDate>Mon, 21 Apr 2025 09:42:47 +0930</pubDate>
<description><![CDATA[

The aim of this post is to enhance and speed up your terminal terminal usage on a Mac. As a programmer you will spend a significant amount of time on the terminal. These tools will speed up common actions and help you become more productive. This post assumes you have Homebrew installed. If you don't follow the instructions on https://brew.sh/

```zsh
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

## 1. Mas - The Terminal App Store Manager

Tired of the clunky Mac App Store? **Mas** is a terminal app store manager that allows you to easily control and manage your apps downloaded from the App Store. It can be quite annoying and slow to open the App Store on your Macbook and this is a much cleaner and faster way to manage your apps.

**Installation:**

To install Mas, run the following command in your terminal:

```bash
brew install mas
```

**Usage:**

*   **Listing installed apps:** To see a list of your installed apps and their versions, use:

    ```bash
    mas list
    ```

    ![Screenshot of `mas list` output](images/cli-tools/mas-list.jpg)

*   **Getting app information:** You can retrieve detailed information about an app using its ID:

    ```bash
    mas info <app_id>
    ```

    To find the app ID, you can use `mas list`. For example, for the "Neptune" app, you can grab its ID and run:

    ```bash
    mas info <Neptune_app_id>
    ```

*   **Checking for outdated apps:** To see if any of your installed apps have updates available, run:

    ```bash
    mas outdated
    ```

*   **Upgrading apps:** To install available updates, use:

    ```bash
    mas upgrade
    ```

    This is particularly useful for updating Xcode command-line tools, which the App Store might not always handle correctly.

*   **Searching for apps:** You can search for specific apps in the App Store using:

    ```bash
    mas search <app_name>
    ```

*   **Downloading apps:** To download an app from the App Store, use its ID with the `purchase` command:

    ```bash
    mas purchase <app_id>
    ```

    For more commands, you can refer to the Mas GitHub page.

## 2. fzf - Fuzzy Finder

**fzf** is a powerful fuzzy finder that allows you to search through your files and directories efficiently in the terminal. The lesser you need to `cd`into directories, the faster your navigation will be.

![Screenshot of `fzf` output](images/cli-tools/fzf.jpg)

**Installation:**

Install fzf using Homebrew:

```bash
brew install fzf
```

**Integration:**

To integrate fzf with your shell (like zsh), you need to initialize it by adding the following line to your `~/.zshrc` (or equivalent RC file):

```bash
[[ -s "$(brew --prefix)/opt/fzf/shell/completion.zsh" ]] && source "$(brew --prefix)/opt/fzf/shell/completion.zsh"
[[ -s "$(brew --prefix)/opt/fzf/shell/key-bindings.zsh" ]] && source "$(brew --prefix)/opt/fzf/shell/key-bindings.zsh"
```

After adding these lines, **save and source your RC file** (e.g., `source ~/.zshrc`). You can verify the installation by running `fzf` or `fzf --version`.

**Key Bindings and Usage**

Custom fzf configurations in the `.zshrc` file.

*   **`Ctrl+T`:** This key binding performs a fuzzy search for all files in the current directory (and subdirectories), using `fd` (explained later) to speed up the search and excluding `.git` files.

*   **`Option+C` (or `Alt+C`):** This command specifically searches for directories, allowing you to quickly jump to a desired directory.

*   **Custom Command (`nvim Ctrl+T` or `code Ctrl+T`):** You can also use fzf with other commands like `nvim` (neovim) and `code` (VS Code) to fuzzy find files and open them in the respective editor. For example, `nvim` followed by `Ctrl+T` allows fuzzy searching through all files and opening the selected one in neovim. Similarly, `code Ctrl+T` does the same for VS Code.

## 3. fd - A Simple, Fast and User-Friendly Alternative to 'find'

You can use **fd** as a faster alternative to the `find` command, especially in your fzf setup.

**Installation:**

Install fd using Homebrew:

```bash
brew install fd
```

fd is used in the default fzf command (`Ctrl+T`) with the `-H` flag to show hidden directories and the `-I` flag to ignore `.git` files.

## 4. bat - A cat(1) Clone with Wings

**bat** is a "must-have" tool, acting as a `cat` replacement but with syntax highlighting for code views.

**Installation:**

Install bat using Homebrew:

```bash
brew install bat
```

**Usage:**

Simply use `bat` followed by a filename to view its content with syntax highlighting:

```bash
bat <filename>
```

![Screenshot of `bat` output](images/cli-tools/bat.jpg)

This is especially useful when combined with fzf.

## 5. tldr - Collaborative Cheat Sheets for Console Commands

**tldr** is similar to man pages but providing only the most commonly used commands and examples. You can use it to quickly get the essential commands without diving into the detailed explanations of man pages.

**Installation:**

Install tldr using Homebrew:

```bash
brew install tldr
```

**Usage:**

To get a quick overview of a command, use `tldr` followed by the command name:

```bash
tldr <command>
```

For example:

```bash
tldr git
```

![Screenshot of `tldr git` output](images/cli-tools/tldr-git.jpg)

## 6. fzf.git - Fuzzy Finder Extensions for Git

**fzf.git** integrates fzf with Git, providing convenient fuzzy searching for Git branches, remotes, and worktrees. This can be sourced into your .zshrc file

**Installation:**

Start by cloning the fzf.git repository:

```bash
git clone https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install
```

**Usage (via key bindings):**

The following key bindings should work in a Git repository with a remote:

*   **`Ctrl+GB`:** Shows all Git branches.

*   **`Ctrl+GR`:** Shows all Git remotes.

*   **`Ctrl+GW`:** Shows Git worktrees.

## 7. zoxide - A Smarter cd Command

**zoxide** is a tool that learns your most frequently used directories and allows you to jump to them quickly without typing the full path. It remembers paths based on recency.

**Installation:**

Install zoxide using Homebrew:

```bash
brew install zoxide
```

**Integration:**

Add the following line to your `~/.zshrc` file to initialize zoxide:

```bash
eval "$(zoxide init zsh)"
```

I have added an alias to zoxide in my ~/.bashrc to replace the `cd` command -

```bash
alias cd='z'
```

This can be super convenient if you frequently traverse deep folder hierarchies.

![Screenshot of `zoxide` output](images/cli-tools/zoxide.jpg)

**Save and source your RC file**.

**Usage:**

*   **Jumping to a directory:** Once zoxide has learned some directories (by you navigating into them using `cd` at least once), you can jump to them using the `z` command followed by a part of the directory name:

    ```bash
    z <partial_directory_name>
    ```

    For example, after navigating to a "projects" directory, you can later jump back with `z pro`.

*   **Returning to home:** Typing `z` alone will take you back to your home directory.

## 8. eza - A Modern, Feature-Rich Replacement for ls

**eza** is an enhanced version of the `ls` command, offering improved output with icons, colors, and more. It provides better visual clarity.

**Installation:**

Install eza using Homebrew:

```bash
brew install eza
```

**Usage:**

You can have your `ls` command aliased to `eza` with various options configured in your `.zshrc`:

```bash
alias ls="eza -l --icons --group-directories-first"
```

![Screenshot of `eza` output](images/cli-tools/eza.jpg)

*   **Basic listing:** Running `ls` (which is `eza` in this case) will provide a detailed listing with icons and grouped directories.

*   **Grid view:** You can also configure eza to display output in a grid format:

    ```bash
    alias ls="eza --grid --icons"
    ```

*   **Integration with fzf:** The `Option+C` (directory search) in fzf also includes eza for previews, showing detailed information about the selected directory.

## 9. yazi - A Blazing Fast Terminal File Manager

**yazi** is a terminal file manager built on Rust. It is a great addition to your terminal.

**Installation:**

Install yazi using Homebrew:

```bash
brew install yazi ffmpeg sevenzip jq poppler fd ripgrep fzf zoxide resvg imagemagick font-symbols-only-nerd-font
```

yazi works well with other tools but if you don't want to install the others use `brew install yazi` only


**Usage:**

Running `y` will open the yazi file manager if you add the following function to your ~/.bashrc -

```bash
function y() {
	local tmp="$(mktemp -t "yazi-cwd.XXXXXX")" cwd
	yazi "$@" --cwd-file="$tmp"
	if cwd="$(command cat -- "$tmp")" && [ -n "$cwd" ] && [ "$cwd" != "$PWD" ]; then
		builtin cd -- "$cwd"
	fi
	rm -f -- "$tmp"
}
```

*   **Navigation:** Use the up and down arrow keys to navigate within the current directory, and the left and right arrow keys to go to the previous and next directories.

    ![Screenshot of the yazi interface](images/cli-tools/yazi.jpg)

*   **Customization:** Yazi can be customized by creating a `.config/yazi` directory and adding configuration files like `yazi.toml`, `keymaps.toml`, and `themes.toml`. Themes can be downloaded from the Yazi GitHub repository.

*   **Key Mappings:** Use Vim-like key mappings (configured in `keymaps.toml`) such as `j` and `k` for moving up and down, `l` to enter a directory, and `h` to go back.

*   **Themes:** Themes can be configured in the `themes.toml` file.

*   **Basic File Operations:** Yazi offers various commands:
    *   `a`: Add a file (e.g., `test.js`).
    *   `/`: Create a directory.
    *   `y`: Yank (copy) a file.
    *   `p`: Paste a file.
    *   `d`: Delete a file to trash.
    *   `D`: Permanently delete a file.
    *   `x`: Cut a file.
    *   `/` or `?`: Start a search.
    *   `Enter`: Open a file with the default editor (can be configured, e.g., to neovim).
    *   `: `: Run a shell command.

## 10. tmux - Terminal Multiplexer

**tmux** is a powerful terminal multiplexer that allows you to manage multiple terminal sessions within a single window.

**Installation:**

Install tmux using Homebrew:

```bash
brew install tmux
```

**Configuration:**

The `~/.tmux.conf` file is used for custom configurations and there's a `~/.tmux/plugins` directory for tmux plugins.

**Key Bindings and Usage:**

*   **Prefix Key:** `Ctrl+b` is used as the prefix key.
*   **Splitting Panes:**
    *   `Ctrl+b` then `%` Split vertically.
    *   `Ctrl+b` then `"`: Split horizontally.
*   **Reload Configuration:** `Ctrl+b` then `r`: Reloads the tmux configuration.
*   **Pane Resizing:** `Ctrl+b` then `H`, `J`, `K`, or `L` (using Vim-like motion keys): Resizes panes.
*   **Selection Mode:** `Ctrl+b` then `[`: Enters selection mode, allowing you to copy text.
*   **Tmux Sessionizer (via script):** A script to manage tmux sessions.
*   **Tmux Plugin Manager (TPM):** TPM is used to manage tmux plugins. After installing TPM (by cloning its GitHub repository), you need to add a line to the end of your `~/.tmux.conf` to run the TPM configuration.

    ```tmux
    run '~/.tmux/plugins/tpm/tpm'
    ```

*   **Plugins:** The following plugins can be used:
    *   `tmux-plugins/tpm`: The tmux plugin manager itself.
    *   `christoomey/vim-tmux-navigator`: Allows seamless navigation between Vim and tmux panes using Vim motion keys.
    *   `thewtex/tmux-sessionx`: Provides an enhanced session management interface, accessible with `Ctrl+b` then `O`. It allows easy switching, creation (by typing a new name), and renaming (with `Ctrl+r`) of tmux sessions.
    *   Plugins for online status, battery, and theming.
    *   `tmux-plugins/tmux-resurrect`: Saves and restores tmux sessions.
    *   `tmux-plugins/tmux-continuum`: Automatically saves tmux sessions every 15 minutes (requires `tmux-resurrect`).

**Plugin Management:** After setting up TPM and listing plugins in `~/.tmux.conf`, you need to:

1.  Save the `~/.tmux.conf` file.
2.  Reload the tmux configuration: `Ctrl+b` then `r`.
3.  Install the plugins: `Ctrl+b` then `I` (capital I).

After this, kill all tmux servers (`tmux kill-server`) and start a new tmux session.

Tmux allows for quickly switching between different project sessions, reducing the need to constantly navigate through directories. I might end up writing a more detailed post on Tmux because it offers so much functionality and deep customisation.

These command-line tools can significantly enhance a programmer's terminal experience, making tasks more efficient and enjoyable. By following the installation and usage instructions on this page, you can start integrating these tools into your workflow. Don't hesitate to explore their advanced features and configurations to tailor them to your specific needs. Remember to refer to the respective project GitHub pages for more detailed information and options.
]]></description>
</item>
<item>
<title>Daysync - An ESP32 based smart screen for your desk</title>
<link>https://intothevoid.github.io/daysync---an-esp32-based-smart-screen-for-your-desk-2025-04-17.html</link>
<pubDate>Thu, 17 Apr 2025 15:32:35 +0930</pubDate>
<description><![CDATA[

## Overview

![Daysync](images/esp32/daysync.gif)

Daysync combines an ESP32 microcontroller with a TFT display (ESP32-2432S028R) to create a smart information display system. It fetches and displays various types of data including weather information, MotoGP and Formula 1 race calendars, cryptocurrency prices, stock market data and news headlines.

```mermaid
graph LR
    subgraph ESP32["ESP32 Display"]
        E[ESP32 Board]
        T[TFT Display]
        E --> T
    end

    subgraph Backend["Go Backend"]
        B[API Server]
        W[Weather API]
        M[MotoGP API]
        F[F1 API]
        C[Crypto API]
        N[News API]
        S[Stock API]
    end

    E <-->|HTTP Requests| B
    B <-->|Data Fetch| W
    B <-->|Data Fetch| M
    B <-->|Data Fetch| F
    B <-->|Data Fetch| C
    B <-->|Data Fetch| N
    B <-->|Data Fetch| S
```

## Parts

### ESP32-2432S028R with 2.4" TFT Display

These boards are cheap and easy to find on AliExpress. You can find them [here](https://vi.aliexpress.com/item/1005008598530650.html?spm=a2g0o.productlist.main.73.472b1ae23nfIwc&algo_pvid=2f24f0e3-cf71-4b57-84db-2e338bede7a4&algo_exp_id=2f24f0e3-cf71-4b57-84db-2e338bede7a4-36&pdp_ext_f=%7B%22order%22%3A%22-1%22%2C%22eval%22%3A%221%22%7D&pdp_npi=4%40dis%21AUD%2120.59%2113.59%21%21%2194.28%2162.23%21%40210337bc17448542486703194e17c5%2112000045890694919%21sea%21AU%212747051205%21X&curPageLogUid=nbT3rzhrKgn2&utparam-url=scene%3Asearch%7Cquery_from%3A).

### 3D Printed Enclosure 

I designed a 3D printed enclosure using the free version of Autodesk Fusion 360 for the ESP32-2432S028R with 2.4" TFT Display. This was quite time consuming and I had to use Vernier Callipers to get accurate measurements. At the end of it, the result was better than I expected. I used a Bambu Labs P1S printer to print the enclosure. I even managed to get a cool `bindok` logo etched at the back of the enclosure.

`bindok` meaning no brains is my gamer alias 🤣 

![Enclosure Design](images/esp32/cyd-case.jpg)

You can download the STL file [here](other/esp32/esp32-2432s028r-bindok.stl)

### Other Parts

- Micro USB Cable for programming the ESP32
- USB power (any cheap 5V adaptor)
- USB-C to micro usb adaptor if your PC only has USB-C ports
- Vernier Callipers for measuring the dimensions of the board

## Source Code

The source code is available on [GitHub](https://github.com/intothevoid/daysync).

## System Architecture

The project consists of two main components:

1. **ESP32 Display System**
   - ESP32 microcontroller
   - TFT display
   - WiFi connectivity
   - LVGL for UI rendering

2. **Go Backend Server**
   - RESTful API endpoints
   - Data caching
   - External API integration
   - Configuration management

```mermaid
graph TD
    subgraph ESP32["ESP32 System"]
        W[WiFi Module]
        M[Memory]
        D[Display Driver]
        W --> M
        M --> D
    end

    subgraph Backend["Backend Services"]
        C[Cache Layer]
        A[API Layer]
        E[External APIs]
        C --> A
        A --> E
    end

    ESP32 <-->|HTTP| Backend
```

## Backend Functionality

### API Endpoints

The Go backend provides several RESTful endpoints:

```mermaid
graph TD
    API[API Server] --> MotoGP[MotoGP Endpoints]
    API --> F1[F1 Endpoints]
    API --> Weather[Weather Endpoint]
    API --> Crypto[Crypto Endpoint]
    API --> News[News Endpoint]
    API --> Finance[Finance Endpoint]

    MotoGP --> |/api/motogp| SeasonData
    MotoGP --> |/api/motogpnextrace| NextRace
    F1 --> |/api/formula1| SeasonData
    F1 --> |/api/formula1nextrace| NextRace
    Weather --> |/api/weather| LocationData
    Crypto --> |/api/crypto| PriceData
    News --> |/api/news| Headlines
    Finance --> |/api/finance| StockData
```

### Data Flow

```mermaid
sequenceDiagram
    participant ESP32
    participant Backend
    participant ExternalAPI

    ESP32->>Backend: HTTP Request
    Backend->>Backend: Check Cache
    alt Cache Hit
        Backend-->>ESP32: Return Cached Data
    else Cache Miss
        Backend->>ExternalAPI: Fetch New Data
        ExternalAPI-->>Backend: Return Data
        Backend->>Backend: Update Cache
        Backend-->>ESP32: Return Fresh Data
    end
```

## ESP32 Display Features

### Screen Management

The ESP32 display automatically cycles through different information screens:

```mermaid
stateDiagram-v2
    [*] --> Weather
    Weather --> MotoGP
    MotoGP --> Formula1
    Formula1 --> StockMarket
    StockMarket --> Cryptocurrency
    Cryptocurrency --> News1
    News1 --> News2
    News2 --> About
    About --> Weather
```

### Data Display

Each screen type has specific information layout:

```mermaid
graph TD
    subgraph Weather["Weather Screen"]
        W1[Location]
        W2[Date/Time]
        W3[Temperature]
        W4[Humidity]
        W5[Conditions]
    end

    subgraph Racing["Race Calendar Screen"]
        R1[Race Name]
        R2[Circuit]
        R3[Date]
        R4[Next Race Info]
    end

    subgraph Finance["Financial Screen"]
        F1[Symbol]
        F2[Current Price]
        F3[Price Change]
        F4[Market Data]
    end
```

## Technical Implementation

### Backend Features

1. **Caching System**
   - 60-minute cache duration
   - Memory-efficient storage
   - Automatic cache invalidation

2. **API Integration**
   - RESTful endpoints
   - Error handling
   - Rate limiting
   - CORS support

3. **Configuration Management**
   - YAML configuration
   - Environment variables
   - Test mode support

### ESP32 Features

1. **Display Management**
   - LVGL for UI
   - Screen rotation
   - Automatic updates (Server side, microcontroller auto updates not supported)
   - Error handling

2. **Data Processing**
   - JSON parsing
   - Data validation
   - Error recovery
   - WiFi reconnection

## Development and Testing

The project includes several development features:

```mermaid
graph LR
    subgraph Dev["Development Tools"]
        T[Test Mode]
        C[Cache Control]
        D[Debug Logging]
        M[Mock Data]
    end

    subgraph Test["Testing Features"]
        U[Unit Tests]
        I[Integration Tests]
        E[Error Simulation]
        P[Performance Tests]
    end
```

## Possible Future Enhancements

1. **Planned Features**
   - Ability for user to change their wifi SSID and password (harcoded at the moment)
   - Custom screen layouts
   - User preferences
   - More data sources
   - Offline mode
   - Automatic OTA updates

This was a fun project that taught me how to use a low power microcontroller to display information. The ESP32 only makes REST calls, the heavy lifting is done by the Go backend. By combining the power of ESP32 with a Go backend, it provides a flexible way to display various types of real-time data.

There can be many improvements to enhance the user experience. User preferences via a web interface would be great. I'll hopefully find time to keep improving this project and make it more user friendly.
]]></description>
</item>
<item>
<title>Kramerbot - a deal finding Telegram bot</title>
<link>https://intothevoid.github.io/kramerbot---a-deal-finding-telegram-bot-2025-04-04.html</link>
<pubDate>Fri, 04 Apr 2025 12:13:19 +1030</pubDate>
<description><![CDATA[

## Demo

A demo of the bot is running at [https://t.me/kramerbot](https://t.me/kramerbot).

## Overview

KramerBot is a Telegram bot designed to help users stay updated with the latest deals from websites like www.ozbargain.com.au and Amazon (via CamelCamelCamel). This bot acts as your personal deal hunter, constantly monitoring for the best bargains and notifying you instantly when they're found.

This bot was created out of a need to find specific deals. Each time I wanted to purchase something online, it was tedious waiting for that product to go on sale. So I decided to write a bot that would notify me when the price dropped below a certain threshold.

Ozbargain is a community driven website where users post deals they find. The deals are voted on by other users and the top deals are displayed on the front page. 

Amazon needs no introduction. CamelCamelCamel is a website that scrapes Amazon and provides an API for the price history of a given product.

## Key Features

The bot is built using Go and uses the Telegram Bot API to send messages to users. Some of its features are:

- **Real-time Deal Notifications**: Get instant updates about deals through Telegram
- **Multiple Deal Sources**:
  - OzBargain (Good deals with 25+ votes)
  - OzBargain (Super deals with 50+ votes)
  - Amazon Australia (Daily deals)
  - Amazon Australia (Weekly deals)
- **Custom Keyword Watchlists**: Set up personalized deal alerts based on keywords
- **Android TV Notifications**: Optional integration with Pipup for TV notifications
- **Admin Features**: Send announcements to all users, if you are an admin
- **SQLite Database**: Persistent storage of user preferences and deal history
- **Docker Support**: Easy deployment with containerization

## Architecture

The architecture of the bot and its main system components are shown in the diagram below:

### System Components

```mermaid
graph TD
    A[Telegram Bot API] --> B[KramerBot]
    B --> C[OzBargain Scraper]
    B --> D[Amazon Scraper]
    B --> E[SQLite Database]
    B --> F[Pipup Service]
    C --> G[OzBargain Website]
    D --> H[Amazon/CamelCamelCamel]
```

### Core Components

1. **Bot Core (`bot/`)**
   - Handles Telegram API interactions
   - Manages user commands and responses
   - Processes deal notifications

2. **Scrapers (`scrapers/`)**
   - OzBargainScraper: Monitors OzBargain deals
   - CamCamCamScraper: Tracks Amazon deals via CamelCamelCamel

3. **Data Models (`models/`)**
   - UserData: Stores user preferences and settings
   - Deal structures for different platforms
   - Notification configurations

4. **Persistence Layer**
   - SQLite database for user data
   - Deal history tracking
   - User preferences storage

## Design

The data model and the flow of the deal processing are shown in the diagrams below:

### User Data Model

```mermaid
classDiagram
    class UserData {
        +int64 ChatID
        +string Username
        +bool OzbGood
        +bool OzbSuper
        +bool AmzDaily
        +bool AmzWeekly
        +[]string Keywords
        +[]string OzbSent
        +[]string AmzSent
    }
```

### Deal Processing Flow

```mermaid
sequenceDiagram
    participant S as Scraper
    participant B as Bot
    participant D as Database
    participant U as User
    
    S->>B: New Deal Found
    B->>D: Check Deal History
    D-->>B: Deal Status
    B->>U: Send Notification
    B->>D: Update History
```

## Functionality

### User Commands

The bot supports the following commands:

1. **Basic Commands**
   - `/start` - Register or view status
   - `/help` - Show help message
   - `/preferences` - View current settings
   - `/test` - Send test notification

2. **Deal Type Toggles**
   - `/ozbgood` - Toggle OzBargain Good deals
   - `/ozbsuper` - Toggle OzBargain Super deals
   - `/amzdaily` - Toggle Amazon Daily deals
   - `/amzweekly` - Toggle Amazon Weekly deals

3. **Keyword Management**
   - `/addkeyword <keyword>` - Add keyword to watchlist
   - `/removekeyword <keyword>` - Remove keyword
   - `/listkeywords` - View current keywords

### Deal Processing

1. **Scraping**
   - Regular interval-based scraping
   - Configurable scrape intervals
   - Maximum deal storage limits
   - Duplicate detection

2. **Notification**
   - Instant Telegram notifications
   - Optional Android TV notifications
   - Deal history tracking
   - Custom formatting for different deal types

### Admin Features

- Send announcements to all users
- System status monitoring
- User management capabilities

## Deployment

### Requirements

- Go 1.18+
- SQLite3
- Telegram Bot Token
- (Optional) Pipup configuration for TV notifications

### Configuration

Primary configuration through `config.yaml`:
- Scraper intervals
- Logging settings
- Database paths
- Notification settings

Environment variables for sensitive data:
- `TELEGRAM_BOT_TOKEN`
- `KRAMERBOT_ADMIN_PASS`
- `SQLITE_DB_PATH`

### Docker Deployment

```bash
# Build
docker build -t kramerbot:latest .

# Run
docker run -d --name kramerbot \
  --env-file ./kramerbot.env \
  -v "$(pwd)/data:/app/data" \
  --restart unless-stopped \
  kramerbot:latest
```

## In the pipeline (and some ideas)

1. Web interface for user management
2. Additional deal sources
3. Advanced filtering options
4. Deal analytics and trends
5. User preferences synchronization
6. Enhanced admin dashboard

KramerBot provides a solution for deal hunters who want to stay updated with the latest bargains without constantly checking multiple websites. It has been designed to be modular and easy to extend. New websites can be added by implementing the `scraper` interface.]]></description>
</item>
<item>
<title>Vim Setup From Scratch</title>
<link>https://intothevoid.github.io/vim-setup-from-scratch-2024-10-14.html</link>
<pubDate>Mon, 14 Oct 2024 09:31:35 +0800</pubDate>
<description><![CDATA[

Vim setups can be quite involved. There is the batteries included approach (SpaceVim, LazyVim etc.) and there's the hand rolled approach. Although the readymade approach is easier, manually setting up your Vim configuration can be a good learning experience.

This guide is a way for me to have all the steps I normally follow when installing Vim. I'll try and keep it simple and focus on languages I enjoy using as a developer - Golang and Python.

This guide will also focus on MacOS as I am using an M1 Macbook Pro at the moment. Most of the steps can be easily translated to other operating systems.

## Installation

Install Vim by issuing the following command -

```bash
brew install vim
```

## Sensible defaults

The sensible defaults plugin is a great way to have a starting point with your .vimrc (Vim's configuration file) without having to copy over someone else's configuration file. 

However, this is a plugin which will have to be installed using the steps in section 'Plugin Manager'

## Plugin Manager

One of the easiest ways to set things up in Vim and install plugins is to use a plugin manager. We'll use vim-plug here in our example -

```bash
sh -c 'curl -fLo "${XDG_DATA_HOME:-$HOME/.local/share}"/nvim/site/autoload/plug.vim --create-dirs \
       https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim'
```
Add a vim-plug section to your ~/.vimrc (or ~/.config/nvim/init.vim for Neovim)

```bash
call plug#begin()

" List your plugins here
Plug 'tpope/vim-sensible'

call plug#end()
```
Reload the file or restart Vim, then you can,
`:PlugInstall` to install the plugins
`:PlugUpdate` to install or update the plugins
`:PlugDiff` to review the changes from the last update
`:PlugClean` to remove plugins no longer in the list


## Useful developer plugins

Below is a list of useful plugins to install in Vim -

* Vim sensible - A set of sensible defaults for your .vimrc
* NERDTree - File explorer
* fzf.vim - Fuzzy file finder
* ALE - Linting and static analysis
* vim-fugitive - Git integration

Add these between the call plug#begin() and call plug#end() sections of your .vimrc -

```bash
Plug 'tpope/vim-sensible'
Plug 'scrooloose/nerdtree'
Plug 'junegunn/fzf'
Plug 'w0rp/ale'
Plug 'tpope/vim-fugitive'
```

Use the `:PlugInstall` command to install above plugins.

## Language support

I am tailoring this guide for Golang as it is my preferred programming language for now. Install the following plugins, by modifying your ~/.vimrc file -

```bash
Plug 'fatih/vim-go'
Plug 'vim-python/python-syntax'

```

## Colour scheme

Choose a color scheme that's easy on the eyes for long coding sessions. Molokai is a popular choice -

### Download

```bash
curl -fLo ~/.vim/colors/monokai.vim --create-dirs https://raw.githubusercontent.com/crusoexia/vim-monokai/refs/heads/master/colors/monokai.vim
```

### Installation

Select the theme by adding the following lines to your ~/.vimrc

```bash
colorscheme monokai
```
## Key mappings

You can add custom keymappings, such as invoking NERDTree, by adding the following to your ~/.vimrc

```bash
nnoremap <C-n> :NERDTreeToggle<CR>
nnoremap <C-p> :FZF<CR>
```

## Go specific plugins

```bash
Plug 'fatih/vim-go', { 'do': ':GoUpdateBinaries' }
Plug 'neoclide/coc.nvim', {'branch': 'release'}
Plug 'SirVer/ultisnips'
Plug 'AndrewRadev/splitjoin.vim'
```

* vim-go: The essential plugin for Go development in Vim
* coc.nvim: For advanced code completion (install gopls separately)
* ultisnips: For code snippets
* splitjoin.vim: Useful for splitting/joining struct literals

## Go specific settings

Use the following settings for an optimal Go programming experience -

```bash
" Go syntax highlighting
let g:go_highlight_fields = 1
let g:go_highlight_functions = 1
let g:go_highlight_function_calls = 1
let g:go_highlight_extra_types = 1
let g:go_highlight_operators = 1

" Auto formatting and importing
let g:go_fmt_autosave = 1
let g:go_fmt_command = "goimports"

" Status line types/signatures
let g:go_auto_type_info = 1

" Run :GoBuild or :GoTestCompile based on the go file
function! s:build_go_files()
  let l:file = expand('%')
  if l:file =~# '^\f\+_test\.go$'
    call go#test#Test(0, 1)
  elseif l:file =~# '^\f\+\.go$'
    call go#cmd#Build(0)
  endif
endfunction

" Map keys for most used commands.
" Ex: `\b` for building, `\r` for running and `\b` for running test.
autocmd FileType go nmap <leader>b :<C-u>call <SID>build_go_files()<CR>
autocmd FileType go nmap <leader>r  <Plug>(go-run)
autocmd FileType go nmap <leader>t  <Plug>(go-test)
```

Remember to run :GoInstallBinaries after setting this up to install necessary Go tools. Also, ensure you have gopls installed (go install golang.org/x/tools/gopls@latest) for the best code completion experience with coc.nvim.

This should give you a solid starting point for your Vim install. You can visit a website like https://vimawesome.com/ to get inspiration for plugins and further customisation.

]]></description>
</item>
<item>
<title>Variable Power Supply</title>
<link>https://intothevoid.github.io/variable-power-supply-2024-07-21.html</link>
<pubDate>Sun, 21 Jul 2024 22:01:24 +0930</pubDate>
<description><![CDATA[

## Introduction

In today's world of electronics and DIY projects, having a reliable power supply is essential. However, purchasing a variable power supply can be quite expensive. In this blog post, we'll guide you through creating your own variable power supply using an old laptop charger. This project not only saves you money but also gives new life to old electronic components that might otherwise end up in landfill.

## Parts Required

To embark on this project, you'll need the following items:

- A 3D printer
- An old laptop power supply with an output DC voltage greater than 6 volts
- A Programmable Constant Voltage Current Step-down Power Supply Module [WZ3605E](https://vi.aliexpress.com/w/wholesale-WZ3605E.html?spm=a2g0o.home.search.0)
- Banana plug socket [Link](https://vi.aliexpress.com/w/wholesale-banana-socket.html?spm=a2g0o.productlist.search.0)
- Barrel connector [Link] https://vi.aliexpress.com/item/1005006512605602.html?spm=a2g0o.productlist.main.1.154d72a0O2bLhC&algo_pvid=a48a2e5b-c98f-46a3-9efb-e96b140d6b7e&algo_exp_id=a48a2e5b-c98f-46a3-9efb-e96b140d6b7e-0&pdp_npi=4%40dis%21AUD%213.02%213.02%21%21%2114.53%2114.53%21%402101c80017215653450678713e0b25%2112000037482647997%21sea%21AU%212747051205%21&curPageLogUid=9XwmU1UcmZtW&utparam-url=scene%3Asearch%7Cquery_from%3A
- Soldering iron
- Quality wires rated for ~5amps
- Autodesk Fusion 360 design files 

## Steps with Assembly Pics

### Step 1: Gather All Components

Ensure all the required components are at hand before starting the assembly process. This includes checking the output voltage of the old laptop charger to confirm it meets the minimum requirement.

### Step 2.1: Design the Enclosure in Autodesk Fusion 360

I designed the enclosure in Autodesk Fusion 360. The sketch of the enclosure is shown below.

![Enclosure Sketch](images/power-supply/sketch.JPG)

The enclosure is a simple box with a lid. The lid has a hole for the power supply module and a hole for the barrel connector. The enclosure will be 3D printed in PLA. The enclosure looks like this once it has been designed.

![Enclosure Design](images/power-supply/model.JPG)

### Step 2.2: 3D Print the Enclosure

Using the Autodesk Fusion 360 design files provided, print the enclosure for your power supply. This will house all the components securely.

![3D Printed Enclosure](images/power-supply/box.jpg)

### Step 3: Prepare the Laptop Charger

If the laptop charger does not have the same barrel connector as the socket you purchased, you can splice the cable and only connect the power cables to the barrel jack. I used a 2.1mm barrel jack and socket, purchased from [Jaycar](https://www.jaycar.com.au)

### Step 4: Assemble the Power Supply Module

Connect the Programmable Constant Voltage Current Step-down Power Supply Module WZ3605E to the barrel connector attached to the back of the lid (see the STL files linked above). Ensure proper connections for input and output voltages to the module. The module I have linked has simple screw in connectors.

Inputs (6V-36V) - Connected from the barrel connector to the modules input terminals.

Outputs (6V-36V) - Connected to the banana socket terminals (which we will use as our outputs)

![Assembled Power Supply](images/power-supply/assemble.jpg)

### Step 5: Install Banana Plug Socket and Barrel Connector

Solder the banana plug socket and barrel connector to the output terminals of the power supply module. These will serve as the connection points for your devices.

![2.1mm Barrel Connector](images/power-supply/barrel.jpg)

### Step 6: Final Assembly and Testing

Place all assembled components inside the 3D printed enclosure. Connect the wires appropriately, ensuring safety and functionality. Test the power supply with a multimeter to confirm the output voltage range.

![Final Assembly](images/power-supply/finished.jpg)

## Conclusion

Creating a variable power supply from an old laptop charger is not only cost-effective but also environmentally friendly. By following the steps outlined in this guide, you've successfully repurposed an old electronic device into a useful tool for your projects. Remember, safety first when dealing with electronics. Enjoy experimenting with your new variable power supply!

---
]]></description>
</item>
<item>
<title>Organise Videos By Resolution</title>
<link>https://intothevoid.github.io/organise-videos-by-resolution-2024-05-18.html</link>
<pubDate>Sat, 18 May 2024 21:40:39 +0930</pubDate>
<description><![CDATA[

Recently I was clearing out some old movie rips that I had created from VideoCDs and DVDs many years ago. I wanted a quick and dirty way to organise these i.e. get rid of the videos of resolutions lower than 720p (1280x720)

With LLMs all the rage at the moment, all it took is a few prompts and corrections and in 15 minutes I had this script ready to go -


```python
import os
import shutil
from moviepy.editor import VideoFileClip

def analyze_and_move_folders(parent_folder):
    # Create the 'inferior' folder if it doesn't exist
    inferior_folder = os.path.join(parent_folder, 'inferior')
    if not os.path.exists(inferior_folder):
        os.makedirs(inferior_folder)

    # Walk through all subdirectories
    for root, dirs, files in os.walk(parent_folder):
        # Check if the current directory contains any video files
        video_files_found = False
        for file in files:
            if file.endswith(('.mkv', '.avi', '.mp4')):
                video_files_found = True
                break
        
        if video_files_found:
            # Load the first video file to check its resolution
            first_video_file = next((f for f in files if f.endswith(('.mkv', '.avi', '.mp4'))), None)
            if first_video_file:
                file_path = os.path.join(root, first_video_file)
                try:
                    clip = VideoFileClip(file_path)
                    
                    # Check if the video resolution is less than 720p
                    if clip.size[0] < 1280 or clip.size[1] < 720:
                        # Use shutil.move() instead of os.rename()
                        shutil.move(root, os.path.join(inferior_folder, os.path.basename(root)))
                        print(f'Moved folder {root} to inferior due to low resolution.')
                except UnicodeDecodeError as e:
                    print(f"Skipping {file_path} due to decoding error: {e}")
                    continue  # Skip this file and continue with the next one

# Specify the path to the parent folder
parent_folder = '/path/to/your/folder'
analyze_and_move_folders(parent_folder)

```

The script uses the moviepy library to analyse movie files. The script works in the following manner -

1. Scan all sub-folders within the root folder for movie files i.e. mkv, avi, and mp4 files
2. Once a movie file is encountered, analyse this file using moviepy
3. Find  out the resolution of the movie
4. If the resolution of the movie is < 1280p horizontally or < 720p vertically, move the file to a folder called 'inferior'
5. Repeat above steps until all sub-folders within the parent folder have been scanned

That's it. The script saved me time and effort from having to load each movie in a video player before deleting it.
]]></description>
</item>
<item>
<title>An introduction to the OpenAI API using Python</title>
<link>https://intothevoid.github.io/an-introduction-to-the-openai-api-using-python-2023-04-19.html</link>
<pubDate>Wed, 19 Apr 2023 22:31:18 +0930</pubDate>
<description><![CDATA[
## Introduction

Ever wanted to play chess against an AI? With the OpenAI API, you can! This blog post will show you how to use Python to interface with the OpenAI API, send and receive requests, and even play a game of chess against an AI opponent. We'll be using the FEN representation of the chess board to communicate game states with the API.

## Setting Up

Before we start, make sure you have the following:

* Python installed on your machine (preferably Python 3.6 or later)
* A valid OpenAI API key (you can sign up at https://beta.openai.com/signup/)

To install the OpenAI package, run the following command:

```bash
pip install openai
```

Now, let's import the required libraries and set up the API key:
    
```python
import openai
import os

openai.api_key = os.environ["OPENAI_API_KEY"]
```

Make sure to replace "OPENAI_API_KEY" with your actual API key or set it as an environment variable.

## Interacting with the OpenAI API

Now that we have the API key set up, let's create a function to send a prompt to the OpenAI API and receive a response. The function will receive the current FEN representation of the chess board and return the next FEN representation after the AI's move.

```python
def get_next_move(fen):
    prompt = f"Given the chess position in FEN notation: {fen}, what is the best move? Please provide the resulting FEN representation after the move."
    
    response = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        max_tokens=100,
        n=1,
        stop=None,
        temperature=0.5,
    )
    
    return response.choices[0].text.strip()
```

This function sends a POST request to the OpenAI API with the current FEN representation and receives the next FEN representation after the AI's move.

## Playing Chess

Now let's create a simple chess game loop that takes user input and sends it to the API for processing. We'll be using the python-chess library to handle the FEN representation and validate moves. You can install it with:

```bash
pip install python-chess
```

Here's the code to set up a simple chess game:

```python
import chess

board = chess.Board()

while not board.is_game_over():
    print(board)
    if board.turn == chess.WHITE:
        move = input("Your move (in UCI format): ")
        try:
            board.push_san(move)
        except ValueError:
            print("Invalid move, try again.")
            continue
    else:
        print("AI is thinking...")
        current_fen = board.fen()
        next_fen = get_next_move(current_fen)
        try:
            board.set_fen(next_fen)
        except ValueError:
            print("Error: AI provided an invalid FEN.")
            break
```

This code sets up a chess board and enters a game loop where the user (White) enters moves in UCI format, and the AI (Black) calculates its move using the OpenAI API.

## Conclusion

And there you have it! You've created a simple chess game using the OpenAI API and Python. ]]></description>
</item>
<item>
<title>Type safety in Python with Pydantic</title>
<link>https://intothevoid.github.io/type-safety-in-python-with-pydantic-2023-04-10.html</link>
<pubDate>Mon, 10 Apr 2023 16:56:00 +0930</pubDate>
<description><![CDATA[

## Introduction to Pydantic

Pydantic is a Python library that provides data validation and settings management, using Python type annotations. It is designed to make it easy to define and validate data models, allowing you to catch errors and handle them gracefully, making your code more robust and easier to maintain.

Pydantic is particularly useful for handling data that is passed between different parts of a system, such as between a front-end web form and a back-end API, or between different microservices. By defining a clear schema for the data, you can ensure that it is consistent and valid, regardless of where it comes from.

## Features of Pydantic

One of the key features of Pydantic is the use of Python type annotations. These annotations provide a way to specify the expected type of a variable or argument, which can be used to validate the data and catch errors early in the development process. Pydantic also provides a number of built-in validators, such as "email" and "url", that can be used to ensure that data conforms to specific formats.

To define a Pydantic model, you simply create a class that inherits from the BaseModel class. In this class, you define the fields that make up the model, using Python type annotations to specify the expected types. You can also specify default values and validation rules for each field.

## Example

```python
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

```
In this model, we have defined four fields: id, name, email, and password. The id field is expected to be an integer, while the other fields are expected to be strings.

To create an instance of the User model, we simply pass in the data as a dictionary:

```python
data = {
    "id": 1,
    "name": "Alice",
    "email": "alice@example.com",
    "password": "secretpassword",
}

user = User(**data)

```
Pydantic will automatically validate the data and ensure that it conforms to the expected types and validation rules. If any errors are found, Pydantic will raise a ValidationError exception, which includes details about the error and the field that caused it.

Pydantic also provides a number of other features, such as support for custom validation functions, automatic conversion of data types, and support for parsing and serializing data to and from JSON.

Pydantic is a powerful library that provides an easy and efficient way to validate data and handle errors in your Python applications. By using Pydantic to define and validate your data models, you can catch errors early in the development process and ensure that your code is more robust and easier to maintain.

## Validation in Pydantic

In Pydantic, to get error details, you need to use a try/except block. The error type will be pydantic.error_wrappers.ValidationError.

Here is an example:

```python
from pydantic import BaseModel, ValidationError

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

try:

    data = {
        "id": 1,
        "name": "Alice",
        "email": "alice@example.com",
        "password": "secret",
        }

    user = User(**data)

except ValidationError as e:
    print(e.json())
``` 

## Custom Validation

Pydantic provides a number of built-in validators, such as "email" and "url", that can be used to ensure that data conforms to specific formats. However, you can also define your own custom validation functions, which can be used to validate data in any way you like.

To define a custom validation function, you simply create a function that accepts a single argument, which will be the value of the field being validated. The function should raise a ValueError if the value is invalid, or return the value if it is valid.

Here is an example of a custom validation function that checks whether a string is a valid email address:

```python
from pydantic import BaseModel, ValidationError, validator
from email_validator import validate_email, EmailNotValidError

class User(BaseModel):
    id: int
    name: str
    email: str
    password: str

    @validator("email")
    def email_validator(cls, v):
        try:
            validate_email(v)
        except EmailNotValidError as e:
            raise ValueError("Invalid email address")
        return v

try:
    user = User(**data)
    pprint(user)

except ValidationError as e:
    print(e.json())
```

## Conclusion

In this article, we have looked at Pydantic, a Python library that provides data validation and settings management, using Python type annotations. We have also looked at some of the features of Pydantic, including the use of Python type annotations to specify the expected types of fields, and the use of custom validation functions to validate data in any way you like.

Pydantic is a powerful library that provides an easy and efficient way to validate data and handle errors in your Python applications. By using Pydantic to define and validate your data models, you can catch errors early in the development process and ensure that your code is more robust and easier to maintain.

## References

- [Pydantic](https://pydantic-docs.helpmanual.io/)]]></description>
</item>
<item>
<title>NoSQL Databases - MongoDB</title>
<link>https://intothevoid.github.io/nosql-databases---mongodb-2022-12-29.html</link>
<pubDate>Fri, 30 Dec 2022 09:51:13 +1030</pubDate>
<description><![CDATA[

NoSQL databases are a type of database that is designed to handle large amounts of data that is distributed across a large number of servers. NoSQL databases are particularly well-suited for handling unstructured data, such as text, images, and videos, and for handling data that is generated by web and mobile applications.

There are several different types of NoSQL databases, including:

- Document databases: These databases store data in the form of documents, which are similar to JSON objects. Document databases are designed to be flexible and scalable, and they are often used for storing large amounts of data that is not well-suited to the tabular structure of a traditional relational database. Examples of document databases include MongoDB, Apache Cassandra, Couchbase, and Amazon DocumentDB.
- Key-value stores: These databases store data as a collection of keys and values. Key-value stores are very fast and scalable, but they do not offer the same level of querying and indexing capabilities as other types of NoSQL databases. Examples of key-value stores include Redis and DynamoDB.
- Column-family databases: These databases store data as a collection of columns, rather than rows. Column-family databases are highly scalable and are often used for storing large amounts of data that needs to be accessed and processed quickly. Examples of column-family databases include Apache Cassandra and Google BigTable.
- Graph databases: These databases store data as a network of nodes and edges, which can be used to represent complex relationships between data items. Graph databases are often used for storing and querying data that has complex relationships, such as social networks or recommendation systems. Examples of graph databases include Neo4j and TigerGraph.

One of the main benefits of NoSQL databases is their ability to scale horizontally, meaning that they can easily add more servers to the database cluster as the amount of data or number of users increases. This makes them well-suited for handling the high volume of data and traffic that is common in modern web and mobile applications. NoSQL databases are also generally easier to set up and maintain than traditional relational databases, which can require more complex schema design and administration.

However, NoSQL databases do have some disadvantages compared to traditional relational databases. One of the main limitations of NoSQL databases is that they do not offer the same level of querying and indexing capabilities as relational databases. This can make it more difficult to perform complex queries and analysis on the data, and it can also make it more difficult to enforce data integrity and consistency. NoSQL databases are also generally not as good at handling transactions and ACID (atomic, consistent, isolated, and durable) guarantees as relational databases.

Despite these limitations, NoSQL databases are a popular choice for many modern applications, and they have a strong following among developers. If you're interested in using a NoSQL database in your project, one option to consider is MongoDB. MongoDB is a popular open-source document database that is widely used for storing and accessing data in web and mobile applications. Here's some sample Go code that demonstrates how to connect to a MongoDB database and insert a document:

```go
package main

import (
	"context"
	"fmt"
	"log"

	"go.mongodb.org/mongo-driver/mongo"
	"go.mongodb.org/mongo-driver/mongo/options"
)

func main() {
	// Set up a connection to the MongoDB server
	client, err := mongo.NewClient(options.Client().ApplyURI("mongodb://localhost:27017"))

if err != nil { log.Fatal(err) } err = client.Connect(context.TODO()) if err != nil { log.Fatal(err) }

// Choose the database and collection to use
collection := client.Database("test").Collection("people")

// Insert a new document
doc := map[string]interface{}{
	"name": "John Smith",
	"age":  30,
}
result, err := collection.InsertOne(context.TODO(), doc)
if err != nil {
	log.Fatal(err)
}
fmt.Println("Inserted document", result.InsertedID)
}

// Choose the database and collection to use
collection := client.Database("test").Collection("people")

// Insert a new document
doc := map[string]interface{}{
	"name": "John Smith",
	"age":  30,
}
result, err := collection.InsertOne(context.TODO(), doc)
if err != nil {
	log.Fatal(err)
}
fmt.Println("Inserted document", result.InsertedID)

```

NoSQL databases are a powerful tool for handling large amounts of unstructured data and for building scalable web and mobile applications. While they do have some limitations compared to traditional relational databases, they are still a popular choice for many developers due to their ease of use and ability to scale. If you're considering using a NoSQL database in your project, be sure to carefully weigh the pros and cons and choose the database that is best suited to your needs.
]]></description>
</item>
<item>
<title>Levenshtein Distance</title>
<link>https://intothevoid.github.io/levenshtein-distance-2022-12-25.html</link>
<pubDate>Mon, 26 Dec 2022 00:04:51 +1030</pubDate>
<description><![CDATA[
Levenshtein distance is a measure of the similarity between two strings, calculated as the minimum number of single-character edits (insertions, deletions, or substitutions) required to transform one string into the other. Here is an example of how you can calculate the Levenshtein distance between two strings in Go:

```Go
package main

import (
	"fmt"
	"math"
)

func main() {
	s1 := "kitten"
	s2 := "sitting"

	distance := levenshteinDistance(s1, s2)
	fmt.Printf("The Levenshtein distance between %s and %s is %d\n", s1, s2, distance)
}

func levenshteinDistance(s1, s2 string) int {
	// Convert strings to rune slices for Unicode support
	r1 := []rune(s1)
	r2 := []rune(s2)

	// Initialize a two-dimensional matrix with all values set to zero
	matrix := make([][]int, len(r1)+1)
	for i := range matrix {
		matrix[i] = make([]int, len(r2)+1)
	}

	// Set the initial values of the first row and column of the matrix
	for i := 1; i <= len(r1); i++ {
		matrix[i][0] = i
	}
	for j := 1; j <= len(r2); j++ {
		matrix[0][j] = j
	}

	// Calculate the Levenshtein distance using the dynamic programming algorithm
	for i := 1; i <= len(r1); i++ {
		for j := 1; j <= len(r2); j++ {
			cost := 0
			if r1[i-1] != r2[j-1] {
				cost = 1
			}
			matrix[i][j] = min(matrix[i-1][j]+1, matrix[i][j-1]+1, matrix[i-1][j-1]+cost)
		}
	}

	// Return the final value in the bottom-right corner of the matrix
	return matrix[len(r1)][len(r2)]
}

func min(a, b, c int) int {
	// Return the minimum of three integers
	return int(math.Min(float64(a), math.Min(float64(b), float64(c))))
}
```
This code defines a function `levenshteinDistance` that takes in two strings as input and returns an integer representing the Levenshtein distance between them. The function first converts the input strings to slices of runes (Unicode characters) to support Unicode characters. It then initializes a two-dimensional matrix with all values set to zero, and sets the initial values of the first row and column of the matrix to the lengths of the input strings.

The function then uses a dynamic programming algorithm to calculate the Levenshtein distance. It iterates over the elements of the input strings and compares them, adding a cost of 1 if they are different and 0 if they are the same. It then calculates the minimum of the three values in the matrix: the value above, the value to the left, and the value to the upper-left (diagonal) of the current position. This value is then added to the current position in the matrix.

Finally, the function returns the final value in the bottom-right corner of the matrix, which represents the Levenshtein distance between the input strings.

Here is an example of how you can use this function:
```go
s1 := "kitten"
s2 := "sitting"
distance := levenshteinDistance(s1, s2)
fmt.Printf("The Levenshtein distance between %s and %s is %d\n", s1, s2, distance)
```

This will output the following:
```bash
The Levenshtein distance between kitten and sitting is 3
```


]]></description>
</item>
<item>
<title>Spotify Playlist Backups using Python</title>
<link>https://intothevoid.github.io/spotify-playlist-backups-using-python-2022-12-11.html</link>
<pubDate>Sun, 11 Dec 2022 11:43:47 +1030</pubDate>
<description><![CDATA[

To create a web application that backs up your Spotify playlists as a JSON file, you will need to do the following:

1.  First, you will need to install the `spotipy` library, which provides a Python interface for the Spotify Web API. You can do this by running the following command:
```bash
pip install spotipy 
```
2. Next, you will need to create a Spotify app and obtain a client ID and client secret for the app. You can do this by logging in to the [Spotify Developer Dashboard](https://developer.spotify.com/dashboard) and following the instructions on the website.

## Creating the Python Backend
Once you have obtained your client ID and client secret, you can use the `spotipy.Spotify` class to authenticate yourself and access the Spotify Web API. An example of how you can do this is shown below:

```python
import spotipy

client_id = "your-client-id"
client_secret = "your-client-secret"

spotify = spotipy.Spotify(
    client_id=client_id,
    client_secret=client_secret
)
```

After authenticating, you can use the `spotify.user_playlists()` method to retrieve a list of all your Spotify playlists. This method returns a paginated list of playlists, so you will need to iterate through the pages and collect the playlists from each page. Here is an example of how you can do this:

```python
playlists = []

response = spotify.user_playlists()
playlists.extend(response["items"])

while response["next"]:
    response = spotify.next(response)
    playlists.extend(response["items"])

```

Once you have collected all your playlists, you can use the `json` module to convert the list of playlists to a JSON string, and then write the JSON string to a file. Here is an example of how you can do this:

```python
import json

with open("playlists.json", "w") as file:
    json.dump(playlists, file)

```

Finally, you can create a simple Flask app that exposes a `/backup` endpoint, which accepts `POST` requests and uses the code from the previous steps to back up your playlists as a JSON file. Here is an example of how you can do this:

```python
from flask import Flask, request
import spotipy
import json

app = Flask(__name__)

client_id = "your-client-id"
client_secret = "your-client-secret"
spotify = spotipy.Spotify(
    client_id=client_id,
    client_secret=client_secret
)

@app.route("/backup", methods=["POST"])
def backup():
    playlists = []

    response = spotify.user_playlists()
    playlists.extend(response["items"])

    while response["next"]:
        response = spotify.next(response)
        playlists.extend(response["items"])

    with open("playlists.json", "w") as file:
        json.dump(playlists, file)

    return "Playlists backed up successfully!"

```

To run the Flask app, you will need to first start the Flask development server by running the following command:

```bash
export FLASK_APP=app.py
flask run

```

This will start the Flask development server and make your app available at `http://localhost:5000`. To make a `POST` request to the `/backup` endpoint, you can use a tool like `curl` or `Postman` to send a `POST` request to `http://localhost:5000/backup`.

Here is an example of how you can use `curl` to make a `POST` request:
```bash
curl -X POST http://localhost:5000/backup

```

Once you have made the request, your playlists should be backed up as a JSON file named `playlists.json` in the current directory.

Note: This code is provided for illustration purposes only, and is not intended for production use. It does not include error handling or other best practices for building a Flask app.

## Creating the Web Frontend
Here's how you could create a web page to create a nice frontend for the Python program that backs up your Spotify playlists:

```html
<!DOCTYPE html>
<html>
<head>
    <title>Spotify Playlist Backup</title>
    <style>
        /* This is some CSS that styles the page */
        body {
            font-family: sans-serif;
            text-align: center;
        }
        input, button {
            padding: 10px 20px;
            font-size: 16px;
        }
        input {
            width: 300px;
        }
        button {
            background-color: #1DB954;
            color: white;
            cursor: pointer;
        }
        pre {
            text-align: left;
            margin: 20px;
            padding: 20px;
            border: 1px solid #ccc;
            background-color: #f1f1f1;
        }
    </style>
</head>
<body>
    <h1>Spotify Playlist Backup</h1>
    <p>Enter your Spotify credentials and username below to backup your playlists as a JSON file.</p>
    <form>
        <input type="text" id="client_id" placeholder="Your Spotify client ID">
        <input type="text" id="client_secret" placeholder="Your Spotify client secret">
        <input type="text" id="username" placeholder="Your Spotify username">
        <button type="submit">Backup Playlists</button>
    </form>
    <pre id="output"></pre>
    <script>
        // This is the JavaScript code that runs when the page is loaded

        // This gets the form element
        const form = document.querySelector("form");

        // This adds an event listener that runs when the form is submitted
        form.addEventListener("submit", async (e) => {
            // This prevents the page from reloading
            e.preventDefault();

            // This gets the input elements
            const clientIdInput = document.querySelector("#client_id");
            const clientSecretInput = document.querySelector("#client_secret");
            const usernameInput = document.querySelector("#username");

            // This gets the values from the input elements
            const clientId = clientIdInput.value;
            const clientSecret = clientSecretInput.value;
            const username = usernameInput.value;

            // This shows a message while the playlists are being backed up
            const output = document.querySelector("#output");
            output.innerHTML = "Backing up your playlists... please wait.";

            // This sends a request to the server to backup the playlists
            const response = await fetch("/backup", {
                method: "POST",
                headers: {
                    "Content-Type": "application/json",
                },
                body: JSON.stringify({
                    clientId,
                    clientSecret,
                    username,
                }),
            });

            // This gets the response from the server
            const data = await response.json();

            // This displays the response from the server
            output.innerHTML = JSON.stringify(data, null, 4);
        });
    </script>
</body>

```

## Spotify backup playlist as a standalone script

```python
import json
import spotipy
from spotipy.oauth2 import SpotifyClientCredentials

# This is your Spotify client ID and secret
client_id = "YOUR_CLIENT_ID"
client_secret = "YOUR_CLIENT_SECRET"

# This is your Spotify username
username = "YOUR_USERNAME"

# This is the path to the JSON file where your playlists will be saved
json_file = "playlists.json"

# This creates a Spotify client using your client ID and secret
client_credentials_manager = SpotifyClientCredentials(client_id=client_id, client_secret=client_secret)
spotify = spotipy.Spotify(client_credentials_manager=client_credentials_manager)

# This gets your user ID
user_id = spotify.current_user()["id"]

# This gets all your playlists
playlists = spotify.user_playlists(user_id)

# This creates an empty list where your playlists will be saved
playlists_data = []

# This iterates over your playlists
for playlist in playlists["items"]:
    # This gets the playlist ID and name
    playlist_id = playlist["id"]
    playlist_name = playlist["name"]

    # This gets the tracks in the playlist
    tracks = spotify.user_playlist_tracks(user_id, playlist_id)

    # This creates an empty list where the tracks will be saved
    tracks_data = []

    # This iterates over the tracks
    for track in tracks["items"]:
        # This gets the track data
        track_data = track["track"]

        # This saves the track data
        tracks_data.append({
            "id": track_data["id"],
            "name": track_data["name"],
            "artists": [artist["name"] for artist in track_data["artists"]],
            "album": track_data["album"]["name"],
        })

	# This saves the playlist data
	playlists_data.append({
	    "id": playlist_id,
	    "name": playlist_name,
	    "tracks": tracks_data,
	})

# This saves the playlists data to the JSON file
with open(json_file, "w") as f: 
	json.dump(playlists_data, f, indent=4)

print("Successfully backed up your playlists to", json_file)
```

In this code, we use the `spotipy` library to access the Spotify API and retrieve the data for your playlists and tracks. We then save this data to a JSON file using the `json` library.

To use this program, you will need to replace the `client_id`, `client_secret`, and `username` variables with your own Spotify credentials and username. You will also need to specify the path to the JSON file where you want your playlists to be saved.

Once you have done this, you can run the program and it will retrieve your playlists and tracks and save them to the JSON file. You can then use this file to backup your playlists or to transfer them to another Spotify account.
]]></description>
</item>
<item>
<title>Transferring Files from iOS to Linux wirelessly</title>
<link>https://intothevoid.github.io/transferring-files-from-ios-to-linux-wirelessly-2022-08-05.html</link>
<pubDate>Sat, 06 Aug 2022 09:29:40 +0930</pubDate>
<description><![CDATA[

## Overview
I was an Android user for a long period of time, right from the Jellybean and 
Gingerbread days. After almost a decade being with Android I finally moved to
iOS, because I was sick of the fragmentation and I wish Android took privacy 
as seriously as iOS does. One of the most common things I need to do is transfer
files across from my iOS device to my Linux laptop. One way of doing this is to 
connect my laptop and iPhone with a lightning cable, however it is super
inconvenient and you don't always have a cable lying around. You can email 
the file to yourself but thats a really roundabout way to do it. 

With an Android device, things are easier as Google allows apps on the playstore
that have built inservers. iOS does not allow such apps. However, what iOS does
support is being able to connect to other servers like an ssh server.

If you have a Macbook and an iOS device, a wireless transfer is very easy - 
Airdrop. However, with Linux, you're out of luck. The only reliable way to
transfer files to a Linux laptop is via sftp or ftp over ssh.

## Install ssh server on Linux
In this part of the post, I'll show you how to ==install== an SSH server on Fedora Linux. 

First, we'll need to install the openssh-server package:
```bash
sudo dnf install openssh-server
```

Once the package is installed, we'll need to start the SSH service:
```bash
sudo systemctl enable sshd # ensure sshd starts up after reboot
sudo systemctl start sshd # starts the ssh daemon
sudo systemctl status sshd # check status of service
```

After you have installed the ssh server on your laptop move on to the next part
of this post. You will need your iOS device - iPhone or iPad for this.

## Install FE File Explorer on your iOS device
FE File Explorer is an app which lets you connect to servers from your iOS device.
It can connect to a variety of sources including NAS, FTP, SAMBA etc.

It is a free application that also has a paid version but even the free version
has a lot of built in functionality and it will suit the needs of most users.

Once you launch the app, hit the + button on the top right hand corner and the 
following screen will be shown -

![FE File Explorer New Window](/images/fexplorer/fexplorer_add.jpg "FE File Explorer New Window")
<p class="subtitle">FE File Explorer New Window</p>

Select 'SFTP'. If 'SFTP' is not available you should be able to select 'FTP' although
I have not tested it with this option.

Enter details in the next screen. The 'Hostname' (IP Address), 'Username' and 'Password'
are the most important fields. You can check the IP address of the target computer
using the following command -
```bash
ip a
```

The username and password are the account details you use on your Linux computer.

If you followed the instructions correctly, you should now see an entry for your
connection at the main screen of the app. Once you tap it, you should be able
to view all the files and folders of your Linux computer.

![FE File Explorer Connection Added](/images/fexplorer/fexplorer_main.jpg "FE File Explorer Connection Added")
<p class="subtitle">FE File Explorer Connection Added</p>

Transferring files is easy, you can simply hit the 3 dots for a File context menu 
as shown below -

![FE File Explorer Popup Menu](/images/fexplorer/fexplorer_filepopup.jpg "FE File Explorer Popup Menu")
<p class="subtitle">FE File Explorer Popup Menu</p>

This seems like a lot of steps but once you have set things up its like using
any other app on your phone.
]]></description>
</item>
<item>
<title>Setup Vim (Astro Vim) on a Macbook</title>
<link>https://intothevoid.github.io/setup-vim-astro-vim-on-a-macbook-2022-07-31.html</link>
<pubDate>Sun, 31 Jul 2022 11:53:48 +0930</pubDate>
<description><![CDATA[

## Overview

Use the following steps to have a decent vim installation relatively quickly on a Macbook with a M1 / M2 chipset

## Brew installation

Brew is a package manager for MacOS which makes it very easy to install packages. I highly recommend this package manager on MacOS. Its worth the effort to get it installed you will thank yourself later on.

From www.brew.sh -

```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

Sample command using brew -

```bash
brew install wget
```

## Install NeoVim

Neovim is our base vim install. Install Neovim using the command -

```bash
brew install neovim
```
## Install Astro Vim

AstroNvim is an aesthetic and feature-rich neovim config that is extensible and easy to use with a great set of plugins. Install AstroNvim by using the commands

```bash
git clone https://github.com/AstroNvim/AstroNvim ~/.config/nvim

nvim +PackerSync
```

## Setup Astro Vim

Make a backup of your existing nvim configuration -

```bash
mv ~/.config/nvim ~/.config/nvimbackup
```

#### Install LSP

Enter `:LspInstall` followed by the name of the server you want to install  
Example: `:LspInstall pyright`

#### Install language parser

Enter `:TSInstall` followed by the name of the language you want to install  
Example: `:TSInstall python`

#### Manage plugins

Run `:PackerClean` to remove any disabled or unused plugins  
Run `:PackerSync` to update and clean plugins

#### Update AstroNvim

Run `:AstroUpdate` to get the latest updates from the repository

#### Install NerdFonts

Follow this step to install your desired fonts and so font glyphs are rendered correctly

Firacode is my current favourite ♥️

Fonts can be downloaded from https://www.nerdfonts.com/font-downloads

#### Optional requirements

These are other optional packages which can be installed -

[ripgrep](https://github.com/BurntSushi/ripgrep "ripgrep") - live grep telescope search (leader + fw)

[lazygit](https://github.com/jesseduffield/lazygit "lazygit") - git ui toggle terminal (leader + tl or leader + gg)

[NCDU](https://dev.yorhel.nl/ncdu "NCDU") - disk usage toggle terminal (leader + tu)

[Htop](https://htop.dev/ "Htop") - process viewer toggle terminal (leader + tt)

[Python](https://www.python.org/ "Python") - python repl toggle terminal (leader + tp)

[Node](https://nodejs.org/en/ "Node") - node repl toggle terminal (leader + tn)


]]></description>
</item>
<item>
<title>Create a blog using Hugo and Github Pages</title>
<link>https://intothevoid.github.io/create-a-blog-using-hugo-and-github-pages-2021-04-05.html</link>
<pubDate>Mon, 05 Apr 2021 14:13:49 +0930</pubDate>
<description><![CDATA[
Update: Added section on adding Github Action for Hugo build

This is a guide on how you can create a blog or website from scratch using Hugo and Github. This is also my first post on this blog, which has been created using the steps mentioned in this article using Hugo and Github Pages.

It is quite easy to create a blog using Wordpress or some other CMS but the simplicity of Hugo is what drove me to it. Hugo is a static html generator. This means, you write posts using Markdown and templates using any text editor. Hugo then processes these files and generates a bunch of html and css. 

This has the benefit of making it extremely easy to deploy to just about any cloud service or provider. This is where Github Pages comes in. Github Pages is free and if you have a Github account, you are already well on your way.

I used a Macbook Air M1 for creating this blog and the steps outlined in this article should be more or less the same whether you use Windows / MacOS or Linux. Lets get our elbows greasy.

### Create Repos
We will begin by creating 2 repositories in our Github account -
1. Base repo 'blog' for hosting our posts / source code / files created by Hugo
2. Blog hosting repo 'username.github.io' for hosting the actual blog, which consists of generated html and css

Create the base repo on Github by clicking the '+' button on your [Github page](https://www.github.com). I've decided to call my base repo 'blog'. Clone this repository to your machine (even if it doesn't contain any files at the moment)
```bash
git clone git@github.com:username/blog.git
cd blog/
```

From the Github website, click the '+' button again to create a new repository called 'username.github.io'. Replace username with your Github user name.

Add this second repo (which we will use for hosting our blog) as a submodule of our base repo -
```bash
git submodule add git@github.com:username/username.github.io.git public
```
The above command will add the hosting repo as a submodule into the 'public' folder.

### Install Hugo
Download the Hugo binaries from the [Hugo Releases](https://github.com/gohugoio/hugo/releases) page. As I am using MacOS, I used the command -
```bash
brew install hugo
```
For detailed installation instructions, visit https://gohugo.io/getting-started/installing/
Verify Hugo installed correctly by 
```bash
hugo version
```

### Create a new site
```bash
hugo create new site myblogname
```
The above command will create a basic website with enough boilerplate to let us get started.

### Add a theme
Visit https://themes.gohugo.io/ for a collection of available Hugo themes. Once you have decided which theme you wish to use for your website / blog, 
```bash
cd myblogname
git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke
```
Your folder structure should now look like
**~/blog/myblogname/config.toml** | Configuration File
**~/blog/myblogname/themes/ananke/** | Theme folder
**~/blog/myblogname/public** | Second hosting repo

### Update config.toml 
```toml
theme = "ananke"
title = "Karan Kadam"
baseURL = "https://username.github.io"
theme = "ananke"
themesDir = "./themes"
```

### Create your first post
```bash
hugo create new posts/my-first-post.md
``` 

### Build your blog
```bash
hugo -t "ananke"
```
This will generate all the html and styling required for your blog and publish them under the "public" folder. The -t flag tells hugo to use your selected theme.

### Local test
```bash
hugo server --bind 0.0.0.0
```
This will launch the built in webserver and your blog will be available locally. Goto a webbrowser and access http://localhost:1313 and you should see your blog, styled according to the theme you chose and containing your first sample post.

### Deploy your website to Github Pages
To be able to share your website with the rest of the world, you will need to host it somewhere. We chose the repository at 'https://username.github.io'. Check in all your changes and push them to the username.github.io -
```bash
cd ~/blog/myblogname/public
git status
git add -A
git commit -m "initial test commit" # commit contents of ~/blog/myblogname/public
git push
cd ../..
git add -A
git commit -m "initial test commit" # commit contents of ~/blog/
```
If it all went well, accessing http://username.github.io should take you to your freshly baked blog. Here on, all you need to do is,
```bash
hugo create new posts/postname.md
hugo -t "ananke"
```

### Markdown resources
It is a good idea to read up and familiarise yourself with Markdown Syntax. It is quite compact and can be learned in a single sitting. I would recommend starting with [The Markdown Cheatsheet](https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf) to quickly understand how to write posts in any editor using the Markdown syntax. Keep writing! :metal:

### Bonus - Automate the Hugo build
To automate your flow even further, you can create a Github Action, which will automatically run Hugo for you and publish to your username.github.io repository for you. This will save you any manual work of building your blog each time and pushing back to your public repo.
The added benefit of this approach is you no longer need access to the cli. You can log into your Github base repo from your phone, add Mardown posts from the WebUI and your Github Actions pipeline will take care of the rest.
Begin by adding a GitHub action file to your base repo -
```bash
vim ./github/workflows/main.yml
```
Here is a what my main.yml file looks like -
```yaml
name: CI
on: push
jobs:
  deploy:
    runs-on: ubuntu-18.04
    steps:
      - name: Git checkout
        uses: actions/checkout@v2

      - name: Update theme
        run: git submodule update --init --recursive

      - name: Setup hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: "0.82.0"

      - name: Build
        run: cd username && hugo -t ananke && pwd && ls -a ./public

      - name: Deploy
        uses: peaceiris/actions-gh-pages@v3
        with:
          deploy_key: ${{ secrets.ACTIONS_DEPLOY_KEY }}
          external_repository: username/username.github.io
          publish_dir: ./intothevoid/public
          user_name: username
          user_email: username@email.com
          publish_branch: main
```
Replace values with your own where needed.

A quick explanation of the above Github Action is given below -
1. Checkout - checkout the base repo
2. Update - Update your theme which was added as a submodule
3. Setup Hugo - Call an external action which basically installs Hugo
4. Build - Build the website by calling **hugo -t ananke**
5. Deploy - Publish the built files to your external public repo username.github.io

**Note:** There is a bit of setup involved where you need to generate your token (indicated by deploy key above). I would highly recommend visiting https://github.com/peaceiris/actions-gh-pages for details of the external action and how to setup your token.

This should get you at a stage where you can simply push a post written in Markdown format to your base repo and the rest should happen automatically. Post --> Commit Base Repo --> Github Action --> Build Blog --> Publish Static HTML.
]]></description>
</item>
</channel>
</rss>